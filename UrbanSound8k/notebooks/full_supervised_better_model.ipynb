{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from functools import reduce \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from datasetManager import DatasetManager\n",
    "from generators import Generator\n",
    "import signal_augmentations as sa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.973823Z",
     "start_time": "2019-11-12T15:36:12.893994Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.value = 0\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.count += 1\n",
    "\n",
    "        \n",
    "class BinaryAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = (y_pred>0.5).float()\n",
    "            correct = (y_pred == y_true).float().sum()\n",
    "            self.value = correct/ (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "        \n",
    "class CategoricalAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.value = torch.mean((y_true == y_pred).float())\n",
    "            self.accumulate_value += self.value\n",
    "\n",
    "            return self.accumulate_value / self.count\n",
    "\n",
    "        \n",
    "class Ratio(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_adv_pred):\n",
    "        super().__call__()\n",
    "        \n",
    "        results = zip(y_pred, y_adv_pred)\n",
    "        results_bool = [int(r[0] != r[1]) for r in results]\n",
    "        self.value = sum(results_bool) / len(results_bool) * 100\n",
    "        self.accumulate_value += self.value\n",
    "        \n",
    "        return self.accumulate_value / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.997511Z",
     "start_time": "2019-11-12T15:36:12.975482Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:13.020782Z",
     "start_time": "2019-11-12T15:36:13.000410Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_seed(seed=43):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:34.259332Z",
     "start_time": "2019-11-12T15:36:34.233822Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## CNN original\n",
    "https://arxiv.org/pdf/1608.04363.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding,\n",
    "                pool_kernel_size, pool_stride):\n",
    "        super(ConvPoolReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvPoolReLU(1, 24, 3, 1, 1, (4,2), (4,2)),\n",
    "            ConvPoolReLU(24, 48, 3, 1, 1, (4,2), (4,2)),\n",
    "            ConvPoolReLU(48, 48, 3, 1, 1, (4,2), (4,2)),\n",
    "            ConvReLU(48, 48, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1008, 10),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, conv_kernel_size, conv_stride, conv_padding):\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(crnn, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvBNReLU(1, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,2), stride=(4,2)),\n",
    "            ConvBNReLU(64, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,2), stride=(4,2)),\n",
    "            ConvBNReLU(64, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,1), stride=(4,1)),\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.GRU(64, 64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.strong = nn.Sequential(\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        \n",
    "        x = x.squeeze(dim=-2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x, h = self.rnn(x)\n",
    "        \n",
    "        strong = self.strong(x)\n",
    "        \n",
    "        weak = strong.permute(0, 2, 1)\n",
    "        weak = F.avg_pool1d(weak, kernel_size=weak.size()[2:])\n",
    "#         max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        weak = weak.view(-1, weak.shape[1])\n",
    "        \n",
    "        \n",
    "        return weak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultisampleDropout2d(nn.Module):\n",
    "    \"\"\"https://arxiv.org/pdf/1905.09788.pdf\"\"\"\n",
    "    def __init__(self, ratio, nb_sample):\n",
    "        super(MultisampleDropout2d, self).__init__()\n",
    "        self.nb_sample = nb_sample\n",
    "        \n",
    "        self.dropouts = [nn.Dropout2d(ratio) for _ in range(nb_sample)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d = [dropout(x) for dropout in self.dropouts]\n",
    "        return torch.mean(torch.stack(d, dim=0), dim=0)\n",
    "    \n",
    "class MultisampleDropout1d(nn.Module):\n",
    "    \"\"\"https://arxiv.org/pdf/1905.09788.pdf\"\"\"\n",
    "    def __init__(self, ratio, nb_sample):\n",
    "        super(MultisampleDropout1d, self).__init__()\n",
    "        self.nb_sample = nb_sample\n",
    "        \n",
    "        self.dropouts = [nn.Dropout(ratio) for _ in range(nb_sample)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d = [dropout(x) for dropout in self.dropouts]\n",
    "        return torch.mean(torch.stack(d, dim=0), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_size, out_size, t, kernel_size, stride, padding):\n",
    "        super(MBConv, self).__init__()\n",
    "        expand_dim = in_size * t\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_size, expand_dim, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(expand_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(expand_dim, expand_dim, kernel_size=kernel_size, stride=stride, padding=padding, groups=expand_dim),\n",
    "            nn.BatchNorm2d(expand_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(expand_dim, out_size, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.stride == 1:\n",
    "            return x + self.conv(x)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 conv_input_dim: tuple = (64, 431),\n",
    "                 conv_in_size: list = [1, 64, 64],\n",
    "                 conv_out_size: list = [64, 64, 64],\n",
    "                 t = [1, 6, 6],\n",
    "                 s = [1, 2, 2],\n",
    "                 n = [1, 2, 2],\n",
    "                ):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.i =0\n",
    "        \n",
    "        self.conv_input_dim = conv_input_dim\n",
    "        self.conv_in_size = conv_in_size\n",
    "        self.conv_out_size = conv_out_size\n",
    "        self.t = t\n",
    "        \n",
    "        conv_layers = []\n",
    "        for i in range(len(conv_in_size)):\n",
    "            if i == 0:\n",
    "                conv_layers.append(nn.Conv2d(conv_in_size[i], conv_out_size[i], 3, 1, 1))\n",
    "                continue\n",
    "            \n",
    "            conv_layers.append( MBConv(conv_in_size[i], conv_out_size[i], t[i], 3, s[i], 1) )\n",
    "            for j in range(n[i]-1):\n",
    "                conv_layers.append( MBConv(conv_out_size[i], conv_out_size[i], t[i], 3, 1, 1) )\n",
    "    \n",
    "        self.features = nn.Sequential(*conv_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            MultisampleDropout2d(0.2, 8),\n",
    "            nn.Conv2d(self.conv_out_size[-1], 10, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.AdaptiveMaxPool2d((1, 1)),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "#         x = x.view(-1, 1, self.conv_input_dim[0], self.conv_input_dim[1])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        x= x.view(-1, x.shape[1])\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN With dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLUPool(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding,\n",
    "                pool_kernel_size, pool_stride, dropout: float = 0.0):\n",
    "        super(ConvBNReLUPool, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn_d, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvBNReLUPool(1, 32, 3, 1, 1, (4,2), (4,2), 0.0),\n",
    "            ConvBNReLUPool(32, 64, 3, 1, 1, (4,2), (4,2), 0.3),\n",
    "            ConvBNReLUPool(64, 64, 3, 1, 1, (4,2), (4,2), 0.3),\n",
    "            ConvReLU(64, 64, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1344, 10),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN compound scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic find valid scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8 2.2\n"
     ]
    }
   ],
   "source": [
    "alpha = np.linspace(1, 1.5, 6)\n",
    "beta = np.linspace(1, 1.5, 6)\n",
    "gamma = np.linspace(1, 1.5, 6)\n",
    "\n",
    "import itertools\n",
    "\n",
    "valid_scaling = []\n",
    "tolerance = 0.1\n",
    "target = 2\n",
    "low_target = target - (target * tolerance)\n",
    "high_target = target + (target * tolerance)\n",
    "print(low_target, high_target)\n",
    "\n",
    "for a, b, g in itertools.product(alpha, beta, gamma):\n",
    "    M = a * b**2 * g**2\n",
    "    \n",
    "    if low_target < M < high_target:\n",
    "        valid_scaling.append((a, b, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0, 1.4),\n",
       " (1.0, 1.1, 1.3),\n",
       " (1.0, 1.2, 1.2),\n",
       " (1.0, 1.3, 1.1),\n",
       " (1.0, 1.4, 1.0),\n",
       " (1.1, 1.0, 1.3),\n",
       " (1.1, 1.0, 1.4),\n",
       " (1.1, 1.1, 1.2),\n",
       " (1.1, 1.2, 1.1),\n",
       " (1.1, 1.3, 1.0),\n",
       " (1.1, 1.4, 1.0),\n",
       " (1.2, 1.0, 1.3),\n",
       " (1.2, 1.1, 1.2),\n",
       " (1.2, 1.2, 1.1),\n",
       " (1.2, 1.3, 1.0),\n",
       " (1.3, 1.0, 1.2),\n",
       " (1.3, 1.0, 1.3),\n",
       " (1.3, 1.1, 1.1),\n",
       " (1.3, 1.2, 1.0),\n",
       " (1.3, 1.3, 1.0),\n",
       " (1.4, 1.0, 1.2),\n",
       " (1.4, 1.1, 1.1),\n",
       " (1.4, 1.2, 1.0),\n",
       " (1.5, 1.0, 1.1),\n",
       " (1.5, 1.0, 1.2),\n",
       " (1.5, 1.1, 1.0),\n",
       " (1.5, 1.1, 1.1),\n",
       " (1.5, 1.2, 1.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLUPool(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding,\n",
    "                pool_kernel_size, pool_stride, dropout: float = 0.0):\n",
    "        super(ConvBNReLUPool, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalableCnn1(nn.Module):\n",
    "    def __init__(self, compound_scales: tuple = (1, 1, 1)):\n",
    "        super(ScalableCnn1, self).__init__()\n",
    "        alpha, beta, gamma = compound_scales[0], compound_scales[1], compound_scales[2]\n",
    "        \n",
    "        initial_conv_inputs = [1, 32, 64, 64]\n",
    "        initial_conv_outputs = [32, 64, 64, 64]\n",
    "        initial_nb_conv = 4\n",
    "        initial_dense_inputs = [1344]\n",
    "        initial_dense_outputs = [10]\n",
    "        initial_nb_dense = 1\n",
    "        initial_resolution = (64, 173)\n",
    "        \n",
    "        # Apply compound scaling\n",
    "        # depth ----\n",
    "        scaled_nb_conv = np.floor(initial_nb_conv * alpha)\n",
    "        scaled_nb_dense = np.floor(initial_nb_dense * alpha)\n",
    "        \n",
    "        if scaled_nb_conv != initial_nb_conv:  # Another conv layer must be created\n",
    "            print(\"More conv layer must be created\")\n",
    "            gaps = np.array(initial_conv_outputs) - np.array(initial_conv_inputs) # average filter gap\n",
    "            avg_gap = gaps.mean()\n",
    "            \n",
    "            while len(initial_conv_inputs) < scaled_nb_conv:\n",
    "                initial_conv_outputs.append(int(np.floor(initial_conv_outputs[-1] + avg_gap)))\n",
    "                initial_conv_inputs.append(initial_conv_outputs[-2])\n",
    "                \n",
    "            print(\"new conv layers:\")\n",
    "            print(\"inputs: \", initial_conv_inputs)\n",
    "            print(\"ouputs: \", initial_conv_outputs)\n",
    "            \n",
    "        if scaled_nb_dense != initial_nb_dense:  # Another dense layer must be created\n",
    "            print(\"More dense layer must be created\")\n",
    "            dense_list = np.linspace(initial_dense_inputs[0], initial_dense_outputs[-1], scaled_nb_dense+1)\n",
    "            initial_dense_inputs = dense_list[:-1]\n",
    "            initial_dense_outputs = dense_list[1:]\n",
    "            \n",
    "            print(\"new dense layers:\")\n",
    "            print(\"inputs: \", initial_dense_inputs)\n",
    "            print(\"ouputs: \", initial_dense_outputs)\n",
    "                \n",
    "        # width ----\n",
    "        scaled_conv_inputs = [int(np.floor(i * beta)) for i in initial_conv_inputs]\n",
    "        scaled_conv_outputs = [int(np.floor(i * beta)) for i in initial_conv_outputs]\n",
    "        scaled_dense_inputs = [int(np.floor(i * beta)) for i in initial_dense_inputs]\n",
    "        scaled_dense_outputs = [int(np.floor(i * beta)) for i in initial_dense_outputs]\n",
    "        \n",
    "        # Check how many conv with pooling layer can be used\n",
    "        nb_max_pooling = np.min([np.log2(initial_resolution[0]), int(np.log2(initial_resolution[1]))])\n",
    "        nb_model_pooling = len(scaled_conv_inputs)\n",
    "        \n",
    "        if nb_model_pooling > nb_max_pooling:\n",
    "            nb_model_pooling = nb_max_pooling\n",
    "        \n",
    "        # fixe initial and final conv & linear input\n",
    "        scaled_conv_inputs[0] = 1\n",
    "        scaled_dense_inputs[0] = self.calc_initial_dense_input(initial_resolution, nb_model_pooling, scaled_conv_outputs)\n",
    "        scaled_dense_outputs[-1] = 10\n",
    "        \n",
    "        # ======== Create the convolution part ========\n",
    "        features = []\n",
    "        \n",
    "        # Create the layers\n",
    "        for idx, (inp, out) in enumerate(zip(scaled_conv_inputs, scaled_conv_outputs)):\n",
    "            if idx < nb_model_pooling:\n",
    "                dropout = 0.3 if idx != 0 else 0.0\n",
    "                features.append(ConvBNReLUPool( inp, out, 3, 1, 1, (2, 2), (2, 2), dropout))\n",
    "            \n",
    "            else:\n",
    "                features.append(ConvReLU(inp, out, 3, 1, 1))\n",
    "            \n",
    "        self.features = nn.Sequential(\n",
    "            *features,\n",
    "        )\n",
    "\n",
    "        # ======== Craete the classifier part ========\n",
    "        linears = []\n",
    "        for inp, out in zip(scaled_dense_inputs[:-1], scaled_dense_outputs[:-1]):\n",
    "            print(inp, out)\n",
    "            linears.append(nn.Linear(inp, out))\n",
    "            linears.append(nn.ReLU6(inplace=True))\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            *linears,\n",
    "            nn.Linear(scaled_dense_inputs[-1], scaled_dense_outputs[-1])\n",
    "        )\n",
    "                      \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def calc_initial_dense_input(self, initial_resolution, nb_model_pooling, conv_outputs):\n",
    "        dim1 = initial_resolution[0]\n",
    "        dim2 = initial_resolution[1]\n",
    "        \n",
    "        for i in range(int(nb_model_pooling)):\n",
    "            dim1 = dim1 // 2\n",
    "            dim2 = dim2 // 2\n",
    "            \n",
    "        return dim1 * dim2 * conv_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== Training ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---- Efficient net ----\n",
    "# model_func = EfficientNet\n",
    "# m1 = EfficientNet(\n",
    "#     conv_in_size= [1, 8, 16, 24, 40],\n",
    "#     conv_out_size= [8, 16, 24, 40, 40],\n",
    "#     t = [1, 6, 6, 4, 6, 6],\n",
    "#     s = [1, 2, 2, 2, 2, 1],\n",
    "#     n = [1, 3, 3, 1, 1, 1]\n",
    "# )\n",
    "\n",
    "# ---- Cnn with dropout ----\n",
    "# model_func = cnn_d\n",
    "# m1 = model_func()\n",
    "\n",
    "# ---- cnn ----\n",
    "# m1 = cnn()\n",
    "\n",
    "# ---- ScallableCNN ----\n",
    "model_func = ScalableCnn1\n",
    "m1 = model_func(valid_scaling[0])\n",
    "\n",
    "# Just trying the different model generated\n",
    "# model_func = ScalableCnn1\n",
    "# for compound_scaler in valid_scaling:\n",
    "#     m1 = model_func(compound_scaler)\n",
    "\n",
    "#     #m1 = m1.cuda()\n",
    "#     print(m1.features)\n",
    "#     print(m1.classifier)\n",
    "    \n",
    "#     from torchsummaryX import summary\n",
    "#     input_tensor = torch.zeros((100, 64, 173), dtype=torch.float)\n",
    "#     #input_tensor = input_tensor.cuda()\n",
    "\n",
    "#     s = summary(m1, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================\n",
      "                               Kernel Shape        Output Shape   Params  \\\n",
      "Layer                                                                      \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [100, 32, 64, 173]    320.0   \n",
      "1_features.0.BatchNorm2d_1             [32]  [100, 32, 64, 173]     64.0   \n",
      "2_features.0.Dropout2d_2                  -  [100, 32, 64, 173]        -   \n",
      "3_features.0.ReLU6_3                      -  [100, 32, 64, 173]        -   \n",
      "4_features.0.MaxPool2d_4                  -   [100, 32, 32, 86]        -   \n",
      "5_features.1.Conv2d_0        [32, 64, 3, 3]   [100, 64, 32, 86]  18.496k   \n",
      "6_features.1.BatchNorm2d_1             [64]   [100, 64, 32, 86]    128.0   \n",
      "7_features.1.Dropout2d_2                  -   [100, 64, 32, 86]        -   \n",
      "8_features.1.ReLU6_3                      -   [100, 64, 32, 86]        -   \n",
      "9_features.1.MaxPool2d_4                  -   [100, 64, 16, 43]        -   \n",
      "10_features.2.Conv2d_0       [64, 64, 3, 3]   [100, 64, 16, 43]  36.928k   \n",
      "11_features.2.BatchNorm2d_1            [64]   [100, 64, 16, 43]    128.0   \n",
      "12_features.2.Dropout2d_2                 -   [100, 64, 16, 43]        -   \n",
      "13_features.2.ReLU6_3                     -   [100, 64, 16, 43]        -   \n",
      "14_features.2.MaxPool2d_4                 -    [100, 64, 8, 21]        -   \n",
      "15_features.3.Conv2d_0       [64, 64, 3, 3]    [100, 64, 8, 21]  36.928k   \n",
      "16_features.3.BatchNorm2d_1            [64]    [100, 64, 8, 21]    128.0   \n",
      "17_features.3.Dropout2d_2                 -    [100, 64, 8, 21]        -   \n",
      "18_features.3.ReLU6_3                     -    [100, 64, 8, 21]        -   \n",
      "19_features.3.MaxPool2d_4                 -    [100, 64, 4, 10]        -   \n",
      "20_classifier.Flatten_0                   -         [100, 2560]        -   \n",
      "21_classifier.Dropout_1                   -         [100, 2560]        -   \n",
      "22_classifier.Linear_2           [2560, 10]           [100, 10]   25.61k   \n",
      "\n",
      "                              Mult-Adds  \n",
      "Layer                                    \n",
      "0_features.0.Conv2d_0         3.188736M  \n",
      "1_features.0.BatchNorm2d_1         32.0  \n",
      "2_features.0.Dropout2d_2              -  \n",
      "3_features.0.ReLU6_3                  -  \n",
      "4_features.0.MaxPool2d_4              -  \n",
      "5_features.1.Conv2d_0        50.724864M  \n",
      "6_features.1.BatchNorm2d_1         64.0  \n",
      "7_features.1.Dropout2d_2              -  \n",
      "8_features.1.ReLU6_3                  -  \n",
      "9_features.1.MaxPool2d_4              -  \n",
      "10_features.2.Conv2d_0       25.362432M  \n",
      "11_features.2.BatchNorm2d_1        64.0  \n",
      "12_features.2.Dropout2d_2             -  \n",
      "13_features.2.ReLU6_3                 -  \n",
      "14_features.2.MaxPool2d_4             -  \n",
      "15_features.3.Conv2d_0        6.193152M  \n",
      "16_features.3.BatchNorm2d_1        64.0  \n",
      "17_features.3.Dropout2d_2             -  \n",
      "18_features.3.ReLU6_3                 -  \n",
      "19_features.3.MaxPool2d_4             -  \n",
      "20_classifier.Flatten_0               -  \n",
      "21_classifier.Dropout_1               -  \n",
      "22_classifier.Linear_2            25.6k  \n",
      "------------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             118.73k\n",
      "Trainable params         118.73k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             85.495008M\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((100, 64, 173), dtype=torch.float)\n",
    "s = summary(m1, input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bddc8843a949fdbc5d850224b63d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/datasetManager.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # calc fold class distribution\n",
      "../src/datasetManager.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_class = meta.loc[meta.classID == c_idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9fd978de9c42e78f8eaf83e626f9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "\n",
    "dataset = DatasetManager(metadata_root, audio_root, subsampling=1.0, subsampling_method=\"balance\", verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "m1 = model_func()\n",
    "m1.cuda()\n",
    "\n",
    "# loss and optimizer\n",
    "criterion_bce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     m1.parameters(),\n",
    "#     weight_decay=1e-3,\n",
    "#     lr=0.05\n",
    "# )\n",
    "optimizer = torch.optim.AdamW(m1.parameters(), weight_decay=1e-3)\n",
    "\n",
    "# Augmentation to use\n",
    "augments = []\n",
    "\n",
    "# train and val loaders\n",
    "train_dataset = Generator(dataset, train=True, val=False, augments=augments, cached=True)\n",
    "val_dataset = Generator(dataset, train=False, val=True, cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epoch = 50\n",
    "batch_size = 64\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# scheduler\n",
    "lr_lambda = lambda epoch: 0.05 * (np.cos(np.pi * epoch / nb_epoch) + 1)\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "callbacks = [lr_scheduler]\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"%s_%s_Cosd-lr_sgd-0.01lr-wd0.001_%de_no_augment\" % ( get_datetime(), model_func.__name__, nb_epoch )\n",
    "tensorboard = SummaryWriter(log_dir=\"tensorboard/%s\" % title, comment=model_func.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5618a55a560a464fbf644f51b9df9d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, 100% \t ce: 1.8092 - acc: 0.2450 - ce val: 2.0073 - acc val: 0.4362 - took: 56.18s\n",
      "Epoch 2, 100% \t ce: 1.8209 - acc: 0.3615 - ce val: 1.2287 - acc val: 0.4949 - took: 2.97s\n",
      "Epoch 3, 100% \t ce: 2.0038 - acc: 0.4219 - ce val: 1.1975 - acc val: 0.5049 - took: 2.89s\n",
      "Epoch 4, 100% \t ce: 1.5419 - acc: 0.4742 - ce val: 1.3238 - acc val: 0.4940 - took: 2.86s\n",
      "Epoch 5, 100% \t ce: 1.2307 - acc: 0.5076 - ce val: 1.3203 - acc val: 0.5118 - took: 2.77s\n",
      "Epoch 6, 100% \t ce: 1.4609 - acc: 0.5327 - ce val: 1.8849 - acc val: 0.5254 - took: 2.81s\n",
      "Epoch 7, 100% \t ce: 1.1705 - acc: 0.5530 - ce val: 2.0343 - acc val: 0.5400 - took: 2.87s\n",
      "Epoch 8, 100% \t ce: 1.0723 - acc: 0.5720 - ce val: 0.8782 - acc val: 0.5518 - took: 2.81s\n",
      "Epoch 9, 100% \t ce: 0.9288 - acc: 0.5978 - ce val: 2.3403 - acc val: 0.5654 - took: 2.79s\n",
      "Epoch 10, 100% \t ce: 1.0271 - acc: 0.6200 - ce val: 1.6650 - acc val: 0.5967 - took: 2.80s\n",
      "Epoch 11, 100% \t ce: 1.2452 - acc: 0.6324 - ce val: 1.8373 - acc val: 0.6054 - took: 2.90s\n",
      "Epoch 12, 100% \t ce: 1.0719 - acc: 0.6376 - ce val: 1.9183 - acc val: 0.6257 - took: 2.86s\n",
      "Epoch 13, 100% \t ce: 0.8961 - acc: 0.6626 - ce val: 1.3605 - acc val: 0.6475 - took: 2.86s\n",
      "Epoch 14, 100% \t ce: 0.4768 - acc: 0.6706 - ce val: 0.3672 - acc val: 0.6763 - took: 2.82s\n",
      "Epoch 15, 100% \t ce: 0.7694 - acc: 0.6806 - ce val: 1.7041 - acc val: 0.6458 - took: 2.84s\n",
      "Epoch 16, 100% \t ce: 1.0587 - acc: 0.6976 - ce val: 0.5267 - acc val: 0.7009 - took: 2.90s\n",
      "Epoch 17, 100% \t ce: 0.6099 - acc: 0.7073 - ce val: 1.2700 - acc val: 0.6866 - took: 2.86s\n",
      "Epoch 18, 100% \t ce: 0.5191 - acc: 0.7138 - ce val: 0.6662 - acc val: 0.6821 - took: 2.90s\n",
      "Epoch 19, 100% \t ce: 0.6054 - acc: 0.7260 - ce val: 0.8505 - acc val: 0.6866 - took: 2.87s\n",
      "Epoch 20, 100% \t ce: 0.7555 - acc: 0.7353 - ce val: 1.8444 - acc val: 0.6868 - took: 3.04s\n",
      "Epoch 21, 100% \t ce: 0.5845 - acc: 0.7425 - ce val: 0.9182 - acc val: 0.6748 - took: 2.87s\n",
      "Epoch 22, 100% \t ce: 0.7357 - acc: 0.7466 - ce val: 1.2040 - acc val: 0.6877 - took: 2.97s\n",
      "Epoch 23, 100% \t ce: 1.1137 - acc: 0.7510 - ce val: 0.9869 - acc val: 0.7156 - took: 2.92s\n",
      "Epoch 24, 100% \t ce: 0.5798 - acc: 0.7577 - ce val: 0.6245 - acc val: 0.7277 - took: 2.89s\n",
      "Epoch 25, 100% \t ce: 0.5823 - acc: 0.7661 - ce val: 0.3771 - acc val: 0.7165 - took: 2.98s\n",
      "Epoch 26, 100% \t ce: 0.7250 - acc: 0.7722 - ce val: 0.3352 - acc val: 0.7234 - took: 2.97s\n",
      "Epoch 27, 100% \t ce: 0.7373 - acc: 0.7791 - ce val: 0.4344 - acc val: 0.7112 - took: 2.93s\n",
      "Epoch 28, 100% \t ce: 0.4105 - acc: 0.7852 - ce val: 0.3555 - acc val: 0.7310 - took: 2.95s\n",
      "Epoch 29, 100% \t ce: 0.5269 - acc: 0.7892 - ce val: 0.6346 - acc val: 0.7179 - took: 3.02s\n",
      "Epoch 30, 100% \t ce: 0.8006 - acc: 0.7885 - ce val: 3.2952 - acc val: 0.6804 - took: 2.87s\n",
      "Epoch 31, 100% \t ce: 0.6059 - acc: 0.8035 - ce val: 0.8191 - acc val: 0.7080 - took: 2.97s\n",
      "Epoch 32, 100% \t ce: 0.7416 - acc: 0.7965 - ce val: 0.1869 - acc val: 0.7199 - took: 2.87s\n",
      "Epoch 33, 100% \t ce: 0.6027 - acc: 0.8041 - ce val: 0.5534 - acc val: 0.7511 - took: 2.90s\n",
      "Epoch 34, 100% \t ce: 0.2622 - acc: 0.8058 - ce val: 1.6227 - acc val: 0.7080 - took: 2.91s\n",
      "Epoch 35, 100% \t ce: 0.1916 - acc: 0.8158 - ce val: 0.7349 - acc val: 0.7259 - took: 2.89s\n",
      "Epoch 36, 100% \t ce: 0.5994 - acc: 0.8172 - ce val: 1.0694 - acc val: 0.7183 - took: 2.83s\n",
      "Epoch 37, 100% \t ce: 0.3919 - acc: 0.8203 - ce val: 0.2136 - acc val: 0.7522 - took: 2.87s\n",
      "Epoch 38, 100% \t ce: 1.0045 - acc: 0.8175 - ce val: 1.5129 - acc val: 0.7212 - took: 3.01s\n",
      "Epoch 39, 100% \t ce: 0.8145 - acc: 0.8267 - ce val: 3.8495 - acc val: 0.6984 - took: 2.91s\n",
      "Epoch 40, 100% \t ce: 0.8052 - acc: 0.8257 - ce val: 4.1109 - acc val: 0.7038 - took: 2.86s\n",
      "Epoch 41, 100% \t ce: 0.3015 - acc: 0.8353 - ce val: 1.8002 - acc val: 0.6826 - took: 2.92s\n",
      "Epoch 42, 100% \t ce: 0.4732 - acc: 0.8294 - ce val: 1.3482 - acc val: 0.7248 - took: 2.97s\n",
      "Epoch 43, 100% \t ce: 0.4640 - acc: 0.8323 - ce val: 0.2190 - acc val: 0.7612 - took: 2.88s\n",
      "Epoch 44, 100% \t ce: 0.5799 - acc: 0.8398 - ce val: 1.8964 - acc val: 0.7426 - took: 3.01s\n",
      "Epoch 45, 100% \t ce: 0.3279 - acc: 0.8331 - ce val: 1.6551 - acc val: 0.7337 - took: 2.84s\n",
      "Epoch 46, 100% \t ce: 0.3146 - acc: 0.8407 - ce val: 1.0458 - acc val: 0.7424 - took: 2.94s\n",
      "Epoch 47, 100% \t ce: 0.4183 - acc: 0.8442 - ce val: 0.2875 - acc val: 0.7679 - took: 2.93s\n",
      "Epoch 48, 100% \t ce: 0.2438 - acc: 0.8491 - ce val: 0.4829 - acc val: 0.7359 - took: 2.84s\n",
      "Epoch 49, 100% \t ce: 0.4415 - acc: 0.8480 - ce val: 1.9904 - acc val: 0.7270 - took: 2.84s\n",
      "Epoch 50, 100% \t ce: 0.3765 - acc: 0.8429 - ce val: 0.3499 - acc val: 0.7402 - took: 2.94s\r"
     ]
    }
   ],
   "source": [
    "acc_func = CategoricalAccuracy()\n",
    "\n",
    "for epoch in tqdm.tqdm_notebook(range(nb_epoch)):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    acc_func.reset()\n",
    "\n",
    "    m1.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        # Transfer to GPU\n",
    "        X = X.cuda().float()\n",
    "        y = y.cuda().long()\n",
    "        \n",
    "        # predict\n",
    "        logits = m1(X)\n",
    "\n",
    "        weak_loss = criterion_bce(logits, y)\n",
    "\n",
    "        total_loss = weak_loss\n",
    "\n",
    "        # calc metrics\n",
    "#         y_pred = torch.log_softmax(logits, dim=1)\n",
    "        _, y_pred = torch.max(logits, 1)\n",
    "        acc = acc_func(y_pred, y)\n",
    "\n",
    "        # ======== back propagation ========\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== history ========\n",
    "        print(\"Epoch {}, {:d}% \\t ce: {:.4f} - acc: {:.4f} - took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            total_loss.item(),\n",
    "            acc,\n",
    "            time.time() - start_time\n",
    "        ),end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\n",
    "    tensorboard.add_scalar('train/ce', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", 100. * acc, epoch )\n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # reset metrics\n",
    "        acc_func.reset()\n",
    "        m1.eval()\n",
    "\n",
    "        for X_val, y_val in val_loader:\n",
    "            # Transfer to GPU\n",
    "            X_val = X_val.cuda().float()\n",
    "            y_val = y_val.cuda().long()\n",
    "\n",
    "\n",
    "#             y_weak_val_pred, _ = model(X_val)\n",
    "            logits = m1(X_val)\n",
    "\n",
    "            # calc loss\n",
    "            weak_loss_val = criterion_bce(logits, y_val)\n",
    "\n",
    "            # metrics\n",
    "#             y_val_pred =torch.log_softmax(logits, dim=1)\n",
    "            _, y_val_pred = torch.max(logits, 1)\n",
    "            acc_val = acc_func(y_val_pred, y_val)\n",
    "\n",
    "            #Print statistics\n",
    "            print(\"Epoch {}, {:d}% \\t ce: {:.4f} - acc: {:.4f} - ce val: {:.4f} - acc val: {:.4f} - took: {:.2f}s\".format(\n",
    "                epoch+1,\n",
    "                int(100 * (i+1) / nb_batch),\n",
    "                total_loss.item(),\n",
    "                acc,\n",
    "                weak_loss_val.item(),\n",
    "                acc_val,\n",
    "                time.time() - start_time\n",
    "            ),end=\"\\r\")\n",
    "\n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('validation/ce', weak_loss_val.item(), epoch)\n",
    "        tensorboard.add_scalar(\"validation/acc\", 100. * acc_val, epoch )\n",
    "\n",
    "    for callback in callbacks:\n",
    "        callback.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.6"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.8*49 + 25.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12.13*50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
