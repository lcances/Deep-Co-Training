{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from util.utils import reset_seed, get_datetime, get_model_from_name, ZipCycle\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from util.checkpoint import CheckPoint\n",
    "\n",
    "from UrbanSound8k.ramps import Warmup, sigmoid_rampup, sigmoid_rampdown\n",
    "from UrbanSound8k.losses import loss_cot, loss_diff, loss_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../../datasets/ubs8k\", type=str)\n",
    "parser.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
    "parser.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "parser.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "parser.add_argument(\"--model\", default=\"cnn03\", type=str)\n",
    "parser.add_argument(\"--batch_size\", default=100, type=int)\n",
    "parser.add_argument(\"--nb_epoch\", default=5000, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=0.0005, type=int)\n",
    "\n",
    "parser.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "parser.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "parser.add_argument(\"--warmup_length\", default=1500, type=int) # kept for retrocompatibility, equal nb epoch \n",
    "parser.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "parser.add_argument(\"--augment\", action=\"append\", help=\"augmentation. use as if python script\")\n",
    "parser.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "parser.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "\n",
    "parser.add_argument(\"--checkpoint_path\", default=\"../../model_save/ubs8k/deep-co-training_independant-loss\", type=str)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--tensorboard_path\", default=\"../../tensorboard/ubs8k/deep-co-training_independant-loss\", type=str)\n",
    "parser.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.warmup_length = args.nb_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08000a63182648068897fe92577a0de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "audio_root = os.path.join(args.dataset_root, \"audio\")\n",
    "metadata_root = os.path.join(args.dataset_root, \"metadata\")\n",
    "all_folds = args.train_folds + args.val_folds\n",
    "\n",
    "manager = DatasetManager(\n",
    "    metadata_root, audio_root,\n",
    "    folds=all_folds,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the sampler with the specified number of supervised file\n",
    "train_dataset = Dataset(manager, folds=args.train_folds, cached=True)\n",
    "val_dataset = Dataset(manager, folds=args.val_folds, cached=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = get_model_from_name(args.model)\n",
    "\n",
    "m1, m2 = model_func(manager=manager), model_func(manager=manager)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-80, clip_max=0, targeted=True\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-80, clip_max=0, targeted=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "rules = {\n",
    "    0: {\"lsup\": 1.0, \"lcot\": 0.0, \"ldiff\": 0.0},\n",
    "    40: {\"lsup\": 0.6, \"lcot\": 0.3, \"ldiff\": 0.1},\n",
    "    80: {\"lsup\": 0.35, \"lcot\": 0.5, \"ldiff\": 0.15},\n",
    "}\n",
    "\n",
    "def uniform_rule(step: int = 0):\n",
    "    sup_steps = [0.34 for _ in range(step)]\n",
    "    cot_steps = [0.33 for _ in range(step)]\n",
    "    diff_steps = [0.33 for _ in range(step)]\n",
    "    \n",
    "    # making the rules\n",
    "    hop_length = np.linspace(0, args.warmup_length, step)\n",
    "    \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def weighted_uniform_rule(step: int = 10):\n",
    "    lcm = args.lambda_cot_max\n",
    "    ldm = args.lambda_diff_max\n",
    "    lsm = 1\n",
    "    \n",
    "    sup_steps = [lsm * 0.34 for _ in range(step)]\n",
    "    cot_steps = [lcm * 0.33 for _ in range(step)]\n",
    "    diff_steps = [ldm * 0.33 for _ in range(step)]\n",
    "    \n",
    "    # normalize\n",
    "    for i in range(step):\n",
    "        summed = sup_steps[i] + cot_steps[i] + diff_steps[i]\n",
    "        sup_steps[i] /= summed\n",
    "        cot_steps[i] /= summed\n",
    "        diff_steps[i] /= summed\n",
    "    \n",
    "    # making the rules\n",
    "    hop_length = np.linspace(0, args.warmup_length, step)\n",
    "    \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "    \n",
    "\n",
    "def rule_maker_weighted_linear(step: int = 10):\n",
    "    lcm = args.lambda_cot_max\n",
    "    ldm = args.lambda_diff_max\n",
    "    lsm = 1\n",
    "    total = lcm + ldm + lsm\n",
    "    \n",
    "    # normalize\n",
    "    lcm /= total\n",
    "    ldm /= total\n",
    "    lsm /= total\n",
    "    \n",
    "    sup_steps = np.linspace(1, lsm, step)\n",
    "    cot_steps = np.linspace(0, lcm, step)\n",
    "    diff_steps = np.linspace(0, ldm, step)\n",
    "    \n",
    "    # making the rules\n",
    "    hop_length = np.linspace(0, args.warmup_length, step)\n",
    "    \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def rule_maker_linear(step: int = 10):\n",
    "    hop_length = np.linspace(0, args.nb_epoch, step)\n",
    "    \n",
    "    sup_steps = np.linspace(1, 0, step)\n",
    "    cot_steps = np.linspace(0, 0.5, step)\n",
    "    diff_steps = np.linspace(0, 0.5, step)\n",
    "    \n",
    "    # normalize\n",
    "#     for i in range(step):\n",
    "#         summed = sup_steps[i] + cot_steps[i] + diff_steps[i]\n",
    "#         sup_steps[i] /= summed\n",
    "#         cot_steps[i] /= summed\n",
    "#         diff_steps[i] /= summed\n",
    "        \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def rule_maker_cosine(step: int = 10, cycle: int = 1):\n",
    "    hop_length = np.linspace(0, args.nb_epoch, step * cycle)\n",
    "    \n",
    "    sup_steps = 0.5 * (np.cos(np.pi * hop_length / (args.nb_epoch / cycle)) + 1)\n",
    "    cot_steps = 1 - (0.5 * (np.cos(np.pi * hop_length / (args.nb_epoch / cycle)) + 1))\n",
    "    diff_steps =  1 - (0.5 * (np.cos(np.pi * hop_length / (args.nb_epoch / cycle)) + 1))\n",
    "    \n",
    "    # normalize\n",
    "    for i in range(step * cycle):\n",
    "        summed = sup_steps[i] + cot_steps[i] + diff_steps[i]\n",
    "        sup_steps[i] /= summed\n",
    "        cot_steps[i] /= summed\n",
    "        diff_steps[i] /= summed\n",
    "    \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def rule_maker_weighted_cosine(step: int = 10, cycle: int = 1):\n",
    "    lcm = args.lambda_cot_max\n",
    "    ldm = args.lambda_diff_max\n",
    "    lsm = 1\n",
    "    \n",
    "    hop_length = np.linspace(0, args.nb_epoch, step * cycle)\n",
    "    \n",
    "    sup_steps = 0.5 * (np.cos(np.pi * hop_length / (args.nb_epoch / cycle)) + 1)\n",
    "    cot_steps = 1 - (0.5 * (np.cos(np.pi * hop_length / (args.nb_epoch / cycle)) + 1))\n",
    "    diff_steps =  1 - (0.5 * (np.cos(np.pi * hop_length / (args.nb_epoch / cycle)) + 1))\n",
    "    \n",
    "    sup_steps *= lsm\n",
    "    cot_steps *= lcm\n",
    "    diff_steps *= ldm\n",
    "    \n",
    "    # normalize\n",
    "    for i in range(step * cycle):\n",
    "        summed = sup_steps[i] + cot_steps[i] + diff_steps[i]\n",
    "        sup_steps[i] /= summed\n",
    "        cot_steps[i] /= summed\n",
    "        diff_steps[i] /= summed\n",
    "    \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def rule_maker_sigmoid(step: int = 10):\n",
    "    hop_length = np.linspace(0, args.nb_epoch, step)\n",
    "    \n",
    "    sup_steps = np.asarray([sigmoid_rampdown(x, args.nb_epoch) for x in hop_length])\n",
    "    cot_steps = np.asarray([sigmoid_rampup(x, args.nb_epoch) for x in hop_length])\n",
    "    diff_steps = np.asarray([sigmoid_rampup(x, args.nb_epoch) for x in hop_length])\n",
    "    \n",
    "    # normalize\n",
    "    for i in range(step):\n",
    "        summed = sup_steps[i] + cot_steps[i] + diff_steps[i]\n",
    "        sup_steps[i] /= summed\n",
    "        cot_steps[i] /= summed\n",
    "        diff_steps[i] /= summed\n",
    "    \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "\n",
    "def rule_maker_weighted_sigmoid(step: int = 10):\n",
    "    lcm = args.lambda_cot_max\n",
    "    ldm = args.lambda_diff_max\n",
    "    lsm = 1\n",
    "\n",
    "    hop_length = np.linspace(0, args.nb_epoch, step)\n",
    "    \n",
    "    sup_steps = np.asarray([lsm * sigmoid_rampdown(x, args.nb_epoch) for x in hop_length])\n",
    "    cot_steps = np.asarray([lcm * sigmoid_rampup(x, args.nb_epoch) for x in hop_length])\n",
    "    diff_steps = np.asarray([ldm * sigmoid_rampup(x, args.nb_epoch) for x in hop_length])\n",
    "    \n",
    "    # normalize\n",
    "    for i in range(step):\n",
    "        summed = sup_steps[i] + cot_steps[i] + diff_steps[i]\n",
    "        sup_steps[i] /= summed\n",
    "        cot_steps[i] /= summed\n",
    "        diff_steps[i] /= summed\n",
    "    \n",
    "    rules = dict()\n",
    "    for i, epoch in enumerate(hop_length):\n",
    "        rules[epoch] = {\"lsup\": sup_steps[i], \"lcot\": cot_steps[i], \"ldiff\": diff_steps[i]}\n",
    "        \n",
    "    return rules\n",
    "\n",
    "\n",
    "def loss_chooser(epoch):\n",
    "    for k in reversed(rules.keys()):\n",
    "        if epoch >= k:\n",
    "            chance = list(rules[k].values())\n",
    "            break\n",
    "    \n",
    "    loss_function = [\"sup\", \"cot\", \"diff\"]\n",
    "    return np.random.choice(loss_function, p=chance)\n",
    "\n",
    "# def loss_chooser(epoch):\n",
    "#     loss_function = [\"sup\", \"cot\", \"diff\"]\n",
    "    \n",
    "#     if epoch < 100:\n",
    "#         return \"sup\"\n",
    "    \n",
    "#     return loss_function[epoch % 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "steps = 10\n",
    "rule_fn = rule_maker_weighted_linear\n",
    "rules = rule_fn(step = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAHBCAYAAAD9zmX+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3hUZfrG8e8zJYSO9Cog0luAQAIrNhRQmiiaEHpottVdf5Z1XXtbu9gJEDoEBAUsFAsqKgECiPTeIkgXaYEk8/7+OIMbYyAZMpkzyTyf65qL5OTMnDsB5s5p7yvGGJRSSqlQ4LA7gFJKKRUoWnpKKaVChpaeUkqpkKGlp5RSKmRo6SmllAoZWnpKKaVChpaeKlJEZLCIGBG5Nh+vYURkgv9S5U9e84jItd51B2dZVse77KkCjFgo5fTzUkWflp5S+SAit2ihKFV4uOwOoJSfTQaSgHMB2t4twCDgqQBtz1e7geJAht1BlAoGWnqqSDHGZAKZducIFsYacinN7hx5JSJuwGmMyXNmERGgpDHmZMElU0WFHt5UBU5Eaud0XklEFnmX/yPb8mUisiHbsmoi8r6I7BGRcyKyT0QSRKRytvVyPKfnPbc1W0R+F5HjIjJXROqKyC4R+eYCuduLyLcickpEDovIWBEpleXr32Dt5Z0/73b+MdjX3N51m4rIAu/2jorIlJzW80VO5/SyLhOR7iKyQkTSRGS/iLwiIn/5ZVhE6ovIZO8657w/t1dEpGS29RqJyHsisl5ETojIaRFZKSLDc3jNp7w5morI6yKSilXQ0Rf5fv44Dyci93j/naQBD3q/nuPfpy/n78Rylzf3ae/3sVhErsvtuSr46Z6eKnDGmN0ishPohPcwoIiEAX8DPN7lb3qXlwHaAB+cf76IXA4sBcKAccB24ErgLuA6EYk0xhy/0PZFpAKwBKjifd2NQEdgMVDyAk+LAD4FxgPTgGuBod68I7zrPI/1i2NHYECW5/7oa24RqevNWAx4B9gL9AAWXOj78oObgbuxfiaJQC+s8jgGvHB+JRFpA3wN/AaMBn4BWgL3AX8TkWuMMene1a8Frsb62e3E+vneDiSISEVjzIs55JgKnAFeAwywPw/Z/wFUAMYAv2L9vPxlMtAXmIX1918M6Ad8ISK3GmPm+XFbKtCMMfrQR4E/gLFY59lKej+/GusNbjLwO+DyLu/hXX5rlufOBQ4CNbO9ZiTWuaqnsiwb7H3+tVmWvexd1i/b888v/ybbcoNVbtHZln8GpAOlsiybgPcoYg7fsy+5p3m3e12WZQJ87F0+IQ8/42u96w7OsqyOd9lTOSw7BdTJtr11wP5sr7sG2ASUzra8dw7bK5lDLgfwDXAccGdZ/tT5n//5v38fvsejQOUcvr4r+9/nRX42OS07/z2NyPZ8F5CCVeRi9/8nfVz6Qw9vqkD5GnADV3k/vx6rEEYBpYG23uXXYRXONwAiUhboDswD0kSk4vkH1hvcNqBzLtvugbX3MD3b8lcv8pylxpjkHL4HF1ZpXJQvuUXE4c2YYoxZfP41jPVu+3Ju28qHOcaYXdm2txioev4wrog0B1pglXKxbN/H91jF2TnLa5w6/7GIhHv3sssDi4AyQKMccrxpjPH1QptJxpiDPj4nL/oDJ4A52b7XcsAnWH/39QtguypA9PCmCpSvvX9eDyz0/rkYWIV1OO16rEOB1wNrjDFHves3xNpTGOp95GRHLtuuCyw3xniyLjTGHBSR33x4zSPePyvksj3wLXdloBTW3lR2G3JY5i+5fY8ngcbez5/2PnJS5fwH3rJ8CrgDqJXDupflsGxLHrL64zl50Rjrl7ADF1mnSgFuXxUwLT0VEMaYX0VkI3C9iJQAooC/G2M8IvIt0ElEPsDaq3g9y1PF++cUYOIFXv5MAUS+2BWgcpGvZV8nL7nPrxvoyS3z8j2e//M1Lnx+8ViWj6dh7eEmAN9hHYbMwDp/+E9yvnjudB7z5uU5F/oZ5vW9ToBDQNxF1lmXx9dSQUhLTwXS11gXcfTAurjjK+/yr7AONd6E9abzdZbnbMN6Iwszxnx5idvdBVwpIo6se3veKyPLXeJrnnehN1lfch/kz3tVWTXJRzZ/2Or9MzO370NEymEV3mRjzJ3ZvnZDAeXL7ijW4dTsrsjj87cCDYBko7dAFEl6Tk8F0tdY/+aeBPYYY7ZnWV4MeBRrr2DJ+ScYY44AnwO3ishfLmX3Xl5eKZftfgJUw7oiL6sHL+WbyOakN8ef3mh9yW2sews/BSKzXhYvIgI87IeM+bEaa8/mThH5S3GIiCvL935+z1GyrVMNGFagKf9nC9BIRGpk2X4x4J48Pn8S1r/RnK4yRUSq5LRcFR66p6cCaTHWRSqNsa56BMAYs0FEfsXaq1lqjDmR7Xl3YV008Z2ITMJ6I3Zg/fbeC+uN6qmLbPclrMNV40WkHda5s6uwbpk4TP4OKyYD9wLvicj5qzuXGWN2+pj7P1h7up+KyNtAKtYecW6FXqCMMUZEBmD9YvKziCQC64ESWLdf3Ir1y8oEY8wJEVkE9BeRM8AKoDYwEuuqx7ycC82vd4BY4Evv4fIwrNtJ8nQI1RgzS0TGA/eKSGusX0YOAzWB9ljfc173GlUQ0tJTAWOMOSYiPwGt+fMhTLyfx+WwHGPMXu+9Yo9glUV/rBuS92Ltxc3MZbuHReQqrPNS8VgltxjrStEV5O+c4HSgFdYb7e1YpTYE2OlLbmPMdhHp6M34d+AsMB/rDftiF1UUOGPMTyLSCqvcegJ3Yl3huAvrl5evsqzeH/gvVmEPwjpc+BjWLwPjA5D1B+8N6P8GXsG6p/B9rNsNvrrIU7O+RryILMa6H/NRrOL8Feuiq0cLILYKILGuUlYq9Hgvpz8MjM5+DkopVTTpOT0VEkSkeA6LH/H++UUgsyil7BOwPT3vuYDuwEFjTLMcvi5YNyrfjHX8fbAxZlVAwqkizzse426sw1xOrKHPumMNGXa192ISpVQRF8g9vQlA14t8/SaskQ7qYx1Lfz8AmVTo+ARrPM3nsEY5aYp1/qyrFp5SoSOg5/REpA7w6QX29EZjjZk33fv5ZqzxE/My+KxSSimVq2C6erMGfx4pPdW77C+lJyIj8I50X7JkyTaNGuU0nF/ebTt4krT0TGpXKEnp8GD6kSillLoUK1euPGyM+cstP8H0Dp/T0E457oYaYxKwhjkiMjLSpKSk5GvDx06do/+4ZWw9cJIX+7XmxiZ6/6lSShVmIrI7p+XBdPVmKn8eoLYmsC8QG76sZBjThkXTuFpp7pqykgXr9IiqUkoVRcFUevOAgd7hmaKB44E8n1e2hJvJw6JoUbMs90xbzSdrAtK3SimlAihgpSci07GmjmkoIqkiMlRE7hSR8zcFf4411ck2rNmQ7w5UtvPKhLuZNDSK1peX4/6k1Xy8OjXQEZRSShWggJ3TM8ZkH+w3+9cNeR8UtsCUKuZiYnw7hk5I4YGZa8jINNwemdO0YEoppQqbYDq8GTRKhLlIHNyWq66syMOzf2b68j12R1JKKeUHWnoXUDzMyZiBkVzToBKPfrSWyUt32R1JKaVUPmnpXUS428noAW24oXFlHp+7nsTvd9odSSmlVD5o6eWimMvJe/3a0LVpVZ75dAMJ323P/UlKKaWCkpZeHoS5HLwd14puLarxwuebeHfxNrsjKaWUugTBNCJLUHM7HYyKicDtEF5ZuJn0TA/3d6qPNTmEUkqpwkBLzwcup4PX7ojA5XTw5pdbycg0/F/nBlp8SilVSGjp+cjpEF6+rQUuh/DO4m2kZ3r4102NtPiUUqoQ0NK7BA6H8ELv5ridDkZ/t4P0TMPj3Rtr8SmlVJDT0rtEDofwTK+muJxC4g87Sc/08HTPpjgcWnxKKRWstPTyQUR4onsT3E4HCd/tIMPj4flbmmvxKaVUkNLSyycR4dGbGuF2Cu8u3k56puGl21rg1OJTSqmgo6XnByLCg50b4v7jqk4Pr97eEpdTb4NUSqlgoqXnJyLCP25ogMshvLpoCxkewxsxEbi1+JRSKmho6fnZvdfXx+108OL8TWR6DKNiWxHm0uJTSqlgoO/GBWDkNfV4vHsT5q/7lbunruJsRqbdkZRSSqGlV2CGXlWXZ3o15cuNB7hz8krS0rX4lFLKblp6BWhg+zq80Ls5izcfYvikFM6c0+JTSik7aekVsLioy3m5Twu+33aY+AkrOH0uw+5ISikVsrT0AuCOyFq8fkdLlu08wuDEFZw8q8WnlFJ20NILkN6tajIqthUr9xxj4Lhl/J6WbnckpZQKOVp6AdSjZXXe6duKn1OPM2Dcco6f0eJTSqlA0tILsJuaV+P9/m3YsO84/cYm89vpc3ZHUkqpkKGlZ4Mbm1QhYUAkWw6cpO+YZRw5edbuSEopFRK09GxyXaPKjB0YyY5DJ4kbs4xDJ7T4lFKqoGnp2ejqBpUYP7gte46eJjZhKQd/T7M7klJKFWlaejbrcGVFJgxpy/7jacQkJLP/+Bm7IymlVJGlpRcEoq6owOSh7Th04iwxo5NJPXba7khKKVUkaekFiTa1yzNlWBTHTp8jZnQye49q8SmllL9p6QWRiFrlmD48mpNnM7hj9FJ2HT5ldySllCpStPSCTLMaZZk+PJqzGR5iEpay/dBJuyMppVSRoaUXhJpUL8P04dFkegwxo5PZeuCE3ZGUUqpI0NILUg2rliZpRDQiEJuQzMb9v9sdSSmlCj0tvSB2ZeXSzBgRjdvpIG5MMut+OW53JKWUKtS09ILcFZVKMWNkNCXCXMSNSebn1N/sjqSUUoWWll4hULtCSZJGRFOmuJt+Y5axas8xuyMppVShpKVXSNQqX4KZI9tTvlQYA8ctZ8Wuo3ZHUkqpQkdLrxCpXq44M0a0p3LpYgxKXE7yjiN2R1JKqUJFS6+QqVo2nKSR0dQoV5zB45fzw7bDdkdSSqlCQ0uvEKpcOpzpI6KpU6Ek8RNW8M3mg3ZHUkqpQkFLr5CqWKoY04ZHU69SKUZMWslXGw/YHUkppYKell4hVr5kGNOGR9GoWmnunLKShet/tTuSUkoFNS29Qq5ciTCmDIuiWY2y3DN1FZ/9vN/uSEopdWmMgcz0At2Ell4RUCbczaT4dkTUKsd9SauZ+9MvdkdSSinfeDJh3t9h1hDr4wKipVdElA53MzG+HZG1L+OfM35i9spUuyMppVTeZJy1ym71ZKjUCKTgqklLrwgpWczFhCHt6FCvIg/OWsOMFXvsjqSUUhd37hRMj4UNc6Hz83D9f0CkwDanpVfEFA9zMnZQJFfXr8Qjs9cyJXm33ZGUUipnZ47BpFtgxzfQ8x3ocG+Bb1JLrwgKdzsZPaANnRpV5j9z1jHhh512R1JKqT87cQDGd4P9P8HtE6H1gIBsVkuviAp3O3m/fxs6N6nCU59sYOySHXZHUkopy7FdkNjF+jNuBjTpGbBNa+kVYWEuB+/2a0235tV47rONvPfNNrsjKaVC3cFNkNjVOrQ5cC7Uuz6gm3cFdGsq4NxOB6NiI3A5hZcXbCYj03Bfp/p2x1JKhaJfVsKU28AZBkM+hypNAx5BSy8EuJwOXr8jAqdDeP2LLWRkevjnjQ2QArxCSiml/mTndzC9L5SoAAPnQPkrbImhpRcinA7hlT4tcTscvPX1NtI9hoe7NNTiU0oVvE2fwYdDrKIb8DGUqWZbFC29EOJ0CC/e2hyXU3j/m+2kZ3h4rFtjLT6lVMH5aTrMvQeqR0C/WVCivK1xtPRCjMMhPHdLM9xOB2O/30mGx/BkjyZafEop/0v+ABY8AnWvgdhpUKyU3Ym09EKRiPBkjya4HMLY73eSnunh2V7NcDi0+JRSfmAMfPsSfPMiNOoOt40Dd7jdqQAtvZAlIjzWrTFul8M61Jnp4cVbW+DU4lNK5YfHAwsfhWUfQEQ/6PEWOIOnaoIniQo4EeHhLg1xOx289dVWMjINr9zeUotPKXVpMjOsmRLWTIPou62xNB3BdTu4ll6IExEeuLEBrvO3M3gMr9/REpczuP6hKqWCXHoazIqHzZ/BdY/B1Q8V6MDRl0pLTwFwX6f6uJ0OXlqwiQyPh1GxrXBr8Sml8uLsCesevF1L4KaXIWqk3YkuSEtP/eGua+vhdgrPfbaRjMxVvBPXmjCXFp9S6iJOH7VGWdm/BnonQMsYuxNdlL6jqT8Z1vEKnu7ZlEUbDnDnlJWkpRfcDMZKqULu930w/iY4sB5ipgR94YGWnsrBoA51eL53M77edJARk7X4lFI5OLLdminheCr0nwWNbrY7UZ5o6akc9Yuqzcu3tWDJ1kMMnbiCM+e0+JRSXr+us2ZKOHsSBn0Cda+2O1GeaempC7qjbS1eu70lS7cfYfD45Zw6m2F3JKWU3fYuhwk3g8MF8QugRmu7E/lES09d1K2ta/JGTAQpu48xKHE5J9LS7Y6klLLLtq9gUi9rpoT4BVCpod2JfKalp3LVK6IGb8W24qe9vzFg3HKOn9HiUyrkrJ8D02KsmRKGLIDLatud6JIEtPREpKuIbBaRbSLyrxy+XlZEPhGRNSKyXkSGBDKfurBuLarxbr/WrN93nAHjlvHb6XN2R1JKBcqqSTBriHUoc/BnULqK3YkuWcBKT0ScwLvATUAToK+INMm22j3ABmNMS+Ba4DURCQtURnVxXZpW5YP+bdi0/wRxY5Zx9JQWn1JF3o9vW0OL1bvemguveDm7E+VLIPf02gHbjDE7jDHngCSgV7Z1DFBarHluSgFHAb16Ioh0alyFMYMi2X7oJHFjkjl88qzdkZRSBcEY+OoZWPQfaHILxE6HsJJ2p8q3QJZeDWBvls9TvcuyegdoDOwD1gL3G2M82V9IREaISIqIpBw6dKig8qoLuKZBJRIHt2XXkVPEJiRz8Pc0uyMppfzJ44HP/g+WvAatB0GfRHAVjYNugSy9nEYeNdk+7wL8BFQHIoB3RKTMX55kTIIxJtIYE1mpUiX/J1W5+tuVFZkwpB37fjtDbEIyvx7X4lOqSMhMh4+GQ8o4+Nv90GMUOJx2p/KbQJZeKlAry+c1sfboshoCfGQs24CdQKMA5VM+ir6iApPi23HwxFliEpbyy29n7I6klMqPc6chqR+smwWdnoQbnwnKmRLyI5CltwKoLyJ1vRenxALzsq2zB+gEICJVgIbAjgBmVD6KrFOeSUPbcfTUOWJGL2Xv0dN2R1JKXYq049bA0VsXQfc3oOMDdicqEAErPWNMBnAvsBDYCMw0xqwXkTtF5E7vas8CHURkLfAV8Igx5nCgMqpL0/ryy5g6LIoTaRnEjF7K7iOn7I6klPLFqcMwoTukLoc+4yAy3u5EBUaMyX5arXCJjIw0KSkpdsdQwLpfrHv4irmcTBsexRWVStkdSSmVm9/2wuTe1sDRMZOh/o12J/ILEVlpjInMvlxHZFF+06xGWaaPiCY900NMQjLbDp6wO5JS6mIOb7UGjj55wLoHr4gU3sVo6Sm/alS1DEkjojEGYkYns/lXLT6lgtK+n6zCy0iDwZ9C7fZ2JwoILT3ld/WrlGbGyGhcTiE2YSnr9x23O5JSKqvdP8LEHuAuDvELoVpLuxMFjJaeKhD1KpVixoj2FHc7iRuzjLWpWnxKBYUti6xzeKWrWjMlVLzS7kQBpaWnCkydiiWZMbI9pcNdxI1NZvWeY3ZHUiq0rZ0FSX2tKYGGzIeyNe1OFHBaeqpA1Spfghkj23NZiTAGjFtOyq6jdkdSKjStGAuzh0GtKGu285IV7U5kCy09VeBqlCvOzJHtqVS6GAMTl7NsxxG7IykVOoyxxtD87P+gQRfoPxvCy9qdyjZaeiogqpYNZ8aIaKqVDWfQ+OX8uE3HHFCqwBkDXzxhzZbQ/HaImWJdvBLCtPRUwFQuE07SiPbULl+SIRNW8N0WnSFDqQLjybTmwfvxLWg7DHongNNtdyrbaempgKpUuhjTR0RzRaVSDJuYwuJNB+2OpFTRk3HWmul89WS4+iG4+VVw6Ns9aOkpG5QvGcb04VE0rFqaEZNTWLT+V7sjKVV0nDsF02Nhw1zo/Dxc/58iN1NCfmjpKVuUKxHGlGFRNK1elrunrmL+2v12R1Kq8DtzDCbdAju+gZ7vQId77U4UdLT0lG3KFnczeWg7WtYqx73TV/PJmuzTKyql8uzEARjfDfb/BLdPgNYD7E4UlLT0lK1Kh7uZGN+ONrUv4/6k1Xy8OtXuSEoVPsd2Q2IXOLYL4mZAk152JwpaWnrKdqWKuZgwpC3RV1TggZlrmJmy1+5IShUeBzdZhXfmKAycC/WutztRUNPSU0GhRJiLxMFtuerKijw862emLdtjdySlgt8vK2F8VzAea1ixWm3tThT0tPRU0Ah3OxkzMJLrGlbi3x+vZdLSXXZHUip47fwOJvaEYmWsgaOrNLU7UaGgpaeCSrjbyQcD2nBjkyo8MXc9Y5fssDuSUsFn02cwpQ+UrWVNDVT+CrsTFRpaeiroFHM5ea9fa25qVpXnPtvIB99utzuSUsHjp+kwYwBUbQZDPocy1exOVKho6amg5HY6eLtvK3q0rM5/52/ina+32h1JKfstGw1z7oQ6f7MuWilR3u5EhY7L7gBKXYjL6eCNO1ridgivLtrCuUzDP2+oj+joEirUGAPfvgzfvACNusNt48AdbneqQklLTwU1l9PBK7e3xOkQ3vpqKxmZHh7q0lCLT4UOjwcW/huWvQ8t46Dn2+DUt+5LpT85FfScDuGl21rgcjp475vtZHgMj97USItPFX2ZGdZMCWumQdRd0OUFHTg6n7T0VKHgcAgv9G6G2ykkfLeDcxkenuzRRItPFV3paTB7KGz6FK57zJotQf+955uWnio0RISnezbF5XCQ+MNOMjwenunZDIdD3whUEXP2BCTFWffi3fQyRI20O1GRoaWnChUR4fHujXG7hNHf7iAj0/BC7+ZafKroOH0UptwG+9dA79HQMtbuREWKlp4qdESEf3VtRJjTwdtfbyM90/BynxY4tfhUYff7PpjcG47uhJgp0OhmuxMVOVp6qlASEf6vc0NcDgdvfLmFDI+H125vicupJ/lVIXV0B0zqZe3p9Z8Fda+2O1GRpKWnCrX7b6iPyym8snAzGR7DmzERuLX4VGHz6zqYcitkpsOgT6BGa7sTFVlaeqrQu+e6KwlzOnj+841kZHp4u29rwlxafKqQ2LscpvYBd0kYMg8qN7I7UZGm7wyqSBh+9RU82aMJC9cf4O6pKzmbkWl3JKVyt/1r65BmiQrWTAlaeAVOS08VGUP+Vpdnb2nGlxsPMmLSStLStfhUENswF6beYc2QMGQBXFbb7kQhQUtPFSkDomvz31ub893WQwybmMKZc1p8KgitmgwfDrbO3Q3+DEpXsTtRyNDSU0VObLvLeaVPS37YfpghE5Zz6myG3ZGU+p8f34Z598IV18GAj6F4ObsThRQtPVUk9WlTkzdjIli+8yiDxy/npBafspsx8NWzsOg/0OQW6JsEYSXtThVytPRUkdUrogZv923Nqj2/MWDcMn5PS7c7kgpVHg98/iAseRVaD4Q+ieAKsztVSNLSU0VatxbVeDeuNet+Oc6Ascs4flqLTwVYZjp8PAJWjIUO90GPt8DhtDtVyNLSU0Ve12ZVeb9fGzbuP0Hc2GSOnTpndyQVKtLPQFI/WPshdHoSOj+rMyXYTEtPhYQbmlQhYWAbth48Sd8xyRw5edbuSKqoSztuDRy9dRF0fwM6PmB3IoWWngoh1zasTOKgtuw6corYhGQOnkizO5Iqqk4dhok9YO8yuG0sRMbbnUh5aempkHJV/YqMH9yO1GNniE1I5sDvWnzKz46nQmJXOLQZYqdD8z52J1JZaOmpkNO+XgUmxrfjwPE0YkYvZd9vZ+yOpIqKw9tgXBc4ecC6B69BZ7sTqWy09FRIale3PJOGRnHk5DliEpay9+hpuyOpwm7/GkjsAhlpMPhTqN3B7kQqB1p6KmS1qX0ZU4ZFcfx0OrEJyew5osWnLtHuH2FCd3CFQ/xCqNbS7kTqArT0VEhrWasc04ZHc+pcBjEJS9l5+JTdkVRhs2WRNdt5qSowdCFUvNLuROoitPRUyGtWoyzTh0dzNsNDzOilbDt40u5IqrBYOwuS+kKlhtbUQGVr2p1I5UJLTymgcbUyJI2IxmMgNmEpWw6csDuSCnYrxsHsYVAryprtvGRFuxOpPNDSU8qrQZXSJI2IxiFCbEIyG/b9bnckFayWvA6fPQD1O0P/2RBe1u5EKo+09JTK4srKpZgxsj3FXA7ixiaz7pfjdkdSwcQY+OIJ+OppaH47xE4Fd3G7UykfaOkplU3diiWZMaI9JcNcxI1JZs3e3+yOpIKBJxM+uQ9+GAVth0HvBHC67U6lfKSlp1QOLq9Qghkjoylbwk3/sctYufuY3ZGUnTLOwqx4WDUJOj4IN78KDn37LIz0b02pC6h5WQlmjmxPxdLFGDhuGct3HrU7krLDuVMwPRY2zIHOz0Gnx3WmhEJMS0+pi6hWtjhJI6KpWjacQYnL+XH7YbsjqUA6c8y6B2/HN9DzHejwd7sTqXzS0lMqF1XKhJM0oj21yhcnfsIKlmw9ZHckFQgnDlijrOxbDbdPgNYD7E6k/EBLT6k8qFS6GNOHR1OnQkmGTkxh8eaDdkdSBenYbhjfFY7ugLgZ0KSX3YmUn2jpKZVHFUpZxVe/cilGTlrJlxsO2B1JFYSDm6ypgU4fgYFzod71didSfqSlp5QPLisZxrRh0TSuVpo7p6xkwbr9dkdS/vTLShh/E5hMGDIfarWzO5HyMy09pXxUtoSbycOiaFGzLPdMW80na/bZHUn5w87vYGJPKFbaGkezSlO7E6kCoKWn1CUoE+5m0tAoWl9ejvuTVjNn9S92R1L5selzmNLHGjA6fgGUv8LuRKqAaOkpdYlKFXMxMb4dUXUr8M+ZPzFrZardkdSlWJMEM/pD1WbWIc0y1e1OpAqQlp5S+VAizEXi4LZcdWVFHpq1hqTle+yOpHyxbDR8PBLq/M26aKVEebsTqQKmpadUPhUPczJmYCTXNKjEvz5ay+Tk3XZHUrkxBr55CeY/DI26Q9yH1rk8VeRp6XA2vVUAACAASURBVCnlB+FuJ6MHtOGGxpV5fM46Er/faXckdSEeDyz8N3zzArSMg9sngjvc7lQqQLT0lPKTYi4n7/VrQ9emVXnm0w0kfLfd7kgqu8wMmHcvJL8HUXdBr3fB6bI7lQogLT2l/CjM5eDtuFZ0a1GNFz7fxLuLt9kdSZ2XngYfDoKfpsJ1j0HXF3WmhBCkv+Io5Wdup4NRMRG4HcIrCzeTnunh/k71ER2Z3z5nT0BSnHUv3k0vQ9RIuxMpm2jpKVUAXE4Hr90Rgcvp4M0vt5KRafi/zg20+Oxw+ihM7QP7foLeo6FlrN2JlI0CWnoi0hUYBTiBscaY/+awzrXAm4AbOGyMuSaQGZXyF6dDePm2FrgcwjuLt5Hu8fCvro20+ALp9/3W1EBHd0DMFGh0s92JlM0CVnoi4gTeBW4EUoEVIjLPGLMhyzrlgPeArsaYPSJSOVD5lCoIDofwQu/muJ0ORn+7g/QMw+PdG2vxBcLRHTDpFmvg6P6zoO7VdidSQSCQe3rtgG3GmB0AIpIE9AI2ZFknDvjIGLMHwBij87eoQs/hEJ7p1RSXU0j8YScZHg9P9WiKw6HFV2AOrLf28DLTYdAnUKO13YlUkPDp0iURuUNEOmf5/AkRSRWRhSJSLZen1wD2Zvk81bssqwbAZSLyjYisFJGBF8gxQkRSRCTl0CGd0FMFPxHhie5NGHH1FUxaupvH5qzD4zF2xyqa9q6A8TeDOK1hxbTwVBa+Xq/71PkPRKQ18G/gLazzb6/l8tycfq3N/r/eBbQBugFdgMdFpMFfnmRMgjEm0hgTWalSpbynV8pGIsKjNzXinuvqMX35Hh6e/TOZWnz+tf1rmNTTGk4sfgFUbmR3IhVkfD28WRvY7P24NzDHGPOyiCwCFuby3FSgVpbPawLZ52RJxbp45RRwSkS+A1oCW3zMqVRQEhEe7NwQt/eqzkyP4ZU+LXA59X6xfNswF2YNhUoNof9HULqK3YlUEPL1f1oacH6Auk7Al96Pj2dZfiErgPoiUldEwoBYYF62deYCHUXEJSIlgChgo48ZlQpqIsI/bmjAQ10a8vHqX/jnzDWkZ3rsjlW4rZoMHw62DmUO/kwLT12Qr3t6S4DXROR7IBLo413egD+fr/sLY0yGiNyLtUfoBBKNMetF5E7v1z8wxmwUkQXAz4AH67aGdT5mVKpQuOe6K3E5hBfnbyIj08Oo2FaEuXSPz2c/vgOLHoN6nSBmMoSVtDuRCmJiTN7PKYhITeB94HJglDEm0bv8TcBhjLmvQFJeRGRkpElJSQn0ZpXym3Hf7+TZTzdwY5MqvBPXimIup92RCgdj4OvnYMmr0OQWuHUMuMLsTqWChIisNMZEZl/u056eMSYV6JHD8n/kI5tSIW3oVXVxO4Un5q7nzskreb9/G8LdWnwX5fFY0wKtGAOtB0H3N8ChPzOVO5+PpYhIuIj0EZFHvDeTIyL1RERnX1TqEg1sX4cXejdn8eZDDJ+UQlp6pt2RgldmujXx64ox8Lf7occoLTyVZ77ep3clsAn4AHgeOF90dwEv+zeaUqElLupyXu7Tgu+3HSZ+wgpOn8uwO1LwST8DSf1g7Uzo9CTc+Azo6DbKB77u6b0JLAKqAGeyLJ8HXOevUEqFqjsia/H6HS1J3nGEwYkrOHlWi+8Pab/DlNtg6yLrcGbHB+xOpAohX0uvA/CqMSb7sZc9QHX/RFIqtPVuVZNRsa1YuecYgxKXcyIt3e5I9jt1GCZ2h73L4LaxEBlvdyJVSF3K9dHuHJZdjnWvnlLKD3q0rM47fVuxZu9v9B+3nONnQrj4jqdCYlc4tBlip0PzPrk/R6kL8LX0FgFZjykYESkDPA185rdUSilual6N9/u3YcO+4/Qbm8xvp8/ZHSnwDm+DcV3g5AEY8DE06Jz7c5S6CF9L7wHgKhHZDIQDM4BdQFXgX/6NppS6sUkVEgZEsuXASfqOWcbRUyFUfPvXQGIXyEiDwZ9C7Q52J1JFgE+lZ4zZB0QALwGjgRTgYaC1MUanO1CqAFzXqDJjB0ay49BJ+iYkc+jEWbsjFbzdS2FCd3CFQ/xCqNbS7kSqiPBpRJZgpCOyqFDx47bDDJ2YQvVy4UwfHk3lMuF2RyoYWxbBzIFQtiYMnGP9qXKUnp5OamoqaWlpdkexTXh4ODVr1sTt/vPlJhcakSXX0hORW4FPjDHp3o8vyBjz0SVkzhctPRVKlu04wpAJK6hSJpxpw6OoVra43ZH8a+0s68bzKk2tmRJKVrQ7UVDbuXMnpUuXpkKFCkgI3q9ojOHIkSOcOHGCunXr/ulr+RmGbBbWObuD3o8vuH2sgaSVUgUk6ooKTB7ajkGJK4gZncy04VHUvKyE3bH8IyURPn3AOnfXdzqEl7U7UdBLS0ujTp06IVl4YM1YUqFCBXyZTDzXc3rGGIcx5mCWjy/00MJTKgDa1C7PlGFRHDt9jpjRyew9etruSPm35HX49J9QvzP0n62F54NQLbzzfP3+fR2G7GoR+cveoYg4ReRqn7aslLpkEbXKMX14NCfPZhAzeim7Dp+yO9KlMQa+eAK+ehqa3w6xU8FdxA7ZFnFOp5OIiAiaNWvG7bffzunTf/0lrE6dOhw+fNiGdH/l6y0Li/nfeJtZlfN+TSkVIM1qlGX68GjSMjzEJCxl+6GTdkfyjScTPrkffhgFbYdB7wRw5jT2hQpmxYsX56effmLdunWEhYXxwQcf2B3ponwtPcE6d5ddBaCQ/qqpVOHVpHoZpg+PJtNjiE1IZuuBE3ZHypuMczArHlZNhI4Pws2vgkMn0C3sOnbsyLZt2y749VOnTtGtWzdatmxJs2bNmDFjBvDnPcGUlBSuvfZaAJ566ikGDBjA9ddfT/369RkzZky+M+ZpPj0Rmef90ABTRCTrjUJOoBnwY77TKKV81rBqaZJGRNN3zDJiE5KZOjyKRlXL2B3rws6dghkDYPtX0Pk56PB3uxMVCU9/sp4N+37362s2qV6GJ3s0zdO6GRkZzJ8/n65du15wnQULFlC9enU++8wawOv48dxHr/z5559JTk7m1KlTtGrVim7dulG9+qUP9ZzXX62OeB8CHMvy+REgFWuqof6XnEIplS9XVi7NjBHRuJ0O+iYks35fkA6Fe+YYTO4NOxZDz3e08IqAM2fOEBERQWRkJJdffjlDhw694LrNmzfnyy+/5JFHHmHJkiWULZv7BUu9evWiePHiVKxYkeuuu47ly5fnK2+e9vSMMUMARGQX1iwLeihTqSBzRaVSzBgZTdyYZcSNWcbkoe1oUbOc3bH+5+RBq/AObYbbJ0CTXnYnKlLyukfmb+fP6eVFgwYNWLlyJZ9//jmPPvoonTt35oknnsDlcuHxeAD+cqN99qsz83u1qq/DkD2thadU8KpdoSRJI6IpHe6i35hlrNpzzO5IlmO7rXE0j+6AfjO18ELUvn37KFGiBP379+fBBx9k1apVgHVOb+XKlQDMnj37T8+ZO3cuaWlpHDlyhG+++Ya2bdvmK0OupSciP4vIZd6P13o/z/GRryRKKb+oVb4EM0e2p3ypMAaOW07KrqP2Bjq4yZoa6PQRGDgX6l1vbx5lm7Vr19KuXTsiIiJ4/vnn+c9//gPAk08+yf3330/Hjh1xOv98y3e7du3o1q0b0dHRPP744/k6nwd5O7w5Gzh/4crFRmRRSgWJ6uWKM2NEe+LGJDMwcTmJg9sSfUWFwAf5ZSVM6WPdijBkvjW8mCpSTp7M/VaZXbt2AdClSxe6dOnyl6937NiRLVu25PjcBg0akJCQkK+MWeVaesaYp3P6WCkV3KqWDSdpZDT9xixj8PjljBvUlr9dGcCxLHcugemxUKKCNXB0+SsCt22lLkBvjFGqCKtcOpzpI6KpU6Ek8RNW8O2WAM0AtulzmHKbNUNC/AItPHVJnnrqKR588EG/vmaue3oispacb0j/C2NMi3wnUkr5VcVSxZg2PJr+Y5cxfGIK7/dvTafGVQpug2uSYM7dUD0C+s2CEjkN4qSUPfI6y4JSqhArXzKMacOjGJi4nDunrOSduNZ0aVrV/xtaNhrmPwx1r4bYaVCstP+3oVQ++HROTylVeJUrEcaUYVEMSlzOPVNX8VbfVtzcvJp/XtwY+PZl+OYFaNQdbhsH7iI6ya0q1PScnlIhpEy4m0nx7YioVY6/T1/NvDX78v+iHg8s/LdVeC3j4PaJWngqaOl9ekqFmNLhbibGtyOy9mX8I2k1H61KvfQXy8yAefdC8nsQdRf0eheceRroSRURBTm10Jw5c9iwYYM/Yv4hL3t62e/Tm32Rh1KqEChZzMWEIe3oUK8i//fhGmau2Ov7i6SnwYeD4KepcO2/oeuLOlNCCCrIqYUKovT0Pj2lQlTxMCdjB0UycvJKHp79M+keD/2iauftyWdPQFIc7PwOur4E0XcWbFhVKHTs2JGff774Qb9Jkybx6quvIiK0aNGCyZMns3v3buLj4zl06BCVKlVi/PjxpKamMm/ePL799luee+45Zs+eTb169fKd8ZKOQ4hIPaCx99ONxpjt+U6ilAq4cLeT0QPacM/UVTz28ToyMg2DOtS5+JNOH4WpfWDfT9B7NLSMDUhWlYv5/4Jf1/r3Nas2h5v+m6dV8zK10Pr163n++ef54YcfqFixIkePWkPk3XvvvQwcOJBBgwaRmJjIfffdx5w5c+jZsyfdu3enT58+fvl2wMcLWUSkgojMAbYCc7yPLSIyV0RsGONIKZVf4W4n7/dvQ5emVXhy3nrGLtlx4ZV/3w/jb4Zf10HMZC085dPUQl9//TV9+vShYkVrZKDy5a17OJcuXUpcXBwAAwYM4Pvvvy+wvL7u6Y0FrgQ6Asu8y6KA94ExwK3+i6aUCpQwl4N34lrzj6SfeO6zjaRnGu66NtuhpKM7YFIva0+v/yzrXjwVPPK4R+ZvvkwtZIzJ09RA+Z0+6GJ8PevcBRhujPnBGJPhffwAjPR+TSlVSLmdDkbFRtArojovLdjE219t/d8XD6y3Zko4exIGzdPCU5ekU6dOzJw5kyNHjgD8cXizQ4cOJCUlATB16lSuuuoqAEqXLs2JEyf8msHX0jsE5DSf3mmsWdSVUoWYy+ng9TsiuLV1DV77YguvL9qM2bvcOqQpTmumhBpt7I6pCqmmTZvy2GOPcc0119CyZUseeOABAN566y3Gjx//x4Uto0aNAiA2NpZXXnmFVq1asX27fy4dEWPyNKymtbLIUKAfMMAY84t3WQ1gIpBkjBnrl1Q+iIyMNCkpKYHerFJFWqbH8O+P1vLLqs9JDH8Dd9lqyMC5cFker+5UAbFx40YaN26c+4pFXE4/BxFZaYyJzL7upQw4XRfYJSK/eD+vAaQBlbHO+SmlCjmnQ3ix8Q48619ha0Z1FtZ+m/vLXU7BnWlRKjB0wGml1F+tmozjk/uQmm2Ze9nTfJB8lN8cG3iyR5MCvchAqYKmA04rpf7sx3dg0WNQ73okZgqPuEuQEbaRsd/vJD3Tw7O9muFwaPGpwkkHyVNKWYyBr5+DJa9Ck1vg1gRwFUOAx7o1xu1y8P4328nINLx4a3MtPlUo+VR6IhIGPAb0BS4H3Fm/boxx+i+aUipgPB6Y/xCsGAutB0L3N8Hxv//OIsLDXRridjp466utpHs8vNKnJU4tPlXI+Lqn9ywQA7wIvAE8BNQBYoHH/ZpMKRUYmekw5y5Y+yF0uA9ufAZyOG8nIjxwYwNcDuH1L7aQkWl4/Y6WuJw6yLQqPHz913oHcKcxZjSQCcw1xtwHPAnc6O9wSqkCln4GkvpZhdfpSej8bI6Fl9V9nerzSNdGzFuzj/uTfiI90xOgsCoY5WdqoaeeeopXX30VgCeeeIIvv/wSgCVLltC0aVMiIiI4c+YMDz30EE2bNuWhhx7Kd15f9/SqAOfneTgJlPN+vAB4Kd9plFKBk3YcpveF3T9Ct9eh7YXHTMzurmvr4XaKd8gyD+/EtSbMpXt8oSjrMGT9+vXjgw8++OOmc18888wzf3w8depUHnzwQYYMGQLA6NGjOXToEMWKFct3Xl//le4Bqns/3sb/hh5rD5zJdxqlVGCcOgwTe8DeZXDbWJ8K77xhHa/g6Z5NWbThAHdNWcnZjMwCCKoKk44dO7Jt27aLrvP888/TsGFDbrjhBjZv3vzH8sGDBzNr1izGjh3LzJkzeeaZZ+jXrx89e/bk1KlTREVFMWPGjHxn9HVP72OgE5AMjAKmi8hwrBvUX8l3GqVUwTueCpNugeN7IXY6NOh8yS81qEMdXE7hsY/XMWLSSkYPaEO4W69ns8NLy19i09FNfn3NRuUb8Ui7R/K0bl6mFlq5ciVJSUmsXr2ajIwMWrduTZs2fx7WbtiwYXz//fd/mlKoVKlSeR7UOjc+lZ4x5tEsH88SkVSgA7DFGPOpXxIppQrO4W3WTAlnf4cBH0PtDvl+yX5RtXE7HDzy0c8MnbiCsQPbUjxMiy9UnJ9aCKw9vYtNLbRkyRJ69+5NiRIlAOjZs2dAMmaVr/v0jDHJWHt9Sqlgt38NTPbO/jX4U6jW0m8vfUfbWricwoMfrmHw+OUkDm5LyWJ6G3Ag5XWPzN98mVoICnbaoLzw+cyziLQWkUkikuJ9TBaR1gURTinlJ7uXwoTu4AqH+IV+Lbzzbm1dkzdiIkjZfYxBics5kZbu922owu3qq6/m448/5syZM5w4cYJPPvkk4Bl8nTm9H7ACqAZ87n1UAZaLSH//x1NK5duWRTC5N5SqAkMXQsUrC2xTvSJq8FZsK37a+xsDE5dz/IwWn/qf1q1bExMTQ0REBLfddhsdO3YMeAZfpxbaBSQYY17ItvxRYKQxpo5f0+WBTi2k1EWsnQUfj4TKTaD/R1CqUkA2u3D9r9w7bRWNq5VhUnw7ypUIC8h2Q41OLWTxZWohXw9vVgJm5rD8Q6yphZRSwSIlEWYPg1pR1jm8ABUeQJemVfmgfxs27T9B3JhlHD11LmDbVupifC29xcC1OSy/Fvg2v2GUUn6y5HX49J9QvzP0nw3hZQMeoVPjKowZFMn2QyeJG5PM4ZNnA55BqexyLT0RufX8A5gPvCgiH4jIYO/jA+AFIPBnJJVSf2YMfPEEfPU0NOsDsVPBXdy2ONc0qETi4LbsOnKKvgnJHDyRZlsWpSAP5/REJK8D6xk7ZlnQc3pKeXkyrb27VRMhcijc/Co4gmNosOQdR4ifsIKqZcKZNjyaqmXD7Y5UJGzcuJFGjRrZfhuAnYwxbNq0yX/n9Iwxjjw+9G5UpeyScQ5mxVuF1/FB6PZa0BQeQPQVFZgY344Dv6cRk7CUX37TUQv9ITw8nCNHjuDLBYlFiTGGI0eOEB6e91+ifLp6Mxjpnp4KeedOwYwBsP0r6PwcdPi73YkuaNWeYwwat5yyJdxMHx5NrfIl7I5UqKWnp5OamkpaWugeNg4PD6dmzZq43X+a3vWCe3o+l56IdAMeAZoABmvWhZeMMZ9fcup80NJTIe3MMZgWA6kroMcoawLYIPdz6m/0H7uM0uFupg2PonaFknZHUkWQX25ZEJFhWINOb8cqvn8BO4GPRSTeH0GVUnl08qA1ysovq6DP+EJReAAtapZj2vBoTp/LIGZ0MjsOnbQ7kgohvh70fwR4wBgzxBgzzvsYDDyIVYBKqUA4thsSu8DRHRA3A5reYncinzSrUZZpw6NJz/QQk5DMtoMn7I6kQoSvpXc51oSx2c0Hauc/jlIqVwc3QWJXOH0EBs6FKzvZneiSNK5WhqQR0RgDsQnJbP5Vi08VvEuZRPbGHJZ3BnbnP45S6qJ+WQXjbwJPBgz+HGq1sztRvtSvUpoZI6NxOoTYhKWs33fc7kiqiPO19F4FRonIGBEZ4r05fSzwhvdrSqmCsnOJNdt5sVIQvwCqNrM7kV/Uq1SKGSPaU9ztJG7MMtamavGpguNT6RljRgMxQGOsknsNaATcYYxJ8H88pRQAmz6HKbdB2ZrW1EAV6tmdyK/qVCzJjJHtKVXMRdzYZFbvOWZ3JFVE5bn0RMQlIjcD3xljrjLGVPA+rjLGzC3AjEqFtjUzYEZ/qNIUhsyHMtXtTlQgapUvwYyR0VxWIowB45aTsuuo3ZFUEZTn0jPGZAAfAaULLo5S6k+WJcDHI6DO32DQPChR3u5EBarmZVbxVSpdjIGJy1m244jdkVQR4+s5vTVAwc1AqZSyGAPfvgzzH4KG3SDuQygWGr9vVitbnBkjoqlWNpzB41fw47bDdkdSRYivpfcU8JqI3CIitUSkfNZHbk8Wka4isllEtonIBe/rE5G2IpIpIn18zKdU4efxwMJ/w+LnoWVfuGMSuENrgObKZcJJGtGeWuWLM2TCCr7bcsjuSKqI8LX0PgOaYx3m3AUc8j4Oe/+8IBFxAu8CN2ENYdZXRJpcYL2XgIU+ZlOq8MvMgHn3QvJ7EHUn9HoPnC67U9miUuliTB8ezRWVSjFsYgqLNx20O5IqAnz933RdPrbVDthmjNkBICJJQC+ssTuz+jswG2ibj20pVfikp8HsobDpU7j233DNwxDCU8YAVChVjOnDo+g/bhkjJqfwblxrOjetancsVYjlaU9PREqIyLvANOBDYCSw3hjzbdZHLi9TA9ib5fNU77Ks26kB9AY+yCXPCBFJEZGUQ4f0sIcqAs6egGl3WIXX9SW49pGQL7zzypUIY+qwaJpUL8vdU1cxf+1+uyOpQiyvhzefBgZjHd6cjjUqy/s+biun/8HZp3h4E3jEGJN5sRcyxiQYYyKNMZGVKlXyMYZSQeb0UZjUC3Z9D7d8ANF32p0o6JQt7mby0Ha0qFmWe6ev5pM1++yOpAqpvB7evBUYaoxJAhCRqcAPIuLMraCySAVqZfm8JpD9X24kkOSdBbgicLOIZBhj5uRxG0oVLr/vh8m9rYGjYyZDo252JwpaZcLdTBoaRfz4FdyftJoMj4ferWraHUsVMnnd06sFLDn/iTFmOZAB+HKX7AqgvojUFZEwIBaYl3UFY0xdY0wdY0wdYBZwtxaeKrKO7rBmSji+F/rP0sLLg1LFXEyIb0tU3Qo8MHMNM1P25v4kpbLIa+k5gXPZlmXgw4Uw3pvb78W6KnMjMNMYs15E7hQRPZ6jQsuB9dZMCWd/h4HzoO7VdicqNEqEuUgc3JarrqzIw7N+ZtqyPXZHUoVInmZOFxEP8AVwNsvim4BvgdPnFxhjevo7YG505nRV6OxdAVP7gLs4DPgYKje2O1GhlJaeyV1TVrJ48yGe6dWUge3r2B1JBZELzZye1z21iTksm5K/SEqFoO2LIakflKpszYV3mU5DeanC3U4+GNCGe6au5om56zmX4WFYxyvsjqWCXJ5KzxgzpKCDKFXkbZhn3YdXob61h1e6it2JCr1iLifv9WvNfdNX89xnG8nwGO68pmjNQKH8y9cRWZRSl2L1FPhwEFSLgCGfaeH5UZjLwdtxrejeohr/nb+Jd77eanckFcRCc3wjpQJp6bvWWJr1roeYKRBW0u5ERY7b6eDNmAjcTgevLtpCeqbhHzfUR/QGf5WNlp5SBcUYa9Do716BJrfArQngKmZ3qiLL5XTw6u0tcTqEUV9tJcPj4cHODbX41J9o6SlVEDwemP8wrBgDrQZAj1HgcNqdqshzOoSXb2uB2ym8u3g76ZmGR29qpMWn/qClp5S/ZabDnLth7Uzo8He48VkdRzOAHA7h+Vua43Y6SPhuB+mZHp7o3kSLTwFaekr5V/oZ+HAwbFkAnZ6Aqx7QwrOBwyE83bMpLoeDxB92kp7p4ZmezXA49O8i1GnpKeUvab/D9L6w+wfo9hq0HWZ3opAmIjzevTFupzD6ux1kZBpe6N1ciy/Eaekp5Q+nDsOUW63hxW4bC8372J1IYRXfv25qhNvp4J3F20jPNLzcpwVOLb6QpaWnVH4dT7VmSvhtD8ROgwZd7E6kshARHuzSELfTwRtfbiHD4+G121vicuptyqFIS0+p/Di8DSbfAmnHrVFWanewO5G6gPtvqI/LKbyycDMZHvPHfX0qtGjpKXWp9v9sHdI0BgZ9AtUj7E6kcnHPdVfidgovfL6JzEzDW31bEebS4gsl+ret1KXYvRQmdANnMYhfoIVXiIy4uh5PdG/CgvW/cvfUlZzNyOs82Koo0NJTyldbv7DO4ZWqbBVexfp2J1I+ir+qLs/2asqXGw8ycvJK0tK1+EKFlp5Svlg3G6bHWkU3ZAGUq2V3InWJBrSvw39vbc63Ww4xfFIKZ85p8YUCLT2l8iplPMwaCjXbweBPoVQluxOpfIptdzmv9GnJ99sOEz9hBafPZdgdSRUwLT2l8uL7N+DTf0D9G6H/bAgva3ci5Sd92tTkjTsiWLbzCIMTV3DyrBZfUaalp9TFGANfPAFfPgXN+lj34YWVsDuV8rNbWtXgrb6tWLnnGAPHLeP3tHS7I6kCoqWn1IV4MuGT++GHURA5FG4dA0633alUAeneojrvxrVm7S/HGTB2GcdPa/EVRVp6SuUk4xzMHgqrJkLH/7PG0nTof5eirmuzqrzfrw0b95+g37hkjp06Z3ck5Wf6v1ip7M6dhqS+sP5ja1qgTk/oTAkh5IYmVRg9sA1bDpyk75hkjpw8a3ck5UdaekpldeY36x687V9Dz7fhb/fZnUjZ4LqGlUkc1JZdR07Rd0wyh05o8RUVWnpKnXfyIEzoDr+shD7jofVAuxMpG11VvyLjB7dj79EzxCYs5cDvaXZHUn6gpacUWDMkJHaBo9shbgY0vcXuRCoItK9XgYnx7fj1eBqxCcnsP37G7kgqn7T0lDq0GcZ1gdNHYOBcuLKT3YlUEGlXtzyThkZx+MRZYkYnk3rstN2RVD5o6anQ9sv/t3fn4VGVdxvHv7+ZTDaWsC8iIiouLIKALGqtr1ZFUUFFWWRTFLVaW221tlZr32pr1Wr1VSugKIsI7lIqNxJPOwAAHghJREFUorVal7IjKKIgiwpKWUQ2E7LN8/5xDjaGSQiQmTPL/bmuXJmcOcncT0Zz85w585xFML4PRMtg5CvQukfQiSQJdWvTkMmX92RrYQkDx8zhi69VfKlKpSeZa807MOE8yKnrLRzdomPQiSSJdW7dgClX9OLbkjIGjp3Nms3fBh1J9oNKTzLT8pkw+UIoaAWXzYLGhwedSFJAx1YFPH1FL4rLogwcM5uVG3cGHUn2kUpPMs+SaTD1EmjeAS6dCfUPCjqRpJBjWtZn6uheRB0MGjuHFRt2BB1J9oFKTzLL3LHw4mg49EQYMR3yGwWdSFLQkc3rMXV0L0LmFd/H67cHHUlqSKUnmcE5+Nc9MPNGOPocGPIs5NQLOpWksCOa1WXalb3JyQoxeNwcln65LehIUgMqPUl/0SjMugXevAM6D4aLJkAkN+hUkgbaNqnDtNG9qZOdxZBxc1iydmvQkWQvVHqS3srLYPq1MOdh6HkV9HsEwllBp5I0ckjjfKZd2YuC/AhDH5vLws+/CTqSVEOlJ+mrrBieHQGLn4JTfg197tKVEiQuDm6YzzNX9qZJvRyGPz6XeWu2BB1JqqC/AJKeinfCUxfBJzOgz5/glF/qSgkSVy0L8pg6uhctCnIZMX4es1d9HXQkiUGlJ+mncAtM7AefvQv9H4VeVwWdSDJE8/q5TB3dm9aN8rj0yXm8++nmoCNJJSo9SS/b18MTZ8N/PoSBk6DL4KATSYZpWi+Hp6/oxaGN63DZhPm8tXxj0JGkApWepI8ta7wrJWxbC0Ofg6P7Bp1IMlTjul7xtWtWl9ETF/KPZRuCjiQ+lZ6khw3LvIWji7d7bzpve3LQiSTDNayTzZTLe3FMy3pc/dRCXl36n6AjCSo9SQdr58MTZ3knqlz6KrTqFnQiEQAK8iNMurwnnVoVcM2URcz44KugI2U8lZ6ktlVveiet5DX0Fo5udnTQiUS+p35uhImjetL1kAZc9/T7vLz4y6AjZTSVnqSuZdNhysXQqK1XeA3bBJ1IJKa6OVlMuKwHPds25vppi3lu4bqgI2UslZ6kpvcne288b9kFRs6Aes2DTiRSrfzsLMaPPJ4Tj2jCjc8tYeq8L4KOlJFUepJ6Zj8ML18Dh50Cw1/yDm2KpIC87DDjhnfnh0c25eYXPmTSnM+DjpRxVHqSOpyDf94Bs34N7fvD4KmQXSfoVCL7JDcSZsywbvzomGbc+tJSnnhvTdCRMopKT1JDNAozb4K374Guw2HAeMjKCTqVyH7JyQrzyCXdOLNDc373t2WMe3t10JEyhkpPkl95Kbx4JcwbCydcB+c+CKFw0KlEDkh2VoiHhnSl77EtufOVj3n4zZVBR8oIusaKJLfSInh2JKx4FU67DU66QQtHS9qIhEM8MLALWSHjnlnLKSt3/PRH7YKOldZUepK8dm2HpwfD5+9B3/vg+FFBJxKpdVnhEPdd3IWsUIj7/7GCsmiUG04/EtM/7uJCpSfJ6dvNMPlC2LAULnwMOg0IOpFI3IRDxj0DjiUSNv7vnyspKY9yc5+jVXxxoNKT5LNtHUw6H7Z+AYOehiPPCDqRSNyFQsYfzu9EVtgY86/VlJU7ftP3GBVfLVPpSXLZvBIm9Ydd22DYi9DmhKATiSRMKGT8vl9HIuEQj7+7htLyKLef24FQSMVXW1R6kjzWfwCTL/DejzdyBrTsHHQikYQzM247pz2RcIixb6+mtNxxZ/+OKr5aotKT5PD5bJgyEHLqwfCXockRQScSCYyZ8auzjiYrZDzy1irKyqPcdeGxhFV8B0ylJ8H79HWYNgwKDvaWFSs4OOhEIoEzM2488yjvbQ1vfEpZ1HHPgGPJCuvt1QdCpSfBWvo8vDAamrWHoS9A3aZBJxJJGmbG9acfSSRs3PvaCsqijvsv7qziOwAqPQnOgidgxvXeySqDn4bcgqATiSSla09tR1Y4xF0zP6GsPMqDg48jouLbL/qtSTDevR9m/AzanQFDn1fhiezFVT88nFvPac/Mpf/hx08torisPOhIKUmlJ4nlHLx+G/zjduh0EQx6CiJ5QacSSQmjTmrL//brwOvLNnDVpIXsKlXx7SuVniROtBz+9lN47wHoPgrOHwvhSNCpRFLK8N6H8ofzO/Hm8k1cMXGBim8fqfQkMcpK4PlRsGgC/OAX0PfPENJ/fiL7Y0jPQ7h7wLG8u3Izlz05n8KSsqAjpQz91ZH4KymEqYPhoxfhjDvgtFt1pQSRA3Rx99bcd3Fn5qz+mpHj57OzWMVXEyo9ia+ird46mqv+Cec9BCf8JOhEImnj/OMO5i+DjmPhF98wYvw8duwqDTpS0lPpSfzs3AhPngNfLoQBT0DXYUEnEkk753U+iIcGH8eStVsZ+vg8thWp+Kqj0pP42PoFjD8TtqyCS56BDv2DTiSSts7q1JJHLunKsq+2ccljc9haWBJ0pKSl0pPat2k5PH4mFH7traN5+KlBJxJJe2d0aMHYYd1ZsWEng8fN5eudxUFHSkoJLT0z62Nmy81spZndHOP+S8zsA//j32amZfZTzZeLYHwfiJbByFegdY+gE4lkjP85uhmPDe/O6k07GTJuLpt2qPgqS1jpmVkYeBg4C2gPDDaz9pV2WwP80Dl3LPB7YGyi8kktWPMOTDgPcurCZa9Ci45BJxLJOCcf2ZQnRh7PF1sKGTR2Nhu37wo6UlJJ5EyvB7DSObfaOVcCTAX6VdzBOfdv59w3/pdzAC23nyqWz4TJF0JBK7hsFjQ+POhEIhnrhCOa8OSlx7N+2y4Gjp3D+m1FQUdKGoksvVbA2gpfr/O3VWUUMDPWHWY22swWmNmCTZs21WJE2S9LpsHUS6B5B7h0JtQ/KOhEIhmv52GNmXhZDzbtKGbgmDms+6Yw6EhJIZGlF+vdyC7mjmb/g1d6v4x1v3NurHOuu3Oue9OmuhRNoOaOhRdHw6EnwojpkN8o6EQi4ut+aCMmjerBN4UlDBwzh7VbVHyJLL11QOsKXx8MfFV5JzM7FngM6Oec+zpB2WRfOQf/uhtm3ghH9YUhz3pXPReRpHLcIQ2ZcnkvdhaXcfGY2Xy2+dugIwUqkaU3H2hnZm3NLBsYBEyvuIOZHQK8AAxzzq1IYDbZF9EozLoF3rwTOg+BiydCJDfoVCJShU4HFzDlip7sKi1n4NjZrNq0M+hIgUlY6TnnyoBrgVnAx8AzzrmPzOwqM7vK3+02oDHwiJktNrMFiconNVReBtOvhTkPQ8+rod/DENa1iEWSXYeDCpg6ujflUcfAMXP4dMOOoCMFwpyL+bJayujevbtbsEDdmBClu7wrJXwyA075NfzwJi0cLZJiVm7cweBxc4lGHZMv78kxLesHHSkuzGyhc6575e1akUVqpngnTLnYK7w+f4JTfqnCE0lBRzSrx7TRvYiEQwwZN4elX24LOlJCqfRk7wq3wMR+8Nm7cP4Y6HXV3r9HRJLWYU3rMu3KXuRFwgwZN4cP1m0NOlLCqPSketvXwxNnw38+hIGToPOgoBOJSC1o07gO067sTf28CJeMm8uiL77Z+zelAZWeVG3Lau9KCdvWwtDn4Oi+QScSkVrUulE+067sTaO62Qx/fB7zP9sSdKS4U+lJbBuWeQtHF+/w3nTe9uSgE4lIHLRqkMe00b1pVi+HEePnMWd1er89WqUne1o7H544CyzkLSvWqlvQiUQkjloU5DJ1dC8OapDHyCfm8d7KzUFHihuVnnzfqje9k1byGnoLRzc7OuhEIpIAzep7xXdo4zpc9uR83lq+MehIcaHSk/9aNt17W0Kjtl7hNWwTdCIRSaAmdXOYckUvDm9al9ETF/LGxxuCjlTrVHrieX8yPDsCWnaBkTOgXvOgE4lIABrVyWbKFT05qkU9rpq8kFkf/SfoSLVKpScw+2F4+Ro47BQY/pJ3aFNEMlaD/GwmX96TDgcVcM1Ti3jlw/VBR6o1Kr1M5hz88w6Y9Wto3x8GT4XsOkGnEpEkUJAXYdKoHnRp3YCfPP0+Ly/+MuhItUKll6miUXjlRnj7Hug6HAaMh6ycoFOJSBKplxthwmU96NamIddPW8zzC9cFHemAqfQyUXkpvHglzB8HJ1wH5z4IoXDQqUQkCdXJyeLJS4+n9+GN+cVzS3hm/tqgIx0QlV6mKS2CaUPhw2fgtNvg9P/VwtEiUq387CweH3E8P2jXlJue/4Cn5n4edKT9ptLLJLu2w+QBsGIW9L0PfvBzFZ6I1EhuJMzYYd049ehm3PLiUib8+7OgI+0XlV6m+HYzTDgH1s6BCx+D40cFnUhEUkxuJMyjQ7txRvvm/Hb6Rzz2zuqgI+0zlV4m2LbOW1Zs03IYNAU6DQg6kYikqOysEA9f0pWzO7Xgjr9/zF/fWhV0pH2SFXQAibPNK2FSf9i1DYa9CG1OCDqRiKS4SDjEg4OOIyu0hD+9+gml5VGuO61d0LFqRKWXztZ/AJMv8N6PN3IGtOwcdCIRSRNZ4RD3D+xCVsi47/UVlJVHuf70I7EkP09ApZeuPp/traOZU99bZaVJavwrTERSRzhk3HNRZ7LCxoP/XElp1HHTmUcldfGp9NLRp6/DtGFQ0AqGvQQNWgedSETSVDhk3HXBsWSFQ/z1rVWUlkW5pe8xSVt8Kr10s/R5eGE0NGsPQ1+Auk2DTiQiaS4UMu7s35FIyHjs3TWURR2/Pbd9UhafSi+dLBgPM26AQ3rDkKmQWxB0IhHJEGbG7ed1ICsc4vF311BaHuX3/ToSCiVX8an00sU798Ebv4N2Z8BFEyA7P+hEIpJhzIzf9D2GSDjEo/9aRVm5448XdEqq4lPppTrn4B+/hfcegI4D4PxHIRwJOpWIZCgz45d9jiL7u5NbotwzoDPhJCk+lV4qi5bDjOth0QToPgrOvhdCWm9ARIJlZtxwxlFkhUP+2xkc913cmaxw8H+fVHqpqqwEXrgClr3kraF56q1aR1NEksp1p7UjK2zc/epyyqOOvwzqQiTg4lPppaKSb723JKx6A07/PZx4XdCJRERi+vEpR5AdDnHH3z+mtDzKQ0O6kp0VXPEFP9eUfVO0FSZdAKvfhPP+T4UnIknv8h8cxu3ntue1ZRu4evJCisvKA8ui0kslOzfCk+fAlwthwBPeFc9FRFLAyBPbckf/jrzxyUZGT1zIrtJgik+llyq++RzGnwlbVsGQadChf9CJRET2ydBebfjThZ14+9NNXD5hAUUliS8+lV4q2LQcxveBwq9h+MtwxGlBJxIR2S8Djz+Eewd05t+rNnPpk/P4trgsoY+v0kt2Xy7yCi9aBiNfgdY9gk4kInJALux2MPcP7ML8z75hxPh57NhVmrDHVuklszXvwIRzIacuXPYqtOgYdCIRkVrRr0srHhx0HIvXbmX4+HlsT1DxqfSS1SevwOQLoeBguGwWND486EQiIrWq77EteWhIV5Z+uY2hj81lW2H8i0+ll4yWTINpQ6F5B7h0JtQ/KOhEIiJx0adjCx4d2o1P1u9g8Lg5bPm2JK6Pp9JLNnPHwouj4dATYcR0yG8UdCIRkbg67ZjmjB3ejZWbdjJ8/FzKoy5uj6UVWZKFc/D2PfDmnXBUXxgwHiK5QacSEUmIU45qxvgRx7O1qCSui1Or9JJBNAqv3QJzHoHOg+G8hyCsp0ZEMstJ7ZrE/TH0lzVo5WXwt+tg8VPQ8yo484+6UoKISJyo9IJUugueHwWfzIBTfgU//KWulCAiEkcqvaAU74Cpl8Caf0Gfu6DX1UEnEhFJeyq9IBRugacGwFeLof+j0GVw0IlERDKCSi/Rtq+HSefDltUwcBIc3TfoRCIiGUOll0hbVsPE/t7C0UOfg7YnB51IRCSjqPQSZcNH3gyvvASGT4eDuwWdSEQk46j0EmHtfO81vEgeXPoqNDs66EQiIhlJbwiLt1VvwsR+kNfQWzhahSciEhiVXjwtmw5TLoaGh3qF17BN0IlERDKaSi9e3p8Mz46All3g0r9DveZBJxIRyXgqvXiY/TC8fA0cdgoMf8k7tCkiIoHTiSy1yTnvKglv3wPt+8EF4yArJ+hUIiLiU+nVlmgUZt4E88fBccPg3AcgFA46lYhI4Mqj5ewq30VhaSGFZYVVfi4qK6JBTgMGHDkgbllUerWhvBRe+jF8+Ayc8BM4/fdaOFpEUo5zjtJo6Z6lVKmgisqKvJIqLYpdYhX3Ky1kV/muGmdo37i9Si+plRbBsyNhxatw2m1w0g0qPBGJu6iLflcqlcso1kyqJvcVlRZR5spqnCE3nEt+JJ+8rDzyI/nkZ3kfjXMb//frCp+/t1+F/Stui4QicfytqfQOzK7t8PRg+Pw96PtnOP7yoBOJSBIqLS+t9rBerGKqXGiVDwMWlRXV+PFDFvquYPIied8VTOPcxrSu1zp2OVXct9L9+VnePuEUfAlHpbe/vt0Mky/wlhe78DHoFL/puIgkRtRF2VW2a4+Cqe61qIr3f2/fCvuVRWs+e8oJ58ScGTXKbVRlCcWcQVUor5xwDqYjUIBKb/9sW+eto7n1Cxg0BY48M+hEIhmn4uypukN3lWdSe5tB1VTF2VPF4mmQ24CWWS2pE6kTe4ZUobgql1heVh5ZIf1Zjif9dvfV5pUwqT/s2gbDXoQ2JwSdSCSpOef2PPGhisLZY1vlEyX2c/aUHcqOWT4Nchrs8drS7tnRHiVV6fs1e0pNKr19sf4D75CmczDib3BQl6ATidSq0mjp9w/XVXV2XqzCqmL/orIiHK5Gj29YzEN1BbkFtMxq+b1iijWT2uO1KH9bvE+OkNSh0qupz2d762jm1PdWWWnSLuhEksGcc3u876nK4olVUjH2KSotoiRaUuMMkVAk5hl4LfJbVDk7ijmrqnBfbjhXsyeJK5VeTXz6OkwbBgWtYNhL0KB10IkkhZRFy2Ke4HCgp5vXdPYE7HHILj+ST/3s+rSo02LPw3l+Ee1tNhUJa/YkqUeltzdLn4cXRkOz9jD0BajbNOhEEifOOYrLi6udGcU8CaKKGdTuoiouL65xhizL8gqm0oyoeX7zPWZPVZ2xV/kkidysXEKmZXZFQKVXvQXjYcYNcEhvGDIVcguCTiS+WLOniidJ1OS1qD3O5CsrJOqiNc4Qa/ZUN7suzfKbxXzDbqz3OdWJ1NHsSSSBVHpVeec+eON30O4MuGgCZOcHnSglOecoiZbU6H1OVR7ei3E4cF9mT2ELx5wRNctvtsdrS7GKLNZnzZ5EUpNKrzLn4B+/hfcegI4D4PxHIUP+9V0eLd9zRYianlpezenm5a68xhnysvL2KJ66kbo0zWta/WnkVRz2y4vkkR3K1skRIgKo9L4vWg4zrodFE6D7KDj7Xggl37/mK86eqloBokYnShzAorC7Z08VT3bIy8qjSV4T8uvt/XWnWDOq3HBuSi5rJCKpI6GlZ2Z9gAeAMPCYc+6uSvebf//ZQCEw0jm3KCHhykrghStg2Uvwg5/DqbfWysLRsRaFrW72tLeVJfZn9lTVorBN8prEnD3VdFFYzZ5EJNUkrPTMLAw8DJwOrAPmm9l059yyCrudBbTzP3oCf/U/x1fJtzBtGKWr3qDw1Fso7DqMwm2ra3yWXnWH/PZ1WaM6WXW+//pSxFtzr7pFYavblqqLwoqIxEMiZ3o9gJXOudUAZjYV6AdULL1+wETnnAPmmFkDM2vpnFsft1TRckZOOZklFFPW9hBYM8n72IvqFoWt6tBetaeYR/L12pOISJwlsvRaAWsrfL2OPWdxsfZpBXyv9MxsNDDa/3KnmS2vhXxNgM218HNSgcaavjJpvBpreqqtsbaJtTGRpRdrClN5SYma7INzbiwwtjZCfffAZgucc91r82cmK401fWXSeDXW9BTvsSby1MR1QMX1uw4GvtqPfURERPZLIktvPtDOzNqaWTYwCJheaZ/pwHDz9AK2xfX1PBERySgJO7zpnCszs2uBWXhvWRjvnPvIzK7y738UeAXv7Qor8d6ycGmi8lHLh0uTnMaavjJpvBpreorrWM07UVJERCT9Jd9yIyIiInGi0hMRkYyR8aVnZn3MbLmZrTSzm4POUxvM7DMz+9DMFpvZAn9bIzN73cw+9T83rLD/r/zxLzezM4NLXjNmNt7MNprZ0grb9nl8ZtbN/z2tNLMHLQlXBqhirLeb2Zf+87vYzM6ucF8qj7W1mb1pZh+b2Udm9lN/e9o9t9WMNe2eWzPLNbN5ZrbEH+vv/O3BPK/OuYz9wDuhZhVwGJANLAHaB52rFsb1GdCk0ra7gZv92zcDf/Jvt/fHnQO09X8f4aDHsJfxnQx0BZYeyPiAeUBvvPeHzgTOCnpsNRzr7cAvYuyb6mNtCXT1b9cDVvhjSrvntpqxpt1z6+eq69+OAHOBXkE9r5k+0/tuaTTnXAmwe2m0dNQPmODfngD0r7B9qnOu2Dm3Bu/M2R4B5Ksx59zbwJZKm/dpfGbWEqjvnJvtvP+bJlb4nqRRxVirkupjXe/8BeadczuAj/FWZEq757aasVYllcfqnHM7/S8j/ocjoOc100uvqmXPUp0DXjOzheYt2QbQ3PnvefQ/N/O3p8vvYF/H18q/XXl7qrjWzD7wD3/uPiyUNmM1s0OB4/BmBWn93FYaK6Thc2tmYTNbDGwEXnfOBfa8Znrp1WjZsxR0onOuK95VK64xs5Or2Tddfwe7VTW+VB73X4HDgS5469L+2d+eFmM1s7rA88DPnHPbq9s1xraUGm+Msablc+ucK3fOdcFbZauHmXWsZve4jjXTSy8tlz1zzn3lf94IvIh3uHKDf3gA//NGf/d0+R3s6/jW+bcrb096zrkN/h+RKDCO/x6OTvmxmlkErwSecs694G9Oy+c21ljT+bkFcM5tBd4C+hDQ85rppVeTpdFSipnVMbN6u28DZwBL8cY1wt9tBPCyf3s6MMjMcsysLd61DOclNnWt2Kfx+YdTdphZL/8MsOEVviep7f5D4Tsf7/mFFB+rn+1x4GPn3H0V7kq757aqsabjc2tmTc2sgX87D/gR8AlBPa9Bn9kT9Afesmcr8M4QuiXoPLUwnsPwznxaAny0e0xAY+AN4FP/c6MK33OLP/7lJNmZX1WM8Wm8Qz+leP/6G7U/4wO64/1RWQU8hL9CUTJ9VDHWScCHwAf+H4iWaTLWk/AOV30ALPY/zk7H57aasabdcwscC7zvj2kpcJu/PZDnVcuQiYhIxsj0w5siIpJBVHoiIpIxVHoiIpIxVHoiIpIxVHoiIpIxVHoiGcTMDjUzZ2bdg84iEgSVnoiIZAyVnoiIZAyVnkgCmecmM1tlZkX+BTGH+vftPvQ4xMzeNbNdZvaJmZ1R6WecbGZz/fs3mNn9/jJ6FR/j5/7FOYvNbJ2Z/bFSlDb+hTsLzWyZmZ1e4fsj/gU6v/K/f62Z3RXXX4xIgqj0RBLrDrylxK7Bu1jmH4ExZta3wj53Aw/irbT/OvCymbUC8D/PxFvW6Tj/Zw32f85ufwBu9bd1AC7i+5dqAbjTf4zOeGvQTvVX/Ae4Dm/dx0F46x4OxFsOSiTlaRkykQTxFwDfDJzhnHunwva/AEcCPwbWAL9xzt3p3xfCW5z3Gefcb8zsTrwSOtJ5K/FjZiOBMUBDvH/Ibsa7VM2jMTIc6j/GVc65Mf623dcp+4Fz7l0zexCvLH/k9AdC0kxW0AFEMkh7IBd41cwqlkkE+KzC17N333DORc1srv+9AMcAs3cXnu9dIBs4wv/5OXgL+Fbngwq3d1+eZfdFPJ/Em2GuMLPXgFeAmZUeUyQlqfREEmf3ywnnAl9Uuq+U2BfJrMyo+sKZVV1oM5bS777JOeddqcXL55xb5M8I+wCnAhOAJWZ2uopPUp1e0xNJnGVAMdDGObey0sfnFfbrtfuGf92wHsDHFX5Gb/+w524nASV4l1vZ/RinHUhQ59wO59yzzrmrgb545XfEgfxMkWSgmZ5IgjjndpjZvcC9fpm9DdTFK7ko8Jq/69VmtgLvumo/BtoAf/XvewT4GfCImT2Ad/3Eu4CHnHOFAP72P5pZsf8YjYFuzrndP6NaZnYD3jX8FuPNCIcA2/Fe9xNJaSo9kcS6FdgA/AKvyLbjlcvdFfa5GbgB6Ap8DpzvnFsH4Jz70szOAu7xv28rMAX4dYXv/xXwjf9YB/uPN3EfMu4AbsQ7c9PhnSl61u5SFUllOntTJElUOLPyeOfcgmDTiKQnvaYnIiIZQ6UnIiIZQ4c3RUQkY2imJyIiGUOlJyIiGUOlJyIiGUOlJyIiGUOlJyIiGeP/Adx8K2LODtYyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lsup = [rules[k][\"lsup\"] for k in rules]\n",
    "lcot = [rules[k][\"lcot\"] for k in rules]\n",
    "ldiff = [rules[k][\"ldiff\"] for k in rules]\n",
    "steps = [k for k in rules]\n",
    "\n",
    "plt.figure(0, figsize=(7, 7))\n",
    "plt.title(\"weighted linear rule\", fontsize=18)\n",
    "plt.xlabel(\"epochs\", fontsize=14)\n",
    "plt.ylabel(\"Probabilities\", fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(steps, lsup, label=\"P lsup\")\n",
    "plt.plot(steps, lcot, label=\"P lcot\")\n",
    "plt.plot(steps, ldiff, label=\"P ldiff\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "s_idx, u_idx = train_dataset.split_s_u(args.supervised_ratio)\n",
    "\n",
    "# Calc the size of the Supervised and Unsupervised batch\n",
    "nb_s_file = len(s_idx)\n",
    "nb_u_file = len(u_idx)\n",
    "\n",
    "ratio = nb_s_file / nb_u_file\n",
    "s_batch_size = int(np.floor(args.batch_size * ratio))\n",
    "u_batch_size = int(np.ceil(args.batch_size * (1 - ratio)))\n",
    "\n",
    "# create the sampler, the loader and \"zip\" them\n",
    "sampler_s1 = data.SubsetRandomSampler(s_idx)\n",
    "sampler_s2 = data.SubsetRandomSampler(s_idx)\n",
    "sampler_u = data.SubsetRandomSampler(u_idx)\n",
    "\n",
    "train_loader_s1 = data.DataLoader(train_dataset, batch_size=s_batch_size, sampler=sampler_s1)\n",
    "train_loader_s2 = data.DataLoader(train_dataset, batch_size=s_batch_size, sampler=sampler_s2)\n",
    "train_loader_u = data.DataLoader(train_dataset, batch_size=u_batch_size, sampler=sampler_u)\n",
    "\n",
    "train_loader = ZipCycle([train_loader_s1, train_loader_s2, train_loader_u])\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1f_%se_%slr_%sw_%s-rfn_%ss\" % (get_datetime(), model_func.__name__, args.supervised_ratio, args.nb_epoch, args.learning_rate, args.warmup_length, rule_fn.__name__, steps)\n",
    "checkpoint_title =  \"%s_%.1f_%se_%slr_%sw_%s-rfn_%ss\" % (model_func.__name__, args.supervised_ratio, args.nb_epoch, args.learning_rate, args.warmup_length, rule_fn.__name__, steps)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (args.tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "\n",
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# Optimizer\n",
    "params = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "\n",
    "# callback\n",
    "lr_lambda = lambda epoch: (1.0 + np.cos((epoch-1) * np.pi / args.nb_epoch))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "callbacks = [lr_scheduler, lambda_cot, lambda_diff]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint_m1 = CheckPoint(m1, optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (args.checkpoint_path, checkpoint_title))\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1083,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint_m1.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "def mprint(epoch, msg, end):\n",
    "    if epoch % 10 == 0:\n",
    "        print(msg, end=end)\n",
    "        \n",
    "print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train_sup_helper(start_time, epoch, batch, x_s1, x_s2, y_s1, y_s2):\n",
    "    x_s1, x_s2, = x_s1.cuda(), x_s2.cuda()\n",
    "    y_s1, y_s2, = y_s1.cuda(), y_s2.cuda()\n",
    "\n",
    "    logits_s1 = m1(x_s1)\n",
    "    logits_s2 = m2(x_s2)\n",
    "\n",
    "    l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "    total_loss = l_sup\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calc the metrics\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # accuracies ----\n",
    "        pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "        pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "        acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "        acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "\n",
    "        avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "        avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "        # logs\n",
    "        mprint(epoch, train_form.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            int(100 * (batch + 1) / len(train_loader)),\n",
    "            \"\", avg_sup.mean, 0.0, 0.0, avg_total.mean,\n",
    "            \"\", acc_s1.mean, 0.0,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train_cot_helper(start_time, epoch, batch, x_u, y_u):\n",
    "    x_u = x_u.cuda()\n",
    "    y_u = y_u.cuda()\n",
    "\n",
    "    logits_u1 = m1(x_u)\n",
    "    logits_u2 = m2(x_u)\n",
    "\n",
    "    l_cot = loss_cot(logits_u1, logits_u2)\n",
    "    total_loss = l_cot * args.lambda_cot_max\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ======== Calc the metrics ========\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "        acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "        acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "        avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "        avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "        # logs\n",
    "        mprint(epoch, train_form.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            int(100 * (batch + 1) / len(train_loader)),\n",
    "            \"\", 0.0, avg_cot.mean, 0.0, avg_total.mean,\n",
    "            \"\", 0.0, acc_u1.mean,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train_diff_helper(start_time, epoch, batch, x_s1, x_s2, x_u, y_s1, y_s2, y_u):\n",
    "    x_s1, x_s2, x_u = x_s1.cuda(), x_s2.cuda(), x_u.cuda()\n",
    "    y_s1, y_s2, y_u = y_s1.cuda(), y_s2.cuda(), y_u.cuda()\n",
    "\n",
    "    logits_s1 = m1(x_s1)\n",
    "    logits_s2 = m2(x_s2)\n",
    "    logits_u1 = m1(x_u)\n",
    "    logits_u2 = m2(x_u)\n",
    "\n",
    "    # pseudo labels of U\n",
    "    pred_u1 = torch.argmax(logits_u1, 1)\n",
    "    pred_u2 = torch.argmax(logits_u2, 1)\n",
    "\n",
    "    # ======== Generate adversarial examples ========\n",
    "    # fix batchnorm ----\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    #generate adversarial examples ----\n",
    "    adv_data_s1 = adv_generator_1.perturb(x_s1, y_s1)\n",
    "    adv_data_u1 = adv_generator_1.perturb(x_u, pred_u1)\n",
    "\n",
    "    adv_data_s2 = adv_generator_2.perturb(x_s2, y_s2)\n",
    "    adv_data_u2 = adv_generator_2.perturb(x_u, pred_u2)\n",
    "\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    # predict adversarial examples ----\n",
    "    adv_logits_s1 = m1(adv_data_s2)\n",
    "    adv_logits_s2 = m2(adv_data_s1)\n",
    "\n",
    "    adv_logits_u1 = m1(adv_data_u2)\n",
    "    adv_logits_u2 = m2(adv_data_u1)\n",
    "\n",
    "    # ======== calculate the differents loss ========\n",
    "    # zero the parameter gradients ----\n",
    "    optimizer.zero_grad()\n",
    "    m1.zero_grad()\n",
    "    m2.zero_grad()\n",
    "\n",
    "    l_diff = loss_diff(\n",
    "        logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
    "        logits_u1, logits_u2, adv_logits_u1, adv_logits_u2\n",
    "    )\n",
    "    total_loss = l_diff * args.lambda_diff_max\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ======== Calc the metrics ========\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # accuracies ----\n",
    "        pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "        pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "        acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "        acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "        acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "        acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "        # ratios  ----\n",
    "        adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "        adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "        adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "        adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "        ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "        ratio_s2 = metrics_fn[\"ratio_s\"][1](adv_pred_s2, y_s2)\n",
    "        ratio_u1 = metrics_fn[\"ratio_u\"][0](adv_pred_u1, y_u)\n",
    "        ratio_u2 = metrics_fn[\"ratio_u\"][1](adv_pred_u2, y_u)\n",
    "        # ========\n",
    "\n",
    "        avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "        avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "\n",
    "        # logs\n",
    "        mprint(epoch, train_form.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            int(100 * (batch + 1) / len(train_loader)),\n",
    "            \"\", 0.0, 0.0, avg_diff.mean, avg_total.mean,\n",
    "            \"\", acc_s1.mean, acc_u1.mean,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    st = time.time()\n",
    "    mprint(epoch, \"\", \"\\n\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "    \n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        chosen_loss = loss_chooser(epoch)\n",
    "        \n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        if chosen_loss == \"sup\":\n",
    "            return train_sup_helper(st, epoch, batch, x_s1, x_s2, y_s1, y_s2)\n",
    "        \n",
    "        elif chosen_loss == \"cot\":\n",
    "            return train_cot_helper(st, epoch, batch, x_u, y_u)\n",
    "        \n",
    "        elif chosen_loss == \"diff\":\n",
    "            return train_diff_helper(st, epoch, batch, x_s1, x_s2, x_u, y_s1, y_s2, y_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "new_run"
    ]
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    start_time = time.time()\n",
    "    mprint(epoch, \"\", \"\\n\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits_1 = m1(x)\n",
    "            logits_2 = m2(x)\n",
    "\n",
    "            # losses ----\n",
    "            l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            mprint(epoch, val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "        \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_fn(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_fn(\"acc_2\", acc_2.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint_m1.step(acc_1.mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": [
     "new_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 3201   - 1      -          0.0000 | 0.0397 | 0.0000 | 0.3972 -           0.0000    | 0.7865   - 0.0306\n",
      "\u001b[1;4mValidati 3201   - 11     -          2.0678 | 0.0000 | 0.0000 | 2.0678 -           0.7915    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 3211   - 1      -          0.0000 | 0.0200 | 0.0000 | 0.2002 -           0.0000    | 0.8202   - 0.0310\n",
      "\u001b[1;4mValidati 3211   - 11     -          2.0440 | 0.0000 | 0.0000 | 2.0440 -           0.7935    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 3221   - 1      -          0.0000 | 0.0362 | 0.0000 | 0.3622 -           0.0000    | 0.8202   - 0.0306\n",
      "\u001b[1;4mValidati 3221   - 11     -          2.0950 | 0.0000 | 0.0000 | 2.0950 -           0.7782    | 0.0000   - 0.0843\u001b[0m\n",
      "Training 3231   - 1      -          0.0000 | 0.0266 | 0.0000 | 0.2657 -           0.0000    | 0.8202   - 0.0299\n",
      "\u001b[1;4mValidati 3231   - 11     -          2.1149 | 0.0000 | 0.0000 | 2.1149 -           0.7772    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 3241   - 1      -          0.0838 | 0.0000 | 0.0000 | 0.0838 -           1.0000    | 0.0000   - 0.0197\n",
      "\u001b[1;4mValidati 3241   - 11     -          2.0310 | 0.0000 | 0.0000 | 2.0310 -           0.8024    | 0.0000   - 0.0843\u001b[0m\n",
      "Training 3251   - 1      -          0.0778 | 0.0000 | 0.0000 | 0.0778 -           1.0000    | 0.0000   - 0.0195\n",
      "\u001b[1;4mValidati 3251   - 11     -          2.1396 | 0.0000 | 0.0000 | 2.1396 -           0.7840    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 3261   - 1      -          0.0000 | 0.0192 | 0.0000 | 0.1922 -           0.0000    | 0.8315   - 0.0303\n",
      "\u001b[1;4mValidati 3261   - 11     -          2.1614 | 0.0000 | 0.0000 | 2.1614 -           0.7893    | 0.0000   - 0.0851\u001b[0m\n",
      "Training 3271   - 1      -          0.1387 | 0.0000 | 0.0000 | 0.1387 -           1.0000    | 0.0000   - 0.0195\n",
      "\u001b[1;4mValidati 3271   - 11     -          2.1791 | 0.0000 | 0.0000 | 2.1791 -           0.7852    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 3281   - 1      -          0.1658 | 0.0000 | 0.0000 | 0.1658 -           1.0000    | 0.0000   - 0.0197\n",
      "\u001b[1;4mValidati 3281   - 11     -          2.1667 | 0.0000 | 0.0000 | 2.1667 -           0.7785    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3291   - 1      -          0.1430 | 0.0000 | 0.0000 | 0.1430 -           1.0000    | 0.0000   - 0.0196\n",
      "\u001b[1;4mValidati 3291   - 11     -          2.0365 | 0.0000 | 0.0000 | 2.0365 -           0.7856    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 3301   - 1      -          0.0000 | 0.0260 | 0.0000 | 0.2595 -           0.0000    | 0.8315   - 0.0305\n",
      "\u001b[1;4mValidati 3301   - 11     -          1.9373 | 0.0000 | 0.0000 | 1.9373 -           0.7953    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 3311   - 1      -          0.2259 | 0.0000 | 0.0000 | 0.2259 -           0.9091    | 0.0000   - 0.0205\n",
      "\u001b[1;4mValidati 3311   - 11     -          1.9501 | 0.0000 | 0.0000 | 1.9501 -           0.7912    | 0.0000   - 0.0842\u001b[0m\n",
      "Training 3321   - 1      -          0.0000 | 0.0235 | 0.0000 | 0.2348 -           0.0000    | 0.8539   - 0.0312\n",
      "\u001b[1;4mValidati 3321   - 11     -          1.9928 | 0.0000 | 0.0000 | 1.9928 -           0.8017    | 0.0000   - 0.0846\u001b[0m\n",
      "Training 3331   - 1      -          0.0000 | 0.0180 | 0.0000 | 0.1797 -           0.0000    | 0.8202   - 0.0309\n",
      "\u001b[1;4mValidati 3331   - 11     -          2.0830 | 0.0000 | 0.0000 | 2.0830 -           0.7915    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 3341   - 1      -          0.0000 | 0.0204 | 0.0000 | 0.2038 -           0.0000    | 0.9101   - 0.0305\n",
      "\u001b[1;4mValidati 3341   - 11     -          1.9423 | 0.0000 | 0.0000 | 1.9423 -           0.8013    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 3351   - 1      -          0.0000 | 0.0290 | 0.0000 | 0.2899 -           0.0000    | 0.8539   - 0.0308\n",
      "\u001b[1;4mValidati 3351   - 11     -          1.9330 | 0.0000 | 0.0000 | 1.9330 -           0.8077    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3361   - 1      -          0.0000 | 0.0178 | 0.0000 | 0.1780 -           0.0000    | 0.8989   - 0.0305\n",
      "\u001b[1;4mValidati 3361   - 11     -          2.0714 | 0.0000 | 0.0000 | 2.0714 -           0.8017    | 0.0000   - 0.0837\u001b[0m\n",
      "Training 3371   - 1      -          0.0000 | 0.0000 | 0.6370 | 0.3185 -           1.0000    | 0.8764   - 0.1051\n",
      "\u001b[1;4mValidati 3371   - 11     -          2.0921 | 0.0000 | 0.0000 | 2.0921 -           0.7986    | 0.0000   - 0.0840\u001b[0m\n",
      "Training 3381   - 1      -          0.3064 | 0.0000 | 0.0000 | 0.3064 -           0.9091    | 0.0000   - 0.0182\n",
      "\u001b[1;4mValidati 3381   - 11     -          2.0855 | 0.0000 | 0.0000 | 2.0855 -           0.7901    | 0.0000   - 0.0909\u001b[0m\n",
      "Training 3391   - 1      -          0.0000 | 0.0305 | 0.0000 | 0.3054 -           0.0000    | 0.9101   - 0.0335\n",
      "\u001b[1;4mValidati 3391   - 11     -          2.1151 | 0.0000 | 0.0000 | 2.1151 -           0.7780    | 0.0000   - 0.0940\u001b[0m\n",
      "Training 3401   - 1      -          0.0749 | 0.0000 | 0.0000 | 0.0749 -           1.0000    | 0.0000   - 0.0201\n",
      "\u001b[1;4mValidati 3401   - 11     -          2.0832 | 0.0000 | 0.0000 | 2.0832 -           0.7923    | 0.0000   - 0.0840\u001b[0m\n",
      "Training 3411   - 1      -          0.2654 | 0.0000 | 0.0000 | 0.2654 -           0.9091    | 0.0000   - 0.0193\n",
      "\u001b[1;4mValidati 3411   - 11     -          2.0346 | 0.0000 | 0.0000 | 2.0346 -           0.7801    | 0.0000   - 0.0840\u001b[0m\n",
      "Training 3421   - 1      -          0.1233 | 0.0000 | 0.0000 | 0.1233 -           1.0000    | 0.0000   - 0.0194\n",
      "\u001b[1;4mValidati 3421   - 11     -          2.0861 | 0.0000 | 0.0000 | 2.0861 -           0.7956    | 0.0000   - 0.0895\u001b[0m\n",
      "Training 3431   - 1      -          0.0910 | 0.0000 | 0.0000 | 0.0910 -           1.0000    | 0.0000   - 0.0185\n",
      "\u001b[1;4mValidati 3431   - 11     -          2.0783 | 0.0000 | 0.0000 | 2.0783 -           0.7942    | 0.0000   - 0.0840\u001b[0m\n",
      "Training 3441   - 1      -          0.1882 | 0.0000 | 0.0000 | 0.1882 -           0.9091    | 0.0000   - 0.0192\n",
      "\u001b[1;4mValidati 3441   - 11     -          2.2145 | 0.0000 | 0.0000 | 2.2145 -           0.7811    | 0.0000   - 0.0851\u001b[0m\n",
      "Training 3451   - 1      -          0.0530 | 0.0000 | 0.0000 | 0.0530 -           1.0000    | 0.0000   - 0.0194\n",
      "\u001b[1;4mValidati 3451   - 11     -          2.1898 | 0.0000 | 0.0000 | 2.1898 -           0.7841    | 0.0000   - 0.0865\u001b[0m\n",
      "Training 3461   - 1      -          0.0600 | 0.0000 | 0.0000 | 0.0600 -           1.0000    | 0.0000   - 0.0180\n",
      "\u001b[1;4mValidati 3461   - 11     -          2.1366 | 0.0000 | 0.0000 | 2.1366 -           0.7868    | 0.0000   - 0.0910\u001b[0m\n",
      "Training 3471   - 1      -          0.0000 | 0.0127 | 0.0000 | 0.1273 -           0.0000    | 0.8539   - 0.0305\n",
      "\u001b[1;4mValidati 3471   - 11     -          2.0724 | 0.0000 | 0.0000 | 2.0724 -           0.7841    | 0.0000   - 0.0840\u001b[0m\n",
      "Training 3481   - 1      -          0.0000 | 0.0130 | 0.0000 | 0.1297 -           0.0000    | 0.8539   - 0.0313\n",
      "\u001b[1;4mValidati 3481   - 11     -          2.1672 | 0.0000 | 0.0000 | 2.1672 -           0.7873    | 0.0000   - 0.0859\u001b[0m\n",
      "Training 3491   - 1      -          0.0000 | 0.0102 | 0.0000 | 0.1025 -           0.0000    | 0.8427   - 0.0307\n",
      "\u001b[1;4mValidati 3491   - 11     -          2.0956 | 0.0000 | 0.0000 | 2.0956 -           0.7889    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 3501   - 1      -          0.0970 | 0.0000 | 0.0000 | 0.0970 -           1.0000    | 0.0000   - 0.0178\n",
      "\u001b[1;4mValidati 3501   - 11     -          2.0641 | 0.0000 | 0.0000 | 2.0641 -           0.7982    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 3511   - 1      -          0.0000 | 0.0161 | 0.0000 | 0.1611 -           0.0000    | 0.8539   - 0.0307\n",
      "\u001b[1;4mValidati 3511   - 11     -          2.0735 | 0.0000 | 0.0000 | 2.0735 -           0.7938    | 0.0000   - 0.0853\u001b[0m\n",
      "Training 3521   - 1      -          0.3591 | 0.0000 | 0.0000 | 0.3591 -           0.9091    | 0.0000   - 0.0197\n",
      "\u001b[1;4mValidati 3521   - 11     -          2.0971 | 0.0000 | 0.0000 | 2.0971 -           0.7911    | 0.0000   - 0.0842\u001b[0m\n",
      "Training 3531   - 1      -          0.0000 | 0.0289 | 0.0000 | 0.2892 -           0.0000    | 0.8764   - 0.0306\n",
      "\u001b[1;4mValidati 3531   - 11     -          2.0137 | 0.0000 | 0.0000 | 2.0137 -           0.7928    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 3541   - 1      -          0.1277 | 0.0000 | 0.0000 | 0.1277 -           1.0000    | 0.0000   - 0.0206\n",
      "\u001b[1;4mValidati 3541   - 11     -          2.0060 | 0.0000 | 0.0000 | 2.0060 -           0.7911    | 0.0000   - 0.0848\u001b[0m\n",
      "Training 3551   - 1      -          0.0000 | 0.0231 | 0.0000 | 0.2314 -           0.0000    | 0.8652   - 0.0307\n",
      "\u001b[1;4mValidati 3551   - 11     -          2.0053 | 0.0000 | 0.0000 | 2.0053 -           0.7885    | 0.0000   - 0.0837\u001b[0m\n",
      "Training 3561   - 1      -          0.1971 | 0.0000 | 0.0000 | 0.1971 -           0.9091    | 0.0000   - 0.0192\n",
      "\u001b[1;4mValidati 3561   - 11     -          2.0212 | 0.0000 | 0.0000 | 2.0212 -           0.8068    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 3571   - 1      -          0.0000 | 0.0199 | 0.0000 | 0.1994 -           0.0000    | 0.8427   - 0.0305\n",
      "\u001b[1;4mValidati 3571   - 11     -          2.0751 | 0.0000 | 0.0000 | 2.0751 -           0.7963    | 0.0000   - 0.0959\u001b[0m\n",
      "Training 3581   - 1      -          0.0000 | 0.0317 | 0.0000 | 0.3166 -           0.0000    | 0.8427   - 0.0300\n",
      "\u001b[1;4mValidati 3581   - 11     -          1.9385 | 0.0000 | 0.0000 | 1.9385 -           0.8118    | 0.0000   - 0.0846\u001b[0m\n",
      "Training 3591   - 1      -          0.0000 | 0.0127 | 0.0000 | 0.1270 -           0.0000    | 0.8539   - 0.0303\n",
      "\u001b[1;4mValidati 3591   - 11     -          2.1555 | 0.0000 | 0.0000 | 2.1555 -           0.7843    | 0.0000   - 0.0867\u001b[0m\n",
      "Training 3601   - 1      -          0.2231 | 0.0000 | 0.0000 | 0.2231 -           1.0000    | 0.0000   - 0.0195\n",
      "\u001b[1;4mValidati 3601   - 11     -          1.9979 | 0.0000 | 0.0000 | 1.9979 -           0.7879    | 0.0000   - 0.0854\u001b[0m\n",
      "Training 3611   - 1      -          0.0000 | 0.0137 | 0.0000 | 0.1371 -           0.0000    | 0.9438   - 0.0314\n",
      "\u001b[1;4mValidati 3611   - 11     -          2.0177 | 0.0000 | 0.0000 | 2.0177 -           0.7895    | 0.0000   - 0.0850\u001b[0m\n",
      "Training 3621   - 1      -          0.3077 | 0.0000 | 0.0000 | 0.3077 -           1.0000    | 0.0000   - 0.0202\n",
      "\u001b[1;4mValidati 3621   - 11     -          2.0644 | 0.0000 | 0.0000 | 2.0644 -           0.7985    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 3631   - 1      -          0.3000 | 0.0000 | 0.0000 | 0.3000 -           0.9091    | 0.0000   - 0.0191\n",
      "\u001b[1;4mValidati 3631   - 11     -          2.0559 | 0.0000 | 0.0000 | 2.0559 -           0.8075    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 3641   - 1      -          0.0000 | 0.0147 | 0.0000 | 0.1472 -           0.0000    | 0.8989   - 0.0303\n",
      "\u001b[1;4mValidati 3641   - 11     -          2.0440 | 0.0000 | 0.0000 | 2.0440 -           0.8022    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 3651   - 1      -          0.1544 | 0.0000 | 0.0000 | 0.1544 -           1.0000    | 0.0000   - 0.0186\n",
      "\u001b[1;4mValidati 3651   - 11     -          1.9508 | 0.0000 | 0.0000 | 1.9508 -           0.8088    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 3661   - 1      -          0.0000 | 0.0163 | 0.0000 | 0.1628 -           0.0000    | 0.8539   - 0.0375\n",
      "\u001b[1;4mValidati 3661   - 11     -          1.9667 | 0.0000 | 0.0000 | 1.9667 -           0.7960    | 0.0000   - 0.0947\u001b[0m\n",
      "Training 3671   - 1      -          0.0000 | 0.0212 | 0.0000 | 0.2116 -           0.0000    | 0.8315   - 0.0308\n",
      "\u001b[1;4mValidati 3671   - 11     -          1.9585 | 0.0000 | 0.0000 | 1.9585 -           0.7982    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 3681   - 1      -          0.0000 | 0.0209 | 0.0000 | 0.2086 -           0.0000    | 0.8764   - 0.0310\n",
      "\u001b[1;4mValidati 3681   - 11     -          2.1011 | 0.0000 | 0.0000 | 2.1011 -           0.7933    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 3691   - 1      -          0.0000 | 0.0204 | 0.0000 | 0.2037 -           0.0000    | 0.8315   - 0.0309\n",
      "\u001b[1;4mValidati 3691   - 11     -          1.9724 | 0.0000 | 0.0000 | 1.9724 -           0.7935    | 0.0000   - 0.0837\u001b[0m\n",
      "Training 3701   - 1      -          0.0000 | 0.0201 | 0.0000 | 0.2013 -           0.0000    | 0.9213   - 0.0308\n",
      "\u001b[1;4mValidati 3701   - 11     -          2.0826 | 0.0000 | 0.0000 | 2.0826 -           0.7877    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 3711   - 1      -          0.0000 | 0.0170 | 0.0000 | 0.1702 -           0.0000    | 0.8876   - 0.0309\n",
      "\u001b[1;4mValidati 3711   - 11     -          2.0203 | 0.0000 | 0.0000 | 2.0203 -           0.7844    | 0.0000   - 0.0843\u001b[0m\n",
      "Training 3721   - 1      -          0.0000 | 0.0073 | 0.0000 | 0.0730 -           0.0000    | 0.8652   - 0.0312\n",
      "\u001b[1;4mValidati 3721   - 11     -          1.9558 | 0.0000 | 0.0000 | 1.9558 -           0.7964    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 3731   - 1      -          0.0000 | 0.0174 | 0.0000 | 0.1744 -           0.0000    | 0.8090   - 0.0309\n",
      "\u001b[1;4mValidati 3731   - 11     -          1.9929 | 0.0000 | 0.0000 | 1.9929 -           0.7961    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3741   - 1      -          0.1777 | 0.0000 | 0.0000 | 0.1777 -           0.8182    | 0.0000   - 0.0196\n",
      "\u001b[1;4mValidati 3741   - 11     -          2.0647 | 0.0000 | 0.0000 | 2.0647 -           0.7861    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 3751   - 1      -          0.1665 | 0.0000 | 0.0000 | 0.1665 -           0.9091    | 0.0000   - 0.0203\n",
      "\u001b[1;4mValidati 3751   - 11     -          2.0655 | 0.0000 | 0.0000 | 2.0655 -           0.7766    | 0.0000   - 0.0850\u001b[0m\n",
      "Training 3761   - 1      -          0.1716 | 0.0000 | 0.0000 | 0.1716 -           1.0000    | 0.0000   - 0.0197\n",
      "\u001b[1;4mValidati 3761   - 11     -          1.9544 | 0.0000 | 0.0000 | 1.9544 -           0.7868    | 0.0000   - 0.0855\u001b[0m\n",
      "Training 3771   - 1      -          0.0073 | 0.0000 | 0.0000 | 0.0073 -           1.0000    | 0.0000   - 0.0196\n",
      "\u001b[1;4mValidati 3771   - 11     -          2.0200 | 0.0000 | 0.0000 | 2.0200 -           0.7803    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3781   - 1      -          0.0000 | 0.0224 | 0.0000 | 0.2245 -           0.0000    | 0.8427   - 0.0303\n",
      "\u001b[1;4mValidati 3781   - 11     -          2.1293 | 0.0000 | 0.0000 | 2.1293 -           0.7895    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 3791   - 1      -          0.1213 | 0.0000 | 0.0000 | 0.1213 -           1.0000    | 0.0000   - 0.0198\n",
      "\u001b[1;4mValidati 3791   - 11     -          2.0150 | 0.0000 | 0.0000 | 2.0150 -           0.8110    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 3801   - 1      -          0.1757 | 0.0000 | 0.0000 | 0.1757 -           1.0000    | 0.0000   - 0.0199\n",
      "\u001b[1;4mValidati 3801   - 11     -          2.0350 | 0.0000 | 0.0000 | 2.0350 -           0.8038    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3811   - 1      -          0.0000 | 0.0179 | 0.0000 | 0.1789 -           0.0000    | 0.8315   - 0.0305\n",
      "\u001b[1;4mValidati 3811   - 11     -          1.9707 | 0.0000 | 0.0000 | 1.9707 -           0.8023    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 3821   - 1      -          0.0000 | 0.0145 | 0.0000 | 0.1449 -           0.0000    | 0.8090   - 0.0302\n",
      "\u001b[1;4mValidati 3821   - 11     -          2.0391 | 0.0000 | 0.0000 | 2.0391 -           0.7947    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 3831   - 1      -          0.0000 | 0.0177 | 0.0000 | 0.1769 -           0.0000    | 0.8876   - 0.0314\n",
      "\u001b[1;4mValidati 3831   - 11     -          1.9908 | 0.0000 | 0.0000 | 1.9908 -           0.7949    | 0.0000   - 0.0853\u001b[0m\n",
      "Training 3841   - 1      -          0.1196 | 0.0000 | 0.0000 | 0.1196 -           1.0000    | 0.0000   - 0.0204\n",
      "\u001b[1;4mValidati 3841   - 11     -          2.0517 | 0.0000 | 0.0000 | 2.0517 -           0.7996    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 3851   - 1      -          0.1694 | 0.0000 | 0.0000 | 0.1694 -           1.0000    | 0.0000   - 0.0206\n",
      "\u001b[1;4mValidati 3851   - 11     -          1.9310 | 0.0000 | 0.0000 | 1.9310 -           0.8042    | 0.0000   - 0.0851\u001b[0m\n",
      "Training 3861   - 1      -          0.0000 | 0.0239 | 0.0000 | 0.2387 -           0.0000    | 0.7865   - 0.0308\n",
      "\u001b[1;4mValidati 3861   - 11     -          1.9364 | 0.0000 | 0.0000 | 1.9364 -           0.7990    | 0.0000   - 0.0840\u001b[0m\n",
      "Training 3871   - 1      -          0.1613 | 0.0000 | 0.0000 | 0.1613 -           1.0000    | 0.0000   - 0.0202\n",
      "\u001b[1;4mValidati 3871   - 11     -          1.9122 | 0.0000 | 0.0000 | 1.9122 -           0.8075    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3881   - 1      -          0.0894 | 0.0000 | 0.0000 | 0.0894 -           1.0000    | 0.0000   - 0.0202\n",
      "\u001b[1;4mValidati 3881   - 11     -          1.9167 | 0.0000 | 0.0000 | 1.9167 -           0.8083    | 0.0000   - 0.0854\u001b[0m\n",
      "Training 3891   - 1      -          0.0473 | 0.0000 | 0.0000 | 0.0473 -           1.0000    | 0.0000   - 0.0176\n",
      "\u001b[1;4mValidati 3891   - 11     -          1.8705 | 0.0000 | 0.0000 | 1.8705 -           0.8129    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 3901   - 1      -          0.0000 | 0.0149 | 0.0000 | 0.1491 -           0.0000    | 0.8876   - 0.0304\n",
      "\u001b[1;4mValidati 3901   - 11     -          1.8696 | 0.0000 | 0.0000 | 1.8696 -           0.8050    | 0.0000   - 0.0834\u001b[0m\n",
      "Training 3911   - 1      -          0.1015 | 0.0000 | 0.0000 | 0.1015 -           1.0000    | 0.0000   - 0.0196\n",
      "\u001b[1;4mValidati 3911   - 11     -          1.8711 | 0.0000 | 0.0000 | 1.8711 -           0.7968    | 0.0000   - 0.0842\u001b[0m\n",
      "Training 3921   - 1      -          0.1489 | 0.0000 | 0.0000 | 0.1489 -           0.9091    | 0.0000   - 0.0190\n",
      "\u001b[1;4mValidati 3921   - 11     -          1.9253 | 0.0000 | 0.0000 | 1.9253 -           0.8094    | 0.0000   - 0.0852\u001b[0m\n",
      "Training 3931   - 1      -          0.0000 | 0.0272 | 0.0000 | 0.2722 -           0.0000    | 0.7865   - 0.0301\n",
      "\u001b[1;4mValidati 3931   - 11     -          2.0000 | 0.0000 | 0.0000 | 2.0000 -           0.8079    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3941   - 1      -          0.0000 | 0.0142 | 0.0000 | 0.1424 -           0.0000    | 0.8202   - 0.0303\n",
      "\u001b[1;4mValidati 3941   - 11     -          2.0750 | 0.0000 | 0.0000 | 2.0750 -           0.8049    | 0.0000   - 0.0849\u001b[0m\n",
      "Training 3951   - 1      -          0.0000 | 0.0163 | 0.0000 | 0.1626 -           0.0000    | 0.8539   - 0.0302\n",
      "\u001b[1;4mValidati 3951   - 11     -          1.9486 | 0.0000 | 0.0000 | 1.9486 -           0.8039    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3961   - 1      -          0.6505 | 0.0000 | 0.0000 | 0.6505 -           0.9091    | 0.0000   - 0.0189\n",
      "\u001b[1;4mValidati 3961   - 11     -          1.9806 | 0.0000 | 0.0000 | 1.9806 -           0.8050    | 0.0000   - 0.0850\u001b[0m\n",
      "Training 3971   - 1      -          0.5048 | 0.0000 | 0.0000 | 0.5048 -           1.0000    | 0.0000   - 0.0195\n",
      "\u001b[1;4mValidati 3971   - 11     -          2.0059 | 0.0000 | 0.0000 | 2.0059 -           0.7971    | 0.0000   - 0.0852\u001b[0m\n",
      "Training 3981   - 1      -          0.0000 | 0.0145 | 0.0000 | 0.1450 -           0.0000    | 0.7640   - 0.0302\n",
      "\u001b[1;4mValidati 3981   - 11     -          2.0932 | 0.0000 | 0.0000 | 2.0932 -           0.7904    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 3991   - 1      -          0.0000 | 0.0218 | 0.0000 | 0.2181 -           0.0000    | 0.8876   - 0.0304\n",
      "\u001b[1;4mValidati 3991   - 11     -          1.9997 | 0.0000 | 0.0000 | 1.9997 -           0.8009    | 0.0000   - 0.0856\u001b[0m\n",
      "Training 4001   - 1      -          0.0000 | 0.0175 | 0.0000 | 0.1746 -           0.0000    | 0.8427   - 0.0303\n",
      "\u001b[1;4mValidati 4001   - 11     -          2.0865 | 0.0000 | 0.0000 | 2.0865 -           0.8001    | 0.0000   - 0.0846\u001b[0m\n",
      "Training 4011   - 1      -          0.3000 | 0.0000 | 0.0000 | 0.3000 -           0.9091    | 0.0000   - 0.0203\n",
      "\u001b[1;4mValidati 4011   - 11     -          2.0296 | 0.0000 | 0.0000 | 2.0296 -           0.8086    | 0.0000   - 0.0853\u001b[0m\n",
      "Training 4021   - 1      -          0.0000 | 0.0190 | 0.0000 | 0.1899 -           0.0000    | 0.8652   - 0.0307\n",
      "\u001b[1;4mValidati 4021   - 11     -          2.0880 | 0.0000 | 0.0000 | 2.0880 -           0.8034    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 4031   - 1      -          0.0000 | 0.0159 | 0.0000 | 0.1594 -           0.0000    | 0.8315   - 0.0307\n",
      "\u001b[1;4mValidati 4031   - 11     -          2.0123 | 0.0000 | 0.0000 | 2.0123 -           0.7971    | 0.0000   - 0.0846\u001b[0m\n",
      "Training 4041   - 1      -          0.1070 | 0.0000 | 0.0000 | 0.1070 -           1.0000    | 0.0000   - 0.0203\n",
      "\u001b[1;4mValidati 4041   - 11     -          1.9292 | 0.0000 | 0.0000 | 1.9292 -           0.7975    | 0.0000   - 0.0850\u001b[0m\n",
      "Training 4051   - 1      -          0.0000 | 0.0088 | 0.0000 | 0.0883 -           0.0000    | 0.9101   - 0.0305\n",
      "\u001b[1;4mValidati 4051   - 11     -          1.9853 | 0.0000 | 0.0000 | 1.9853 -           0.7944    | 0.0000   - 0.0848\u001b[0m\n",
      "Training 4061   - 1      -          0.0536 | 0.0000 | 0.0000 | 0.0536 -           1.0000    | 0.0000   - 0.0197\n",
      "\u001b[1;4mValidati 4061   - 11     -          1.8772 | 0.0000 | 0.0000 | 1.8772 -           0.8109    | 0.0000   - 0.0851\u001b[0m\n",
      "Training 4071   - 1      -          0.0000 | 0.0116 | 0.0000 | 0.1165 -           0.0000    | 0.8876   - 0.0304\n",
      "\u001b[1;4mValidati 4071   - 11     -          1.9543 | 0.0000 | 0.0000 | 1.9543 -           0.7982    | 0.0000   - 0.0843\u001b[0m\n",
      "Training 4081   - 1      -          0.0000 | 0.0171 | 0.0000 | 0.1711 -           0.0000    | 0.8427   - 0.0303\n",
      "\u001b[1;4mValidati 4081   - 11     -          1.9687 | 0.0000 | 0.0000 | 1.9687 -           0.8004    | 0.0000   - 0.0846\u001b[0m\n",
      "Training 4091   - 1      -          0.2888 | 0.0000 | 0.0000 | 0.2888 -           0.9091    | 0.0000   - 0.0189\n",
      "\u001b[1;4mValidati 4091   - 11     -          1.9759 | 0.0000 | 0.0000 | 1.9759 -           0.8023    | 0.0000   - 0.0833\u001b[0m\n",
      "Training 4101   - 1      -          0.0000 | 0.0276 | 0.0000 | 0.2760 -           0.0000    | 0.8315   - 0.0304\n",
      "\u001b[1;4mValidati 4101   - 11     -          1.9804 | 0.0000 | 0.0000 | 1.9804 -           0.7974    | 0.0000   - 0.0831\u001b[0m\n",
      "Training 4111   - 1      -          0.0000 | 0.0154 | 0.0000 | 0.1540 -           0.0000    | 0.8202   - 0.0350\n",
      "\u001b[1;4mValidati 4111   - 11     -          1.9260 | 0.0000 | 0.0000 | 1.9260 -           0.7964    | 0.0000   - 0.0897\u001b[0m\n",
      "Training 4121   - 1      -          0.0000 | 0.0180 | 0.0000 | 0.1797 -           0.0000    | 0.7978   - 0.0303\n",
      "\u001b[1;4mValidati 4121   - 11     -          2.0479 | 0.0000 | 0.0000 | 2.0479 -           0.7865    | 0.0000   - 0.0852\u001b[0m\n",
      "Training 4131   - 1      -          0.0000 | 0.0122 | 0.0000 | 0.1216 -           0.0000    | 0.9213   - 0.0304\n",
      "\u001b[1;4mValidati 4131   - 11     -          1.9838 | 0.0000 | 0.0000 | 1.9838 -           0.8086    | 0.0000   - 0.0835\u001b[0m\n",
      "Training 4141   - 1      -          0.4533 | 0.0000 | 0.0000 | 0.4533 -           0.8182    | 0.0000   - 0.0216\n",
      "\u001b[1;4mValidati 4141   - 11     -          1.9929 | 0.0000 | 0.0000 | 1.9929 -           0.7996    | 0.0000   - 0.0979\u001b[0m\n",
      "Training 4151   - 1      -          0.7139 | 0.0000 | 0.0000 | 0.7139 -           0.8182    | 0.0000   - 0.0198\n",
      "\u001b[1;4mValidati 4151   - 11     -          1.9706 | 0.0000 | 0.0000 | 1.9706 -           0.8042    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 4161   - 1      -          0.0000 | 0.0169 | 0.0000 | 0.1695 -           0.0000    | 0.8090   - 0.0301\n",
      "\u001b[1;4mValidati 4161   - 11     -          2.0224 | 0.0000 | 0.0000 | 2.0224 -           0.8041    | 0.0000   - 0.0827\u001b[0m\n",
      "Training 4171   - 1      -          0.0940 | 0.0000 | 0.0000 | 0.0940 -           1.0000    | 0.0000   - 0.0197\n",
      "\u001b[1;4mValidati 4171   - 11     -          1.9335 | 0.0000 | 0.0000 | 1.9335 -           0.8150    | 0.0000   - 0.0835\u001b[0m\n",
      "Training 4181   - 1      -          0.0604 | 0.0000 | 0.0000 | 0.0604 -           1.0000    | 0.0000   - 0.0193\n",
      "\u001b[1;4mValidati 4181   - 11     -          2.0375 | 0.0000 | 0.0000 | 2.0375 -           0.7906    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 4191   - 1      -          0.0000 | 0.0177 | 0.0000 | 0.1775 -           0.0000    | 0.7978   - 0.0335\n",
      "\u001b[1;4mValidati 4191   - 11     -          1.9077 | 0.0000 | 0.0000 | 1.9077 -           0.8102    | 0.0000   - 0.0851\u001b[0m\n",
      "Training 4201   - 1      -          0.1003 | 0.0000 | 0.0000 | 0.1003 -           1.0000    | 0.0000   - 0.0198\n",
      "\u001b[1;4mValidati 4201   - 11     -          1.8659 | 0.0000 | 0.0000 | 1.8659 -           0.8047    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 4211   - 1      -          0.4991 | 0.0000 | 0.0000 | 0.4991 -           0.9091    | 0.0000   - 0.0206\n",
      "\u001b[1;4mValidati 4211   - 11     -          2.0046 | 0.0000 | 0.0000 | 2.0046 -           0.8008    | 0.0000   - 0.0853\u001b[0m\n",
      "Training 4221   - 1      -          0.0000 | 0.0159 | 0.0000 | 0.1585 -           0.0000    | 0.8539   - 0.0309\n",
      "\u001b[1;4mValidati 4221   - 11     -          2.0241 | 0.0000 | 0.0000 | 2.0241 -           0.7974    | 0.0000   - 0.0858\u001b[0m\n",
      "Training 4231   - 1      -          0.0000 | 0.0122 | 0.0000 | 0.1225 -           0.0000    | 0.8427   - 0.0311\n",
      "\u001b[1;4mValidati 4231   - 11     -          1.9414 | 0.0000 | 0.0000 | 1.9414 -           0.8117    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 4241   - 1      -          0.0000 | 0.0197 | 0.0000 | 0.1973 -           0.0000    | 0.8652   - 0.0307\n",
      "\u001b[1;4mValidati 4241   - 11     -          2.0062 | 0.0000 | 0.0000 | 2.0062 -           0.8071    | 0.0000   - 0.0900\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 4251   - 1      -          0.0983 | 0.0000 | 0.0000 | 0.0983 -           1.0000    | 0.0000   - 0.0194\n",
      "\u001b[1;4mValidati 4251   - 11     -          1.8673 | 0.0000 | 0.0000 | 1.8673 -           0.8150    | 0.0000   - 0.0857\u001b[0m\n",
      "Training 4261   - 1      -          0.0000 | 0.0135 | 0.0000 | 0.1347 -           0.0000    | 0.8090   - 0.0308\n",
      "\u001b[1;4mValidati 4261   - 11     -          1.8653 | 0.0000 | 0.0000 | 1.8653 -           0.8184    | 0.0000   - 0.0842\u001b[0m\n",
      "Training 4271   - 1      -          0.2478 | 0.0000 | 0.0000 | 0.2478 -           0.9091    | 0.0000   - 0.0231\n",
      "\u001b[1;4mValidati 4271   - 11     -          1.9671 | 0.0000 | 0.0000 | 1.9671 -           0.8101    | 0.0000   - 0.0931\u001b[0m\n",
      "Training 4281   - 1      -          0.0000 | 0.0223 | 0.0000 | 0.2230 -           0.0000    | 0.8989   - 0.0303\n",
      "\u001b[1;4mValidati 4281   - 11     -          2.0207 | 0.0000 | 0.0000 | 2.0207 -           0.8052    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 4291   - 1      -          0.0000 | 0.0131 | 0.0000 | 0.1306 -           0.0000    | 0.8876   - 0.0305\n",
      "\u001b[1;4mValidati 4291   - 11     -          2.0044 | 0.0000 | 0.0000 | 2.0044 -           0.8153    | 0.0000   - 0.0835\u001b[0m\n",
      "Training 4301   - 1      -          0.0000 | 0.0123 | 0.0000 | 0.1227 -           0.0000    | 0.8427   - 0.0319\n",
      "\u001b[1;4mValidati 4301   - 11     -          2.1208 | 0.0000 | 0.0000 | 2.1208 -           0.8059    | 0.0000   - 0.0885\u001b[0m\n",
      "Training 4311   - 1      -          0.0000 | 0.0172 | 0.0000 | 0.1717 -           0.0000    | 0.8539   - 0.0314\n",
      "\u001b[1;4mValidati 4311   - 11     -          1.9711 | 0.0000 | 0.0000 | 1.9711 -           0.8123    | 0.0000   - 0.0891\u001b[0m\n",
      "Training 4321   - 1      -          0.0000 | 0.0214 | 0.0000 | 0.2137 -           0.0000    | 0.8427   - 0.0303\n",
      "\u001b[1;4mValidati 4321   - 11     -          1.9274 | 0.0000 | 0.0000 | 1.9274 -           0.8143    | 0.0000   - 0.0837\u001b[0m\n",
      "Training 4331   - 1      -          0.2496 | 0.0000 | 0.0000 | 0.2496 -           0.9091    | 0.0000   - 0.0296\n",
      "\u001b[1;4mValidati 4331   - 11     -          1.9844 | 0.0000 | 0.0000 | 1.9844 -           0.8013    | 0.0000   - 0.0959\u001b[0m\n",
      "Training 4341   - 1      -          0.0000 | 0.0231 | 0.0000 | 0.2310 -           0.0000    | 0.8539   - 0.0326\n",
      "\u001b[1;4mValidati 4341   - 11     -          1.9788 | 0.0000 | 0.0000 | 1.9788 -           0.8086    | 0.0000   - 0.0921\u001b[0m\n",
      "Training 4351   - 1      -          0.0000 | 0.0116 | 0.0000 | 0.1164 -           0.0000    | 0.8090   - 0.0299\n",
      "\u001b[1;4mValidati 4351   - 11     -          2.0217 | 0.0000 | 0.0000 | 2.0217 -           0.7982    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 4361   - 1      -          0.0000 | 0.0145 | 0.0000 | 0.1447 -           0.0000    | 0.9101   - 0.0305\n",
      "\u001b[1;4mValidati 4361   - 11     -          1.9244 | 0.0000 | 0.0000 | 1.9244 -           0.7985    | 0.0000   - 0.0954\u001b[0m\n",
      "Training 4371   - 1      -          0.0000 | 0.0213 | 0.0000 | 0.2132 -           0.0000    | 0.8989   - 0.0302\n",
      "\u001b[1;4mValidati 4371   - 11     -          1.9932 | 0.0000 | 0.0000 | 1.9932 -           0.8019    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 4381   - 1      -          0.0000 | 0.0137 | 0.0000 | 0.1368 -           0.0000    | 0.8764   - 0.0311\n",
      "\u001b[1;4mValidati 4381   - 11     -          1.9331 | 0.0000 | 0.0000 | 1.9331 -           0.8026    | 0.0000   - 0.0837\u001b[0m\n",
      "Training 4391   - 1      -          0.0000 | 0.0202 | 0.0000 | 0.2019 -           0.0000    | 0.8315   - 0.0307\n",
      "\u001b[1;4mValidati 4391   - 11     -          1.8747 | 0.0000 | 0.0000 | 1.8747 -           0.8017    | 0.0000   - 0.0918\u001b[0m\n",
      "Training 4401   - 1      -          0.0000 | 0.0163 | 0.0000 | 0.1634 -           0.0000    | 0.9101   - 0.0312\n",
      "\u001b[1;4mValidati 4401   - 11     -          1.9126 | 0.0000 | 0.0000 | 1.9126 -           0.8210    | 0.0000   - 0.0900\u001b[0m\n",
      "Training 4411   - 1      -          0.0000 | 0.0114 | 0.0000 | 0.1139 -           0.0000    | 0.8876   - 0.0307\n",
      "\u001b[1;4mValidati 4411   - 11     -          1.9573 | 0.0000 | 0.0000 | 1.9573 -           0.8120    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 4421   - 1      -          0.0000 | 0.0234 | 0.0000 | 0.2336 -           0.0000    | 0.8427   - 0.0304\n",
      "\u001b[1;4mValidati 4421   - 11     -          1.8884 | 0.0000 | 0.0000 | 1.8884 -           0.8099    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 4431   - 1      -          0.0000 | 0.0145 | 0.0000 | 0.1455 -           0.0000    | 0.7753   - 0.0303\n",
      "\u001b[1;4mValidati 4431   - 11     -          1.9626 | 0.0000 | 0.0000 | 1.9626 -           0.8053    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 4441   - 1      -          0.0790 | 0.0000 | 0.0000 | 0.0790 -           1.0000    | 0.0000   - 0.0190\n",
      "\u001b[1;4mValidati 4441   - 11     -          1.9206 | 0.0000 | 0.0000 | 1.9206 -           0.7977    | 0.0000   - 0.0935\u001b[0m\n",
      "Training 4451   - 1      -          0.0000 | 0.0174 | 0.0000 | 0.1737 -           0.0000    | 0.8876   - 0.0330\n",
      "\u001b[1;4mValidati 4451   - 11     -          1.9202 | 0.0000 | 0.0000 | 1.9202 -           0.8064    | 0.0000   - 0.0889\u001b[0m\n",
      "Training 4461   - 1      -          0.0000 | 0.0241 | 0.0000 | 0.2413 -           0.0000    | 0.8315   - 0.0307\n",
      "\u001b[1;4mValidati 4461   - 11     -          1.9164 | 0.0000 | 0.0000 | 1.9164 -           0.8077    | 0.0000   - 0.0884\u001b[0m\n",
      "Training 4471   - 1      -          0.0000 | 0.0157 | 0.0000 | 0.1574 -           0.0000    | 0.8764   - 0.0301\n",
      "\u001b[1;4mValidati 4471   - 11     -          1.9436 | 0.0000 | 0.0000 | 1.9436 -           0.8142    | 0.0000   - 0.0939\u001b[0m\n",
      "Training 4481   - 1      -          0.0000 | 0.0252 | 0.0000 | 0.2521 -           0.0000    | 0.8876   - 0.0325\n",
      "\u001b[1;4mValidati 4481   - 11     -          1.9663 | 0.0000 | 0.0000 | 1.9663 -           0.8055    | 0.0000   - 0.0921\u001b[0m\n",
      "Training 4491   - 1      -          0.0000 | 0.0100 | 0.0000 | 0.1000 -           0.0000    | 0.8090   - 0.0312\n",
      "\u001b[1;4mValidati 4491   - 11     -          1.8933 | 0.0000 | 0.0000 | 1.8933 -           0.8147    | 0.0000   - 0.0970\u001b[0m\n",
      "Training 4501   - 1      -          0.0000 | 0.0178 | 0.0000 | 0.1784 -           0.0000    | 0.8427   - 0.0310\n",
      "\u001b[1;4mValidati 4501   - 11     -          2.0168 | 0.0000 | 0.0000 | 2.0168 -           0.7947    | 0.0000   - 0.0947\u001b[0m\n",
      "Training 4511   - 1      -          0.1254 | 0.0000 | 0.0000 | 0.1254 -           0.9091    | 0.0000   - 0.0201\n",
      "\u001b[1;4mValidati 4511   - 11     -          1.9123 | 0.0000 | 0.0000 | 1.9123 -           0.8094    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 4521   - 1      -          0.0000 | 0.0203 | 0.0000 | 0.2031 -           0.0000    | 0.8315   - 0.0303\n",
      "\u001b[1;4mValidati 4521   - 11     -          2.0033 | 0.0000 | 0.0000 | 2.0033 -           0.7992    | 0.0000   - 0.0838\u001b[0m\n",
      "Training 4531   - 1      -          0.0000 | 0.0000 | 0.4991 | 0.2496 -           1.0000    | 0.8539   - 0.1045\n",
      "\u001b[1;4mValidati 4531   - 11     -          1.9845 | 0.0000 | 0.0000 | 1.9845 -           0.8030    | 0.0000   - 0.0833\u001b[0m\n",
      "Training 4541   - 1      -          0.0000 | 0.0138 | 0.0000 | 0.1377 -           0.0000    | 0.8652   - 0.0306\n",
      "\u001b[1;4mValidati 4541   - 11     -          1.9103 | 0.0000 | 0.0000 | 1.9103 -           0.8172    | 0.0000   - 0.0855\u001b[0m\n",
      "Training 4551   - 1      -          0.1198 | 0.0000 | 0.0000 | 0.1198 -           1.0000    | 0.0000   - 0.0202\n",
      "\u001b[1;4mValidati 4551   - 11     -          1.9226 | 0.0000 | 0.0000 | 1.9226 -           0.8049    | 0.0000   - 0.0854\u001b[0m\n",
      "Training 4561   - 1      -          0.0563 | 0.0000 | 0.0000 | 0.0563 -           1.0000    | 0.0000   - 0.0209\n",
      "\u001b[1;4mValidati 4561   - 11     -          1.9738 | 0.0000 | 0.0000 | 1.9738 -           0.8008    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4571   - 1      -          0.0000 | 0.0181 | 0.0000 | 0.1814 -           0.0000    | 0.8764   - 0.0311\n",
      "\u001b[1;4mValidati 4571   - 11     -          1.9865 | 0.0000 | 0.0000 | 1.9865 -           0.8113    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4581   - 1      -          0.3784 | 0.0000 | 0.0000 | 0.3784 -           0.9091    | 0.0000   - 0.0199\n",
      "\u001b[1;4mValidati 4581   - 11     -          1.9898 | 0.0000 | 0.0000 | 1.9898 -           0.8053    | 0.0000   - 0.0855\u001b[0m\n",
      "Training 4591   - 1      -          0.0000 | 0.0153 | 0.0000 | 0.1531 -           0.0000    | 0.8764   - 0.0307\n",
      "\u001b[1;4mValidati 4591   - 11     -          2.0347 | 0.0000 | 0.0000 | 2.0347 -           0.8038    | 0.0000   - 0.0861\u001b[0m\n",
      "Training 4601   - 1      -          0.0000 | 0.0141 | 0.0000 | 0.1412 -           0.0000    | 0.8539   - 0.0308\n",
      "\u001b[1;4mValidati 4601   - 11     -          1.9554 | 0.0000 | 0.0000 | 1.9554 -           0.8052    | 0.0000   - 0.0870\u001b[0m\n",
      "Training 4611   - 1      -          0.0000 | 0.0117 | 0.0000 | 0.1175 -           0.0000    | 0.8090   - 0.0314\n",
      "\u001b[1;4mValidati 4611   - 11     -          1.9971 | 0.0000 | 0.0000 | 1.9971 -           0.8049    | 0.0000   - 0.0855\u001b[0m\n",
      "Training 4621   - 1      -          0.0000 | 0.0144 | 0.0000 | 0.1435 -           0.0000    | 0.8876   - 0.0310\n",
      "\u001b[1;4mValidati 4621   - 11     -          1.9281 | 0.0000 | 0.0000 | 1.9281 -           0.8117    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 4631   - 1      -          0.0000 | 0.0104 | 0.0000 | 0.1038 -           0.0000    | 0.8427   - 0.0308\n",
      "\u001b[1;4mValidati 4631   - 11     -          1.9877 | 0.0000 | 0.0000 | 1.9877 -           0.8038    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 4641   - 1      -          0.0000 | 0.0120 | 0.0000 | 0.1196 -           0.0000    | 0.8090   - 0.0310\n",
      "\u001b[1;4mValidati 4641   - 11     -          1.9735 | 0.0000 | 0.0000 | 1.9735 -           0.8012    | 0.0000   - 0.0852\u001b[0m\n",
      "Training 4651   - 1      -          0.1402 | 0.0000 | 0.0000 | 0.1402 -           0.9091    | 0.0000   - 0.0254\n",
      "\u001b[1;4mValidati 4651   - 11     -          1.8753 | 0.0000 | 0.0000 | 1.8753 -           0.8095    | 0.0000   - 0.0934\u001b[0m\n",
      "Training 4661   - 1      -          0.0000 | 0.0127 | 0.0000 | 0.1274 -           0.0000    | 0.8427   - 0.0313\n",
      "\u001b[1;4mValidati 4661   - 11     -          1.8935 | 0.0000 | 0.0000 | 1.8935 -           0.8077    | 0.0000   - 0.0849\u001b[0m\n",
      "Training 4671   - 1      -          0.3355 | 0.0000 | 0.0000 | 0.3355 -           1.0000    | 0.0000   - 0.0218\n",
      "\u001b[1;4mValidati 4671   - 11     -          1.9198 | 0.0000 | 0.0000 | 1.9198 -           0.8080    | 0.0000   - 0.0863\u001b[0m\n",
      "Training 4681   - 1      -          0.0000 | 0.0149 | 0.0000 | 0.1487 -           0.0000    | 0.8989   - 0.0308\n",
      "\u001b[1;4mValidati 4681   - 11     -          2.0517 | 0.0000 | 0.0000 | 2.0517 -           0.8022    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4691   - 1      -          0.0000 | 0.0136 | 0.0000 | 0.1363 -           0.0000    | 0.8652   - 0.0313\n",
      "\u001b[1;4mValidati 4691   - 11     -          2.0305 | 0.0000 | 0.0000 | 2.0305 -           0.8074    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4701   - 1      -          0.1342 | 0.0000 | 0.0000 | 0.1342 -           1.0000    | 0.0000   - 0.0227\n",
      "\u001b[1;4mValidati 4701   - 11     -          1.9802 | 0.0000 | 0.0000 | 1.9802 -           0.8112    | 0.0000   - 0.0864\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 4711   - 1      -          0.0000 | 0.0151 | 0.0000 | 0.1510 -           0.0000    | 0.8876   - 0.0314\n",
      "\u001b[1;4mValidati 4711   - 11     -          2.0600 | 0.0000 | 0.0000 | 2.0600 -           0.8052    | 0.0000   - 0.0845\u001b[0m\n",
      "Training 4721   - 1      -          0.0000 | 0.0101 | 0.0000 | 0.1007 -           0.0000    | 0.9213   - 0.0316\n",
      "\u001b[1;4mValidati 4721   - 11     -          1.9497 | 0.0000 | 0.0000 | 1.9497 -           0.8000    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 4731   - 1      -          0.0000 | 0.0170 | 0.0000 | 0.1700 -           0.0000    | 0.8090   - 0.0310\n",
      "\u001b[1;4mValidati 4731   - 11     -          1.8842 | 0.0000 | 0.0000 | 1.8842 -           0.8102    | 0.0000   - 0.0855\u001b[0m\n",
      "Training 4741   - 1      -          0.0000 | 0.0234 | 0.0000 | 0.2340 -           0.0000    | 0.7753   - 0.0315\n",
      "\u001b[1;4mValidati 4741   - 11     -          2.0039 | 0.0000 | 0.0000 | 2.0039 -           0.8074    | 0.0000   - 0.0847\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 4751   - 1      -          0.0000 | 0.0100 | 0.0000 | 0.0998 -           0.0000    | 0.8539   - 0.0313\n",
      "\u001b[1;4mValidati 4751   - 11     -          2.0136 | 0.0000 | 0.0000 | 2.0136 -           0.8112    | 0.0000   - 0.0865\u001b[0m\n",
      "Training 4761   - 1      -          0.0000 | 0.0114 | 0.0000 | 0.1140 -           0.0000    | 0.8427   - 0.0309\n",
      "\u001b[1;4mValidati 4761   - 11     -          2.0295 | 0.0000 | 0.0000 | 2.0295 -           0.7973    | 0.0000   - 0.0831\u001b[0m\n",
      "Training 4771   - 1      -          0.0000 | 0.0159 | 0.0000 | 0.1594 -           0.0000    | 0.8876   - 0.0306\n",
      "\u001b[1;4mValidati 4771   - 11     -          1.9923 | 0.0000 | 0.0000 | 1.9923 -           0.8101    | 0.0000   - 0.0859\u001b[0m\n",
      "Training 4781   - 1      -          0.0000 | 0.0130 | 0.0000 | 0.1296 -           0.0000    | 0.8315   - 0.0307\n",
      "\u001b[1;4mValidati 4781   - 11     -          1.9294 | 0.0000 | 0.0000 | 1.9294 -           0.8128    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 4791   - 1      -          0.0000 | 0.0245 | 0.0000 | 0.2451 -           0.0000    | 0.8876   - 0.0310\n",
      "\u001b[1;4mValidati 4791   - 11     -          2.0226 | 0.0000 | 0.0000 | 2.0226 -           0.8063    | 0.0000   - 0.0846\u001b[0m\n",
      "Training 4801   - 1      -          0.0000 | 0.0134 | 0.0000 | 0.1343 -           0.0000    | 0.8764   - 0.0308\n",
      "\u001b[1;4mValidati 4801   - 11     -          1.9770 | 0.0000 | 0.0000 | 1.9770 -           0.8000    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 4811   - 1      -          0.0000 | 0.0199 | 0.0000 | 0.1988 -           0.0000    | 0.8876   - 0.0308\n",
      "\u001b[1;4mValidati 4811   - 11     -          1.9585 | 0.0000 | 0.0000 | 1.9585 -           0.8079    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4821   - 1      -          0.0000 | 0.0185 | 0.0000 | 0.1849 -           0.0000    | 0.8202   - 0.0322\n",
      "\u001b[1;4mValidati 4821   - 11     -          1.9457 | 0.0000 | 0.0000 | 1.9457 -           0.8061    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 4831   - 1      -          0.0592 | 0.0000 | 0.0000 | 0.0592 -           1.0000    | 0.0000   - 0.0203\n",
      "\u001b[1;4mValidati 4831   - 11     -          1.9939 | 0.0000 | 0.0000 | 1.9939 -           0.8063    | 0.0000   - 0.0860\u001b[0m\n",
      "Training 4841   - 1      -          0.0000 | 0.0078 | 0.0000 | 0.0778 -           0.0000    | 0.8876   - 0.0323\n",
      "\u001b[1;4mValidati 4841   - 11     -          1.8554 | 0.0000 | 0.0000 | 1.8554 -           0.8207    | 0.0000   - 0.0855\u001b[0m\n",
      "Training 4851   - 1      -          0.0000 | 0.0107 | 0.0000 | 0.1071 -           0.0000    | 0.9326   - 0.0307\n",
      "\u001b[1;4mValidati 4851   - 11     -          1.9305 | 0.0000 | 0.0000 | 1.9305 -           0.8128    | 0.0000   - 0.0842\u001b[0m\n",
      "Training 4861   - 1      -          0.2577 | 0.0000 | 0.0000 | 0.2577 -           0.8182    | 0.0000   - 0.0207\n",
      "\u001b[1;4mValidati 4861   - 11     -          1.9487 | 0.0000 | 0.0000 | 1.9487 -           0.8117    | 0.0000   - 0.0865\u001b[0m\n",
      "Training 4871   - 1      -          0.0000 | 0.0109 | 0.0000 | 0.1093 -           0.0000    | 0.8876   - 0.0310\n",
      "\u001b[1;4mValidati 4871   - 11     -          1.9590 | 0.0000 | 0.0000 | 1.9590 -           0.8134    | 0.0000   - 0.0837\u001b[0m\n",
      "Training 4881   - 1      -          0.0000 | 0.0194 | 0.0000 | 0.1944 -           0.0000    | 0.8427   - 0.0306\n",
      "\u001b[1;4mValidati 4881   - 11     -          1.9822 | 0.0000 | 0.0000 | 1.9822 -           0.8090    | 0.0000   - 0.0830\u001b[0m\n",
      "Training 4891   - 1      -          0.0000 | 0.0102 | 0.0000 | 0.1022 -           0.0000    | 0.8876   - 0.0307\n",
      "\u001b[1;4mValidati 4891   - 11     -          1.9276 | 0.0000 | 0.0000 | 1.9276 -           0.8053    | 0.0000   - 0.0841\u001b[0m\n",
      "Training 4901   - 1      -          0.0000 | 0.0125 | 0.0000 | 0.1246 -           0.0000    | 0.9213   - 0.0304\n",
      "\u001b[1;4mValidati 4901   - 11     -          1.9842 | 0.0000 | 0.0000 | 1.9842 -           0.8060    | 0.0000   - 0.0836\u001b[0m\n",
      "Training 4911   - 1      -          0.0000 | 0.0136 | 0.0000 | 0.1361 -           0.0000    | 0.8652   - 0.0308\n",
      "\u001b[1;4mValidati 4911   - 11     -          1.9781 | 0.0000 | 0.0000 | 1.9781 -           0.8053    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4921   - 1      -          0.0000 | 0.0118 | 0.0000 | 0.1178 -           0.0000    | 0.8652   - 0.0306\n",
      "\u001b[1;4mValidati 4921   - 11     -          2.0275 | 0.0000 | 0.0000 | 2.0275 -           0.8038    | 0.0000   - 0.0839\u001b[0m\n",
      "Training 4931   - 1      -          0.0000 | 0.0128 | 0.0000 | 0.1276 -           0.0000    | 0.8202   - 0.0307\n",
      "\u001b[1;4mValidati 4931   - 11     -          1.9349 | 0.0000 | 0.0000 | 1.9349 -           0.8086    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4941   - 1      -          0.0000 | 0.0215 | 0.0000 | 0.2148 -           0.0000    | 0.8652   - 0.0313\n",
      "\u001b[1;4mValidati 4941   - 11     -          1.9865 | 0.0000 | 0.0000 | 1.9865 -           0.8082    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4951   - 1      -          0.0000 | 0.0000 | 0.5772 | 0.2886 -           0.8182    | 0.8427   - 0.1049\n",
      "\u001b[1;4mValidati 4951   - 11     -          2.0735 | 0.0000 | 0.0000 | 2.0735 -           0.8019    | 0.0000   - 0.0864\u001b[0m\n",
      "Training 4961   - 1      -          0.0000 | 0.0099 | 0.0000 | 0.0987 -           0.0000    | 0.9213   - 0.0313\n",
      "\u001b[1;4mValidati 4961   - 11     -          1.9587 | 0.0000 | 0.0000 | 1.9587 -           0.8038    | 0.0000   - 0.0917\u001b[0m\n",
      "Training 4971   - 1      -          0.0000 | 0.0126 | 0.0000 | 0.1264 -           0.0000    | 0.8427   - 0.0308\n",
      "\u001b[1;4mValidati 4971   - 11     -          1.9333 | 0.0000 | 0.0000 | 1.9333 -           0.7996    | 0.0000   - 0.0844\u001b[0m\n",
      "Training 4981   - 1      -          0.0000 | 0.0130 | 0.0000 | 0.1299 -           0.0000    | 0.8315   - 0.0308\n",
      "\u001b[1;4mValidati 4981   - 11     -          1.8878 | 0.0000 | 0.0000 | 1.8878 -           0.8109    | 0.0000   - 0.0847\u001b[0m\n",
      "Training 4991   - 1      -          0.0000 | 0.0134 | 0.0000 | 0.1341 -           0.0000    | 0.8202   - 0.0301\n",
      "\u001b[1;4mValidati 4991   - 11     -          1.9110 | 0.0000 | 0.0000 | 1.9110 -           0.8112    | 0.0000   - 0.0827\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {
    "tags": [
     "new_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule function:  <function rule_maker_weighted_linear at 0x7f1291a8ab80>\n",
      "nb steps:  10\n",
      "model:  <class 'UrbanSound8k.models_test.cnn03'>\n",
      "max acc 1: 82.26\n",
      "max acc 2: 80.95\n"
     ]
    }
   ],
   "source": [
    "print(\"rule function: \", rule_fn)\n",
    "print(\"nb steps: \", steps)\n",
    "print(\"model: \", model_func)\n",
    "print(\"max acc 1: %.2f\" % (maximum_fn.max[\"acc_1\"].item() * 100))\n",
    "print(\"max acc 2: %.2f\" % (maximum_fn.max[\"acc_2\"].item() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .llll||=||llll."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
