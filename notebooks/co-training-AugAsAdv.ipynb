{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from DCT.util.utils import reset_seed, get_datetime, get_model_from_name, ZipCycle, load_dataset\n",
    "\n",
    "from DCT.ramps import Warmup, sigmoid_rampup\n",
    "from DCT.losses import loss_cot, loss_diff, loss_sup\n",
    "\n",
    "import augmentation_utils.spec_augmentations as spec_aug\n",
    "from DCT.augmentation_list import augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets/ubs8k\", type=str)\n",
    "parser.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
    "parser.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "parser.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
"parser.add_argument(\"--model\", default=\"cnn03\", type=str)\n",
    "parser.add_argument(\"--batch_size\", default=100, type=int)\n",
    "parser.add_argument(\"--nb_epoch\", default=100, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=0.003, type=int)\n",
    "\n",
    "parser.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "parser.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "parser.add_argument(\"--warmup_length\", default=80, type=int)\n",
    "parser.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "parser.add_argument(\"--augment\", action=\"append\", help=\"augmentation. use as if python script\")\n",
    "parser.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "parser.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "\n",
    "parser.add_argument(\"--checkpoint_path\", default=\"../model_save/ubs8k/deep-co-training_aug4adv\", type=str)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--tensorboard_path\", default=\"../tensorboard/ubs8k/deep-co-training_aug4adv\", type=str)\n",
    "parser.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args([\"--augment\", \"rtd1\", \"--augment\", \"rtd2\"])"
   ]
  },
  {
   "cell_type": "code",
"execution_count": 8,
"metadata": {},
"outputs": [],
"source": [
"import argparse\n",
"parser = argparse.ArgumentParser()\n",
"\n",
"\n",
"parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
"parser.add_argument(\"-D\", \"--dataset\", default=\"ubs8k\", type=str, help=\"available [ubs8k | cifar10]\")\n",
"# parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
"\n",
"group_t = parser.add_argument_group(\"Commun parameters\")\n",
"group_t.add_argument(\"-m\", \"--model\", default=\"cnn03\", type=str)\n",
"group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
"group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
"group_t.add_argument(\"--nb_epoch\", default=300, type=int)\n",
"group_t.add_argument(\"--learning_rate\", default=0.003, type=float)\n",
"group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
"group_t.add_argument(\"--seed\", default=1234, type=int)\n",
"\n",
"group_u = parser.add_argument_group(\"UrbanSound8k parameters\")\n",
"group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
"group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
"\n",
"group_c = parser.add_argument_group(\"Cifar10 parameters\")\n",
"\n",
"group_h = parser.add_argument_group('hyperparameters')\n",
"group_h.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
"group_h.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
"group_h.add_argument(\"--warmup_length\", default=80, type=int)\n",
"\n",
"group_a = parser.add_argument_group(\"Augmentation\")\n",
"group_a.add_argument(\"--augment_m1\", default=\"s_n_20\", help=\"augmentation. use as if python script\")\n",
"group_a.add_argument(\"--augment_m2\", default=\"flip_lr\", help=\"augmentation. use as if python script\")\n",
"\n",
"group_l = parser.add_argument_group(\"Logs\")\n",
"group_l.add_argument(\"--checkpoint_path\", default=\"../model_save/ubs8k/deep-co-training_aug4adv/test\", type=str)\n",
"group_l.add_argument(\"--tensorboard_path\", default=\"../tensorboard/ubs8k/deep-co-training_aug4adv/test\", type=str)\n",
"group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
"\n",
"args = parser.parse_args(\"\")"
]
},
{
"cell_type": "code",
"execution_count": 9,
"metadata": {},
"outputs": [
{
"data": {
"text/plain": [
"['rtd1', 'rtd2']"
]
},
"execution_count": 16,
"metadata": {},
"output_type": "execute_result"
}
],
"source": [
"# modify checkpoint and tensorboard path to fit the dataset\n",
"checkpoint_path_ = args.checkpoint_path.split(\"/\")\n",
"tensorboard_path_ = args.tensorboard_path.split(\"/\")\n",
"\n",
"checkpoint_path_[3] = args.dataset\n",
"tensorboard_path_[3] = args.dataset\n",
"\n",
"args.checkpoint_path = \"/\".join(checkpoint_path_)\n",
"args.tensorboard_path = \"/\".join(tensorboard_path_)\n",
"args"
]
},
{
"cell_type": "code",
"execution_count": 10,
"metadata": {},
"outputs": [],
"source": [
"augmentation_list = list(augmentations.keys())"
]
},
{
"cell_type": "code",
"execution_count": 11,
"metadata": {},
"outputs": [],
"source": [
"reset_seed(1234)"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
"# Prepare the dataset and the dataloader\n",
"Train_laoder will return a 8 different batches and can lead to high memeory usage. Maybe better system is required\n",
"- train_loader\n",
"    - train_loader_s1\n",
"    - train_loader_s1\n",
"    - train_loader_u1\n",
"    - train_loader_u2\n",
"    - adv_loader_s1\n",
"    - adv_loader_s2\n",
"    - adv_loader_u1\n",
"    - adv_loader_u2"
]
},
{
"cell_type": "code",
"execution_count": 6,
"metadata": {
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [
{
"data": {
"application/vnd.jupyter.widget-view+json": {
"model_id": "f78f74bc8ec64983bdc75e3936e446e1",
"version_major": 2,
"version_minor": 0
},
"text/plain": [
"HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
]
},
"metadata": {},
"output_type": "display_data"
},
{
"name": "stdout",
"output_type": "stream",
"text": [
"\n"
]
}
],
"source": [
"reset_seed(1234)\n",
"\n",
"audio_root = os.path.join(args.dataset_root, \"audio\")\n",
"metadata_root = os.path.join(args.dataset_root, \"metadata\")\n",
"all_folds = args.train_folds + args.val_folds\n",
"\n",
"manager = DatasetManager(\n",
"    metadata_root, audio_root,\n",
"    folds=all_folds,\n",
"    verbose=2\n",
")"
]
},
{
"cell_type": "code",
"execution_count": 7,
"metadata": {},
"outputs": [],
"source": [
"# prepare the sampler with the specified number of supervised file\n",
"train_dataset = Dataset(manager, folds=args.train_folds, cached=True)\n",
"val_dataset = Dataset(manager, folds=args.val_folds, cached=True)\n"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
"## Models"
]
},
{
"cell_type": "code",
"execution_count": 8,
"metadata": {
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"torch.cuda.empty_cache()\n",
"model_func = get_model_from_name(args.model)\n",
"\n",
"m1, m2 = model_func(manager=manager), model_func(manager=manager)\n",
"\n",
"m1 = m1.cuda()\n",
"m2 = m2.cuda()"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
"## Loaders"
]
},
{
"cell_type": "code",
"execution_count": 9,
"metadata": {
"collapsed": false,
"jupyter": {
"outputs_hidden": false
},
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [
{
"name": "stderr",
"output_type": "stream",
"text": [
"/home/lcances/sync/Documents_sync/Projet/Datasets/UrbanSound8K/ubs8k/datasets.py:69: SettingWithCopyWarning: \n",
"A value is trying to be set on a copy of a slice from a DataFrame.\n",
"Try using .loc[row_indexer,col_indexer] = value instead\n",
"\n",
"See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
"  self.y[\"idx\"] = list(range(len(self.y)))\n"
]
}
],
"source": [
"manager, train_loader, val_loader = load_dataset(\n",
"    args.dataset,\n",
"    \"aug4adv\",\n",
"    dataset_root = args.dataset_root,\n",
"    supervised_ratio = args.supervised_ratio,\n",
"    batch_size = args.batch_size,\n",
"    train_folds = args.train_folds,\n",
"    val_folds = args.val_folds,\n",
"    \n",
"    augment_name_m1 = args.augment_m1,\n",
"    augment_name_m2 = args.augment_m2,\n",
"    verbose = 2\n",
")"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
]
},
{
"cell_type": "code",
"execution_count": 10,
"metadata": {},
"outputs": [],
"source": [
"train_aug1_name = \"noise_snr10\"\n",
"train_aug2_name = \"rtd_1\"\n",
"    \n",
"train_dataset_aug1 = Dataset(manager, folds=args.train_folds, augments=(augmentations[train_aug1_name], ), cached=True)\n",
"train_dataset_aug2 = Dataset(manager, folds=args.train_folds, augments=(augmentations[train_aug2_name], ), cached=True)"
]
},
{
"cell_type": "code",
"execution_count": 11,
"metadata": {
"pycharm": {
"is_executing": false,
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"torch.cuda.empty_cache()\n",
"model_func = get_model_from_name(args.model)\n",
"\n",
"m1, m2 = model_func(manager=manager), model_func(manager=manager)\n",
"\n",
"m1 = m1.cuda()\n",
"m2 = m2.cuda()"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
"## training parameters"
]
},
{
"cell_type": "code",
"execution_count": 13,
"metadata": {
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"# tensorboard\n",
"tensorboard_title = \"%s_%s_%.1f_%s-%s\" % (get_datetime(), model_func.__name__, args.supervised_ratio, args.augment_m1, args.augment_m2)\n",
"checkpoint_title = \"%s_%.1f\" % (model_func.__name__, args.supervised_ratio)\n",
"tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (args.tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
"\n",
"# Losses\n",
"# see losses.py\n",
"\n",
"# Optimizer\n",
"params = list(m1.parameters()) + list(m2.parameters())\n",
"optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
"\n",
"# define the warmups\n",
"lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
"lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
"\n",
"# callback\n",
"lr_lambda = lambda epoch: (1.0 + np.cos((epoch-1) * np.pi / args.nb_epoch))\n",
"lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
"callbacks = [lr_scheduler, lambda_cot, lambda_diff]\n",
"\n",
"# checkpoints\n",
"checkpoint_m1 = CheckPoint(m1, optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (args.checkpoint_path, checkpoint_title))\n",
"\n",
"# metrics\n",
"metrics_fn = dict(\n",
"    ratio_s=[Ratio(), Ratio()],\n",
"    ratio_u=[Ratio(), Ratio()],\n",
"    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
"    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
"    f1_s=[FScore(), FScore()],\n",
"    f1_u=[FScore(), FScore()],\n",
"    \n",
"    avg_total=ContinueAverage(),\n",
"    avg_sup=ContinueAverage(),\n",
"    avg_cot=ContinueAverage(),\n",
"    avg_diff=ContinueAverage(),\n",
")\n",
"\n",
"def reset_metrics():\n",
"    for item in metrics_fn.values():\n",
"        if isinstance(item, list):\n",
"            for f in item:\n",
"                f.reset()\n",
"        else:\n",
"            item.reset()"
]
},
{
"cell_type": "code",
"execution_count": 14,
"metadata": {
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"reset_metrics()"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
"## Can resume previous training"
]
},
{
"cell_type": "code",
"execution_count": 15,
"metadata": {
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"if args.resume:\n",
"    checkpoint_m1.load_last()"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
"## Metrics and hyperparameters"
]
},
{
"cell_type": "code",
"execution_count": 16,
"metadata": {
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"def get_lr(optimizer):\n",
"    for param_group in optimizer.param_groups:\n",
"        return param_group['lr']\n",
"    \n",
"def maximum():\n",
"    def func(key, value):\n",
"        if key not in func.max:\n",
"            func.max[key] = value\n",
"        else:\n",
"            if func.max[key] < value:\n",
"                func.max[key] = value\n",
"        return func.max[key]\n",
"\n",
"    func.max = dict()\n",
"    return func\n",
"maximum_fn = maximum()"
]
},
{
"cell_type": "markdown",
"metadata": {
"pycharm": {
"name": "#%% md\n"
}
},
"source": [
"# Training functions"
]
},
{
"cell_type": "code",
"execution_count": 17,
"metadata": {},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
]
}
],
"source": [
"UNDERLINE_SEQ = \"\\033[1;4m\"\n",
"\n",
"RESET_SEQ = \"\\033[0m\"\n",
"\n",
"\n",
"header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
"value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
"\n",
"header = header_form.format(\n",
"    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
")\n",
"\n",
"\n",
"train_form = value_form\n",
"val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
"\n",
"print(header)"
]
},
{
"cell_type": "code",
"execution_count": 18,
"metadata": {
"collapsed": false,
"jupyter": {
"outputs_hidden": false
},
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"def split_to_cuda(x_y):\n",
"    x, y = x_y\n",
"    x = x.cuda()\n",
"    y = y.cuda()\n",
"    return x, y"
]
},
{
"cell_type": "code",
"execution_count": 18,
"metadata": {
"collapsed": false,
"jupyter": {
"outputs_hidden": false
},
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"def train(epoch):\n",
"    start_time = time.time()\n",
"    print(\"\")\n",
"\n",
"    reset_metrics()\n",
"    m1.train()\n",
"    m2.train()\n",
"\n",
"    for batch, (t_s1, t_s2, t_u1, t_u2, a_s1, a_s2, a_u1, a_u2) in enumerate(train_loader):\n",
"        x_s1, y_s1 = split_to_cuda(t_s1)\n",
"        x_s2, y_s2 = split_to_cuda(t_s2)\n",
"        x_u1, y_u1 = split_to_cuda(t_u1)\n",
"        x_u2, y_u2 = split_to_cuda(t_u2)\n",
"        \n",
"        ax_s1, ay_s1 = split_to_cuda(a_s1)\n",
"        ax_s2, ay_s2 = split_to_cuda(a_s2)\n",
"        ax_u1, ay_u1 = split_to_cuda(a_u1)\n",
"        ax_u2, ay_u2 = split_to_cuda(a_u2)\n",
"\n",
"        # Predict normal data\n",
"        logits_s1 = m1(x_s1)\n",
"        logits_s2 = m2(x_s2)\n",
"        logits_u1 = m1(x_u1)\n",
"        logits_u2 = m2(x_u2)\n",
"\n",
"        # pseudo labels of U\n",
"        pred_u1 = torch.argmax(logits_u1, 1)\n",
"        pred_u2 = torch.argmax(logits_u2, 1)\n",
"        \n",
"        # Predict augmented (adversarial data)\n",
"        adv_logits_s1 = m1(ax_s2)\n",
"        adv_logits_u1 = m1(ax_u2)\n",
"        adv_logits_s2 = m2(ax_s1)\n",
"        adv_logits_u2 = m2(ax_u1)\n",
"\n",
"        # ======== calculate the differents loss ========\n",
"        # zero the parameter gradients ----\n",
"        optimizer.zero_grad()\n",
"        m1.zero_grad()\n",
"        m2.zero_grad()\n",
"\n",
"        # losses ----\n",
"        l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
"\n",
"        l_cot = loss_cot(logits_u1, logits_u2)\n",
"\n",
"        l_diff = loss_diff(\n",
"            logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
"            logits_u1, logits_u2, adv_logits_u1, adv_logits_u2\n",
"        )\n",
"\n",
"        total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff\n",
"        total_loss.backward()\n",
"        optimizer.step()\n",
"\n",
"        # ======== Calc the metrics ========\n",
"        with torch.set_grad_enabled(False):\n",
"            # accuracies ----\n",
"            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
"            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
"\n",
"            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
"            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
"            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u1)\n",
"            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u2)\n",
"\n",
"            # ratios  ----\n",
"            adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
"            adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
"            adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
"            adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
"\n",
"            ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
"            ratio_s2 = metrics_fn[\"ratio_s\"][1](adv_pred_s2, y_s2)\n",
"            ratio_u1 = metrics_fn[\"ratio_u\"][0](adv_pred_u1, y_u1)\n",
"            ratio_u2 = metrics_fn[\"ratio_u\"][1](adv_pred_u2, y_u2)\n",
"            # ========\n",
"\n",
"            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
"            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
"            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
"            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
"\n",
"            # logs\n",
"            print(train_form.format(\n",
"                \"Training: \",\n",
"                epoch + 1,\n",
"                int(100 * (batch + 1) / len(train_loader)),\n",
"                \"\", avg_sup.mean, avg_cot.mean, avg_diff.mean, avg_total.mean,\n",
"                \"\", acc_s1.mean, acc_u1.mean,\n",
"                time.time() - start_time\n",
"            ), end=\"\\r\")\n",
"\n",
"\n",
"    # using tensorboard to monitor loss and acc\\n\",\n",
"    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
"    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
"    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
"    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
"    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
"    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
"\n",
"    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
"    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
"    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
"    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
"\n",
"    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
"    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
"    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
"    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
"\n",
"    # Return the total loss to check for NaN\n",
"    return total_loss.item()"
]
},
{
"cell_type": "code",
"execution_count": 20,
"metadata": {
"collapsed": false,
"jupyter": {
"outputs_hidden": false
},
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [],
"source": [
"def test(epoch, msg = \"\"):\n",
"    start_time = time.time()\n",
"    print(\"\")\n",
"\n",
"    reset_metrics()\n",
"    m1.eval()\n",
"    m2.eval()\n",
"\n",
"    with torch.set_grad_enabled(False):\n",
"        for batch, (X, y) in enumerate(val_loader):\n",
"            x = X.cuda().float()\n",
"            y = y.cuda().long()\n",
"\n",
"            logits_1 = m1(x)\n",
"            logits_2 = m2(x)\n",
"\n",
"            # losses ----\n",
"            l_sup = loss_sup(logits_1, logits_2, y, y)\n",
"\n",
"            # ======== Calc the metrics ========\n",
"            # accuracies ----\n",
"            pred_1 = torch.argmax(logits_1, dim=1)\n",
"            pred_2 = torch.argmax(logits_2, dim=1)\n",
"\n",
"            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
"            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
"\n",
"            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
"\n",
"            # logs\n",
"            print(val_form.format(\n",
"                \"Validation: \",\n",
"                epoch + 1,\n",
"                int(100 * (batch + 1) / len(train_loader)),\n",
"                \"\", avg_sup.mean, 0.0, 0.0, avg_sup.mean,\n",
"                \"\", acc_1.mean, 0.0,\n",
"                time.time() - start_time\n",
"            ), end=\"\\r\")\n",
"\n",
"    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
"    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
"        \n",
"    tensorboard.add_scalar(\"max/acc_1\", maximum_fn(\"acc_1\", acc_1.mean), epoch )\n",
"    tensorboard.add_scalar(\"max/acc_2\", maximum_fn(\"acc_2\", acc_2.mean), epoch )\n",
"    \n",
"    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
"    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
"    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
"\n",
"    # Apply callbacks\n",
"    for c in callbacks:\n",
"        c.step()\n",
"\n",
"    # call checkpoint\n",
"    checkpoint_m1.step(acc_1.mean)\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"collapsed": false,
"jupyter": {
"outputs_hidden": false
},
"pycharm": {
"name": "#%%\n"
}
},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
"\n",
]
}
],
"source": [
"print(header)\n",
"\n",
"for epoch in range(0, args.nb_epoch):\n",
"    total_loss = train(epoch)\n",
"    \n",
"    if np.isnan(total_loss):\n",
"        print(\"Losses are NaN, stoping the training here\")\n",
"        break\n",
"        \n",
"    test(epoch)\n",
"\n",
"tensorboard.flush()\n",
"tensorboard.close()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": []
}
],
"metadata": {
"kernelspec": {
"display_name": "dct",
"language": "python",
"name": "dct"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.0"
},
"toc-autonumbering": true
},
"nbformat": 4,
"nbformat_minor": 4
}
