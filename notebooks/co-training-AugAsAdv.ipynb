{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from DCT.util.utils import reset_seed, get_datetime, get_model_from_name, ZipCycle, load_dataset\n",
    "\n",
    "from DCT.ramps import Warmup, sigmoid_rampup\n",
    "from DCT.losses import loss_cot, loss_diff, loss_sup\n",
    "\n",
    "import augmentation_utils.spec_augmentations as spec_aug\n",
    "from DCT.augmentation_list import augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets/ubs8k\", type=str)\n",
    "parser.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
    "parser.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "parser.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "parser.add_argument(\"--model\", default=\"cnn03\", type=str)\n",
    "parser.add_argument(\"--batch_size\", default=100, type=int)\n",
    "parser.add_argument(\"--nb_epoch\", default=100, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=0.003, type=int)\n",
    "\n",
    "parser.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "parser.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "parser.add_argument(\"--warmup_length\", default=80, type=int)\n",
    "parser.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "parser.add_argument(\"--augment\", action=\"append\", help=\"augmentation. use as if python script\")\n",
    "parser.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "parser.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "\n",
    "parser.add_argument(\"--checkpoint_path\", default=\"../model_save/ubs8k/deep-co-training_aug4adv\", type=str)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--tensorboard_path\", default=\"../tensorboard/ubs8k/deep-co-training_aug4adv\", type=str)\n",
    "parser.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args([\"--augment\", \"rtd1\", \"--augment\", \"rtd2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"ubs8k\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "# parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"cnn03\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=300, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.003, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"UrbanSound8k parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_c = parser.add_argument_group(\"Cifar10 parameters\")\n",
    "\n",
    "group_h = parser.add_argument_group('hyperparameters')\n",
    "group_h.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "group_h.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "group_h.add_argument(\"--warmup_length\", default=80, type=int)\n",
    "\n",
    "group_a = parser.add_argument_group(\"Augmentation\")\n",
    "group_a.add_argument(\"--augment_m1\", default=\"s_n_20\", help=\"augmentation. use as if python script\")\n",
    "group_a.add_argument(\"--augment_m2\", default=\"flip_lr\", help=\"augmentation. use as if python script\")\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"../model_save/ubs8k/deep-co-training_aug4adv/test\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"../tensorboard/ubs8k/deep-co-training_aug4adv/test\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(augment_m1='s_n_20', augment_m2='flip_lr', batch_size=100, checkpoint_path='../model_save/ubs8k/ubs8k/test', dataset='ubs8k', dataset_root='../datasets', lambda_cot_max=10, lambda_diff_max=0.5, learning_rate=0.003, model='cnn03', nb_epoch=300, resume=False, seed=1234, supervised_ratio=0.1, tensorboard_path='../tensorboard/ubs8k/ubs8k/test', tensorboard_sufix='', train_folds=[1, 2, 3, 4, 5, 6, 7, 8, 9], val_folds=[10], warmup_length=80)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify checkpoint and tensorboard path to fit the dataset\n",
    "checkpoint_path_ = args.checkpoint_path.split(\"/\")\n",
    "tensorboard_path_ = args.tensorboard_path.split(\"/\")\n",
    "\n",
    "checkpoint_path_[3] = args.dataset\n",
    "tensorboard_path_[3] = args.dataset\n",
    "\n",
    "args.checkpoint_path = \"/\".join(checkpoint_path_)\n",
    "args.tensorboard_path = \"/\".join(tensorboard_path_)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list = list(augmentations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset and the dataloader\n",
    "Train_laoder will return a 8 different batches and can lead to high memeory usage. Maybe better system is required\n",
    "- train_loader\n",
    "    - train_loader_s1\n",
    "    - train_loader_s1\n",
    "    - train_loader_u1\n",
    "    - train_loader_u2\n",
    "    - adv_loader_s1\n",
    "    - adv_loader_s2\n",
    "    - adv_loader_u1\n",
    "    - adv_loader_u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e966ec7ddf7d40b99fd4cb04fdfe7ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<augmentation_utils.signal_augmentations.Noise object at 0x7fd4d715e2b0>\n",
      "<augmentation_utils.spec_augmentations.HorizontalFlip object at 0x7fd4d715eb80>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcances/sync/Documents_sync/Projet/Datasets/UrbanSound8K/ubs8k/datasets.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.y[\"idx\"] = list(range(len(self.y)))\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"aug4adv\",\n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "    \n",
    "    augment_name_m1 = args.augment_m1,\n",
    "    augment_name_m2 = args.augment_m2,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = get_model_from_name(args.model)\n",
    "\n",
    "m1, m2 = model_func(manager=manager), model_func(manager=manager)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1f_%s-%s\" % (get_datetime(), model_func.__name__, args.supervised_ratio, args.augment_m1, args.augment_m2)\n",
    "checkpoint_title = \"%s_%.1f\" % (model_func.__name__, args.supervised_ratio)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (args.tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "\n",
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# Optimizer\n",
    "params = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "\n",
    "# callback\n",
    "lr_lambda = lambda epoch: (1.0 + np.cos((epoch-1) * np.pi / args.nb_epoch))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "callbacks = [lr_scheduler, lambda_cot, lambda_diff]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint_m1 = CheckPoint(m1, optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (args.checkpoint_path, checkpoint_title))\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint_m1.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_to_cuda(x_y):\n",
    "    x, y = x_y\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (t_s1, t_s2, t_u1, t_u2, a_s1, a_s2, a_u1, a_u2) in enumerate(train_loader):\n",
    "        x_s1, y_s1 = split_to_cuda(t_s1)\n",
    "        x_s2, y_s2 = split_to_cuda(t_s2)\n",
    "        x_u1, y_u1 = split_to_cuda(t_u1)\n",
    "        x_u2, y_u2 = split_to_cuda(t_u2)\n",
    "        \n",
    "        ax_s1, ay_s1 = split_to_cuda(a_s1)\n",
    "        ax_s2, ay_s2 = split_to_cuda(a_s2)\n",
    "        ax_u1, ay_u1 = split_to_cuda(a_u1)\n",
    "        ax_u2, ay_u2 = split_to_cuda(a_u2)\n",
    "\n",
    "        # Predict normal data\n",
    "        logits_s1 = m1(x_s1)\n",
    "        logits_s2 = m2(x_s2)\n",
    "        logits_u1 = m1(x_u1)\n",
    "        logits_u2 = m2(x_u2)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "        \n",
    "        # Predict augmented (adversarial data)\n",
    "        adv_logits_s1 = m1(ax_s2)\n",
    "        adv_logits_u1 = m1(ax_u2)\n",
    "        adv_logits_s2 = m2(ax_s1)\n",
    "        adv_logits_u2 = m2(ax_u1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "\n",
    "        # losses ----\n",
    "        l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "        l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "        l_diff = loss_diff(\n",
    "            logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
    "            logits_u1, logits_u2, adv_logits_u1, adv_logits_u2\n",
    "        )\n",
    "\n",
    "        total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u1)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u2)\n",
    "\n",
    "            # ratios  ----\n",
    "            adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "            adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "            adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "            adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "            ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "            ratio_s2 = metrics_fn[\"ratio_s\"][1](adv_pred_s2, y_s2)\n",
    "            ratio_u1 = metrics_fn[\"ratio_u\"][0](adv_pred_u1, y_u1)\n",
    "            ratio_u2 = metrics_fn[\"ratio_u\"][1](adv_pred_u2, y_u2)\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, avg_cot.mean, avg_diff.mean, avg_total.mean,\n",
    "                \"\", acc_s1.mean, acc_u1.mean,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "\n",
    "            logits_1 = m1(x)\n",
    "            logits_2 = m2(x)\n",
    "\n",
    "            # losses ----\n",
    "            l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "        \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_fn(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_fn(\"acc_2\", acc_2.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint_m1.step(acc_1.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
      "\n",
      "Training 1      - 100    -          4.3515 | 0.0129 | 4.5789 | 6.7702 -           0.1852    | 0.1853   - 22.9003\n",
      "\u001b[1;4mValidati 1      - 11     -          4.2904 | 0.0000 | 0.0000 | 4.2904 -           0.2356    | 0.0000   - 4.6643\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 2      - 100    -          4.0788 | 0.1204 | 5.1794 | 4.1077 -           0.2318    | 0.2188   - 23.1326\n",
      "\u001b[1;4mValidati 2      - 11     -          3.9640 | 0.0000 | 0.0000 | 3.9640 -           0.2738    | 0.0000   - 0.1130\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 3      - 100    -          3.9287 | 0.1675 | 5.6769 | 3.9676 -           0.2420    | 0.2633   - 23.6546\n",
      "\u001b[1;4mValidati 3      - 11     -          3.9750 | 0.0000 | 0.0000 | 3.9750 -           0.2455    | 0.0000   - 0.1051\u001b[0m\n",
      "Training 4      - 100    -          3.7483 | 0.2048 | 6.0637 | 3.7978 -           0.3057    | 0.3135   - 23.7705\n",
      "\u001b[1;4mValidati 4      - 11     -          3.2810 | 0.0000 | 0.0000 | 3.2810 -           0.3394    | 0.0000   - 0.1123\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 5      - 100    -          3.6267 | 0.2330 | 6.4610 | 3.6877 -           0.3466    | 0.3392   - 23.1590\n",
      "\u001b[1;4mValidati 5      - 11     -          3.5850 | 0.0000 | 0.0000 | 3.5850 -           0.3494    | 0.0000   - 0.1044\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 6      - 100    -          3.4648 | 0.2386 | 6.4702 | 3.5342 -           0.4011    | 0.3810   - 23.0831\n",
      "\u001b[1;4mValidati 6      - 11     -          3.2501 | 0.0000 | 0.0000 | 3.2501 -           0.3663    | 0.0000   - 0.1017\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 7      - 100    -          3.4176 | 0.2562 | 6.6300 | 3.4991 -           0.3955    | 0.3874   - 23.3200\n",
      "\u001b[1;4mValidati 7      - 11     -          3.2520 | 0.0000 | 0.0000 | 3.2520 -           0.4129    | 0.0000   - 0.1390\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 8      - 100    -          3.2748 | 0.2710 | 6.8929 | 3.3705 -           0.4114    | 0.3994   - 23.0881\n",
      "\u001b[1;4mValidati 8      - 11     -          3.2131 | 0.0000 | 0.0000 | 3.2131 -           0.3965    | 0.0000   - 0.1118\u001b[0m\n",
      "Training 9      - 100    -          3.3208 | 0.2785 | 6.9425 | 3.4298 -           0.4114    | 0.4086   - 23.1707\n",
      "\u001b[1;4mValidati 9      - 11     -          3.1267 | 0.0000 | 0.0000 | 3.1267 -           0.3964    | 0.0000   - 0.1155\u001b[0m\n",
      "Training 10     - 100    -          3.1243 | 0.2832 | 7.0251 | 3.2479 -           0.4523    | 0.4192   - 22.9166\n",
      "\u001b[1;4mValidati 10     - 11     -          4.2142 | 0.0000 | 0.0000 | 4.2142 -           0.2489    | 0.0000   - 0.0984\u001b[0m\n",
      "Training 11     - 100    -          3.0080 | 0.3010 | 7.1150 | 3.1509 -           0.4602    | 0.4436   - 23.1704\n",
      "\u001b[1;4mValidati 11     - 11     -          3.1533 | 0.0000 | 0.0000 | 3.1533 -           0.4244    | 0.0000   - 0.1121\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 12     - 100    -          2.9950 | 0.3123 | 7.2555 | 3.1587 -           0.4557    | 0.4443   - 23.1329\n",
      "\u001b[1;4mValidati 12     - 11     -          3.4619 | 0.0000 | 0.0000 | 3.4619 -           0.4673    | 0.0000   - 0.1101\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 13     - 100    -          2.9966 | 0.3119 | 7.2329 | 3.1783 -           0.4852    | 0.4629   - 23.3073\n",
      "\u001b[1;4mValidati 13     - 11     -          2.8307 | 0.0000 | 0.0000 | 2.8307 -           0.4754    | 0.0000   - 0.1113\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 14     - 100    -          2.7985 | 0.3231 | 7.4239 | 3.0067 -           0.5023    | 0.4679   - 23.0043\n",
      "\u001b[1;4mValidati 14     - 11     -          2.9383 | 0.0000 | 0.0000 | 2.9383 -           0.4971    | 0.0000   - 0.0973\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 15     - 100    -          2.8331 | 0.3240 | 7.3539 | 3.0632 -           0.4761    | 0.4638   - 23.3627\n",
      "\u001b[1;4mValidati 15     - 11     -          2.9777 | 0.0000 | 0.0000 | 2.9777 -           0.4669    | 0.0000   - 0.1102\u001b[0m\n",
      "Training 16     - 100    -          2.8597 | 0.3201 | 7.3204 | 3.1125 -           0.4784    | 0.4798   - 23.0460\n",
      "\u001b[1;4mValidati 16     - 11     -          3.0280 | 0.0000 | 0.0000 | 3.0280 -           0.4664    | 0.0000   - 0.1122\u001b[0m\n",
      "Training 17     - 100    -          2.6893 | 0.3414 | 7.6854 | 2.9851 -           0.5159    | 0.4960   - 23.4771\n",
      "\u001b[1;4mValidati 17     - 11     -          3.1056 | 0.0000 | 0.0000 | 3.1056 -           0.4711    | 0.0000   - 0.1002\u001b[0m\n",
      "Training 18     - 100    -          2.6724 | 0.3431 | 7.6327 | 2.9986 -           0.5125    | 0.4993   - 23.2368\n",
      "\u001b[1;4mValidati 18     - 11     -          2.9667 | 0.0000 | 0.0000 | 2.9667 -           0.4825    | 0.0000   - 0.1012\u001b[0m\n",
      "Training 19     - 100    -          2.6806 | 0.3359 | 7.4913 | 3.0333 -           0.5182    | 0.5085   - 23.1406\n",
      "\u001b[1;4mValidati 19     - 11     -          2.9472 | 0.0000 | 0.0000 | 2.9472 -           0.4517    | 0.0000   - 0.1152\u001b[0m\n",
      "Training 20     - 100    -          2.5582 | 0.3466 | 7.6066 | 2.9554 -           0.5455    | 0.5209   - 23.0738\n",
      "\u001b[1;4mValidati 20     - 11     -          2.8480 | 0.0000 | 0.0000 | 2.8480 -           0.5156    | 0.0000   - 0.0996\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 21     - 100    -          2.4644 | 0.3507 | 7.6660 | 2.9052 -           0.5614    | 0.5261   - 23.0390\n",
      "\u001b[1;4mValidati 21     - 11     -          2.9196 | 0.0000 | 0.0000 | 2.9196 -           0.5159    | 0.0000   - 0.1002\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 22     - 100    -          2.4661 | 0.3508 | 7.6912 | 2.9508 -           0.5830    | 0.5338   - 23.0762\n",
      "\u001b[1;4mValidati 22     - 11     -          2.9105 | 0.0000 | 0.0000 | 2.9105 -           0.5085    | 0.0000   - 0.1105\u001b[0m\n",
      "Training 23     - 100    -          2.3827 | 0.3440 | 7.4558 | 2.9003 -           0.5943    | 0.5377   - 23.3177\n",
      "\u001b[1;4mValidati 23     - 11     -          2.7953 | 0.0000 | 0.0000 | 2.7953 -           0.5180    | 0.0000   - 0.1097\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 24     - 100    -          2.3563 | 0.3423 | 7.2872 | 2.9146 -           0.5739    | 0.5398   - 23.2717\n",
      "\u001b[1;4mValidati 24     - 11     -          2.7309 | 0.0000 | 0.0000 | 2.7309 -           0.5877    | 0.0000   - 0.1105\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 25     - 100    -          2.3991 | 0.3418 | 7.0045 | 2.9963 -           0.5909    | 0.5286   - 23.1120\n",
      "\u001b[1;4mValidati 25     - 11     -          2.9099 | 0.0000 | 0.0000 | 2.9099 -           0.5585    | 0.0000   - 0.1108\u001b[0m\n",
      "Training 26     - 100    -          2.3210 | 0.3415 | 6.7739 | 2.9611 -           0.5955    | 0.5533   - 23.2468\n",
      "\u001b[1;4mValidati 26     - 11     -          2.9795 | 0.0000 | 0.0000 | 2.9795 -           0.5311    | 0.0000   - 0.1132\u001b[0m\n",
      "Training 27     - 100    -          2.2015 | 0.3510 | 6.9311 | 2.9163 -           0.6148    | 0.5606   - 22.7947\n",
      "\u001b[1;4mValidati 27     - 11     -          2.9855 | 0.0000 | 0.0000 | 2.9855 -           0.5355    | 0.0000   - 0.1149\u001b[0m\n",
      "Training 28     - 100    -          2.2549 | 0.3493 | 6.7289 | 3.0189 -           0.6170    | 0.5656   - 23.3499\n",
      "\u001b[1;4mValidati 28     - 11     -          3.0679 | 0.0000 | 0.0000 | 3.0679 -           0.5427    | 0.0000   - 0.1118\u001b[0m\n",
      "Training 29     - 100    -          2.1913 | 0.3480 | 6.5732 | 3.0096 -           0.6466    | 0.5763   - 22.6917\n",
      "\u001b[1;4mValidati 29     - 11     -          3.1043 | 0.0000 | 0.0000 | 3.1043 -           0.6123    | 0.0000   - 0.1105\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 30     - 100    -          2.1152 | 0.3425 | 6.4633 | 2.9878 -           0.6205    | 0.5665   - 23.2100\n",
      "\u001b[1;4mValidati 30     - 11     -          2.9530 | 0.0000 | 0.0000 | 2.9530 -           0.5743    | 0.0000   - 0.1150\u001b[0m\n",
      "Training 31     - 100    -          2.1419 | 0.3295 | 6.1374 | 3.0445 -           0.6511    | 0.5746   - 23.2678\n",
      "\u001b[1;4mValidati 31     - 11     -          2.9794 | 0.0000 | 0.0000 | 2.9794 -           0.5765    | 0.0000   - 0.0980\u001b[0m\n",
      "Training 32     - 100    -          2.0286 | 0.3284 | 5.9971 | 2.9914 -           0.6659    | 0.5826   - 22.7439\n",
      "\u001b[1;4mValidati 32     - 11     -          3.2075 | 0.0000 | 0.0000 | 3.2075 -           0.6498    | 0.0000   - 0.0999\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 33     - 100    -          1.9588 | 0.3298 | 5.8829 | 2.9901 -           0.6875    | 0.5811   - 23.5340\n",
      "\u001b[1;4mValidati 33     - 11     -          3.2884 | 0.0000 | 0.0000 | 3.2884 -           0.6410    | 0.0000   - 0.1154\u001b[0m\n",
      "Training 34     - 100    -          2.0413 | 0.3075 | 5.6742 | 3.0938 -           0.6989    | 0.5929   - 22.9852\n",
      "\u001b[1;4mValidati 34     - 11     -          3.1613 | 0.0000 | 0.0000 | 3.1613 -           0.6102    | 0.0000   - 0.0974\u001b[0m\n",
      "Training 35     - 100    -          1.9968 | 0.3051 | 5.5878 | 3.1158 -           0.6898    | 0.5709   - 23.6795\n",
      "\u001b[1;4mValidati 35     - 11     -          3.3633 | 0.0000 | 0.0000 | 3.3633 -           0.5638    | 0.0000   - 0.0975\u001b[0m\n",
      "Training 36     - 100    -          2.0189 | 0.2860 | 5.4400 | 3.1660 -           0.7114    | 0.5608   - 22.9219\n",
      "\u001b[1;4mValidati 36     - 11     -          3.3417 | 0.0000 | 0.0000 | 3.3417 -           0.5735    | 0.0000   - 0.0990\u001b[0m\n",
      "Training 37     - 100    -          2.0117 | 0.2640 | 5.3419 | 3.1821 -           0.7091    | 0.5572   - 23.8580\n",
      "\u001b[1;4mValidati 37     - 11     -          3.3198 | 0.0000 | 0.0000 | 3.3198 -           0.5643    | 0.0000   - 0.0997\u001b[0m\n",
      "Training 38     - 100    -          2.0723 | 0.2511 | 5.2354 | 3.2819 -           0.7034    | 0.5562   - 23.0351\n",
      "\u001b[1;4mValidati 38     - 11     -          3.2282 | 0.0000 | 0.0000 | 3.2282 -           0.6235    | 0.0000   - 0.1030\u001b[0m\n",
      "Training 39     - 100    -          2.0732 | 0.2270 | 5.0908 | 3.2869 -           0.6943    | 0.5296   - 23.2526\n",
      "\u001b[1;4mValidati 39     - 11     -          3.5634 | 0.0000 | 0.0000 | 3.5634 -           0.5898    | 0.0000   - 0.1119\u001b[0m\n",
      "Training 40     - 100    -          1.9744 | 0.2076 | 5.0207 | 3.2079 -           0.7330    | 0.5155   - 23.1779\n",
      "\u001b[1;4mValidati 40     - 11     -          3.3605 | 0.0000 | 0.0000 | 3.3605 -           0.5538    | 0.0000   - 0.1102\u001b[0m\n",
      "Training 41     - 2      -          2.5895 | 0.1566 | 5.0316 | 3.7589 -           0.7273    | 0.4831   - 2.3687\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
