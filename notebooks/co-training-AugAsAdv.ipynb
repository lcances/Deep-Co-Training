{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from DCT.util.utils import reset_seed, get_datetime, get_model_from_name, ZipCycle\n",
    "\n",
    "from DCT.ramps import Warmup, sigmoid_rampup\n",
    "from DCT.losses import loss_cot, loss_diff, loss_sup\n",
    "\n",
    "import augmentation_utils.spec_augmentations as spec_aug\n",
    "from DCT.augmentation_list import augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets/ubs8k\", type=str)\n",
    "parser.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
    "parser.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "parser.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "parser.add_argument(\"--model\", default=\"cnn03\", type=str)\n",
    "parser.add_argument(\"--batch_size\", default=100, type=int)\n",
    "parser.add_argument(\"--nb_epoch\", default=100, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=0.003, type=int)\n",
    "\n",
    "parser.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "parser.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "parser.add_argument(\"--warmup_length\", default=80, type=int)\n",
    "parser.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "parser.add_argument(\"--augment\", action=\"append\", help=\"augmentation. use as if python script\")\n",
    "parser.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "parser.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "\n",
    "parser.add_argument(\"--checkpoint_path\", default=\"../model_save/ubs8k/deep-co-training_aug4adv\", type=str)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--tensorboard_path\", default=\"../tensorboard/ubs8k/deep-co-training_aug4adv\", type=str)\n",
    "parser.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args([\"--augment\", \"rtd1\", \"--augment\", \"rtd2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_list = list(augmentations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rtd1', 'rtd2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ()\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78f74bc8ec64983bdc75e3936e446e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reset_seed(1234)\n",
    "\n",
    "audio_root = os.path.join(args.dataset_root, \"audio\")\n",
    "metadata_root = os.path.join(args.dataset_root, \"metadata\")\n",
    "all_folds = args.train_folds + args.val_folds\n",
    "\n",
    "manager = DatasetManager(\n",
    "    metadata_root, audio_root,\n",
    "    folds=all_folds,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the sampler with the specified number of supervised file\n",
    "train_dataset = Dataset(manager, folds=args.train_folds, cached=True)\n",
    "val_dataset = Dataset(manager, folds=args.val_folds, cached=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = get_model_from_name(args.model)\n",
    "\n",
    "m1, m2 = model_func(manager=manager), model_func(manager=manager)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcances/sync/Documents_sync/Projet/Datasets/UrbanSound8K/ubs8k/datasets.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.y[\"idx\"] = list(range(len(self.y)))\n"
     ]
    }
   ],
   "source": [
    "s_idx, u_idx = train_dataset.split_s_u(args.supervised_ratio)\n",
    "\n",
    "# Calc the size of the Supervised and Unsupervised batch\n",
    "nb_s_file = len(s_idx)\n",
    "nb_u_file = len(u_idx)\n",
    "\n",
    "ratio = nb_s_file / nb_u_file\n",
    "s_batch_size = int(np.floor(args.batch_size * ratio))\n",
    "u_batch_size = int(np.ceil(args.batch_size * (1 - ratio)))\n",
    "\n",
    "# create the sampler, the loader and \"zip\" them\n",
    "sampler_s1 = data.SubsetRandomSampler(s_idx)\n",
    "sampler_s2 = data.SubsetRandomSampler(s_idx)\n",
    "sampler_u = data.SubsetRandomSampler(u_idx)\n",
    "\n",
    "train_loader_s1 = data.DataLoader(train_dataset, batch_size=s_batch_size, sampler=sampler_s1)\n",
    "train_loader_s2 = data.DataLoader(train_dataset, batch_size=s_batch_size, sampler=sampler_s2)\n",
    "train_loader_u = data.DataLoader(train_dataset, batch_size=u_batch_size, sampler=sampler_u)\n",
    "\n",
    "train_loader = ZipCycle([train_loader_s1, train_loader_s2, train_loader_u])\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug1_name = \"noise_snr10\"\n",
    "train_aug2_name = \"rtd_1\"\n",
    "    \n",
    "train_dataset_aug1 = Dataset(manager, folds=args.train_folds, augments=(augmentations[train_aug1_name], ), cached=True)\n",
    "train_dataset_aug2 = Dataset(manager, folds=args.train_folds, augments=(augmentations[train_aug2_name], ), cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader_s1_aug1 = data.DataLoader(train_dataset_aug1, batch_size=s_batch_size, sampler=sampler_s1, num_workers=4)\n",
    "train_loader_u_aug1 = data.DataLoader(train_dataset_aug1, batch_size=u_batch_size, sampler=sampler_u, num_workers=4)\n",
    "train_loader_aug1 = ZipCycle([train_loader_s1_aug1, train_loader_u_aug1])\n",
    "\n",
    "train_loader_s2_aug2 = data.DataLoader(train_dataset_aug2, batch_size=s_batch_size, sampler=sampler_s2, num_workers=4)\n",
    "train_loader_u_aug2 = data.DataLoader(train_dataset_aug2, batch_size=u_batch_size, sampler=sampler_u, num_workers=4)\n",
    "train_loader_aug2 = ZipCycle([train_loader_s2_aug2, train_loader_u_aug2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_loader = ZipCycle([train_loader, train_loader_aug1, train_loader_aug2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1f_%s-%s\" % (get_datetime(), model_func.__name__, args.supervised_ratio, train_aug1_name, train_aug2_name)\n",
    "\n",
    "checkpoint_title = \"%s_%.1f\" % (model_func.__name__, args.supervised_ratio)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (args.tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "\n",
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# Optimizer\n",
    "params = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "\n",
    "# callback\n",
    "lr_lambda = lambda epoch: (1.0 + np.cos((epoch-1) * np.pi / args.nb_epoch))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "callbacks = [lr_scheduler, lambda_cot, lambda_diff]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint_m1 = CheckPoint(m1, optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (args.checkpoint_path, checkpoint_title))\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint_m1.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def format_data(data: tuple):\n",
    "    def format_triple(data):\n",
    "        S1, S2, U = data\n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        x_s1, x_s2, x_u = x_s1.cuda().float(), x_s2.cuda().float(), x_u.cuda().float()\n",
    "        y_s1, y_s2, y_u = y_s1.cuda().long(), y_s2.cuda().long(), y_u.cuda().long()\n",
    "        return x_s1, x_s2, x_u, y_s1, y_s2, y_u\n",
    "    \n",
    "    def format_double(data: tuple):\n",
    "        S, U = data\n",
    "        x_s, _ = S\n",
    "        x_u, _ = U\n",
    "        x_s, x_u = x_s.cuda().float(), x_u.cuda().float()\n",
    "        \n",
    "        return x_s, x_u\n",
    "    \n",
    "    if len(data) == 3:\n",
    "        return format_triple(data)\n",
    "    elif len(data) == 2:\n",
    "        return format_double(data)\n",
    "    else:\n",
    "        raise ValueError(\"data must a Iterable of size 2 or 3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (normal, aug1, aug2) in enumerate(final_train_loader):\n",
    "        x_s1, x_s2, x_u, y_s1, y_s2, y_u = format_data(normal)\n",
    "        x_s1_aug1, x_u_aug1 = format_data(aug1)\n",
    "        x_s2_aug2, x_u_aug2 = format_data(aug2)\n",
    "        \n",
    "        # Predict normal data\n",
    "        logits_s1 = m1(x_s1)\n",
    "        logits_s2 = m2(x_s2)\n",
    "        logits_u1 = m1(x_u)\n",
    "        logits_u2 = m2(x_u)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "        \n",
    "        # Predict augmented (adversarial data)\n",
    "        adv_logits_s1 = m1(x_s2_aug2)\n",
    "        adv_logits_u1 = m1(x_u_aug2)\n",
    "        \n",
    "        adv_logits_s2 = m2(x_s1_aug1)\n",
    "        adv_logits_u2 = m2(x_u_aug1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "\n",
    "        # losses ----\n",
    "        l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "        l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "        l_diff = loss_diff(\n",
    "            logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
    "            logits_u1, logits_u2, adv_logits_u1, adv_logits_u2\n",
    "        )\n",
    "\n",
    "        total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "            # ratios  ----\n",
    "            adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "            adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "            adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "            adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "            ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "            ratio_s2 = metrics_fn[\"ratio_s\"][1](adv_pred_s2, y_s2)\n",
    "            ratio_u1 = metrics_fn[\"ratio_u\"][0](adv_pred_u1, y_u)\n",
    "            ratio_u2 = metrics_fn[\"ratio_u\"][1](adv_pred_u2, y_u)\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, avg_cot.mean, avg_diff.mean, avg_total.mean,\n",
    "                \"\", acc_s1.mean, acc_u1.mean,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "\n",
    "            logits_1 = m1(x)\n",
    "            logits_2 = m2(x)\n",
    "\n",
    "            # losses ----\n",
    "            l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "        \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_fn(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_fn(\"acc_2\", acc_2.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint_m1.step(acc_1.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
      "\n",
      "Training 1      - 100    -          4.0864 | 0.1005 | 5.7180 | 4.0864 -           0.2727    | 0.2724   - 55.4731\n",
      "\u001b[1;4mValidati 1      - 11     -          3.6099 | 0.0000 | 0.0000 | 3.6099 -           0.3457    | 0.0000   - 5.4542\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 2      - 100    -          3.6466 | 0.0869 | 6.4439 | 3.6778 -           0.3205    | 0.3287   - 15.9779\n",
      "\u001b[1;4mValidati 2      - 11     -          3.3323 | 0.0000 | 0.0000 | 3.3323 -           0.3607    | 0.0000   - 0.1077\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 3      - 100    -          3.3442 | 0.1043 | 6.9836 | 3.3833 -           0.3920    | 0.3605   - 15.5770\n",
      "\u001b[1;4mValidati 3      - 11     -          3.6732 | 0.0000 | 0.0000 | 3.6732 -           0.2014    | 0.0000   - 0.1076\u001b[0m\n",
      "Training 4      - 100    -          3.2591 | 0.1068 | 7.5747 | 3.3063 -           0.3807    | 0.3679   - 15.6534\n",
      "\u001b[1;4mValidati 4      - 11     -          3.3610 | 0.0000 | 0.0000 | 3.3610 -           0.3369    | 0.0000   - 0.1011\u001b[0m\n",
      "Training 5      - 100    -          3.1395 | 0.1018 | 7.5341 | 3.1920 -           0.3943    | 0.3875   - 15.8922\n",
      "\u001b[1;4mValidati 5      - 11     -          3.1034 | 0.0000 | 0.0000 | 3.1034 -           0.3598    | 0.0000   - 0.1210\u001b[0m\n",
      "Training 6      - 100    -          3.0692 | 0.1091 | 7.9141 | 3.1316 -           0.4227    | 0.4037   - 16.4719\n",
      "\u001b[1;4mValidati 6      - 11     -          2.9366 | 0.0000 | 0.0000 | 2.9366 -           0.4599    | 0.0000   - 0.1118\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 7      - 100    -          2.9113 | 0.1068 | 8.2434 | 2.9832 -           0.4580    | 0.4262   - 15.2606\n",
      "\u001b[1;4mValidati 7      - 11     -          3.1594 | 0.0000 | 0.0000 | 3.1594 -           0.4424    | 0.0000   - 0.1414\u001b[0m\n",
      "Training 8      - 100    -          2.8317 | 0.1067 | 8.1152 | 2.9114 -           0.4852    | 0.4526   - 15.4113\n",
      "\u001b[1;4mValidati 8      - 11     -          2.9593 | 0.0000 | 0.0000 | 2.9593 -           0.4628    | 0.0000   - 0.1150\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 9      - 100    -          2.6604 | 0.1127 | 8.7193 | 2.7560 -           0.5034    | 0.4710   - 15.4019\n",
      "\u001b[1;4mValidati 9      - 11     -          2.8711 | 0.0000 | 0.0000 | 2.8711 -           0.4538    | 0.0000   - 0.1067\u001b[0m\n",
      "Training 10     - 100    -          2.6774 | 0.1190 | 8.7311 | 2.7857 -           0.5114    | 0.4843   - 15.6999\n",
      "\u001b[1;4mValidati 10     - 11     -          2.9966 | 0.0000 | 0.0000 | 2.9966 -           0.5177    | 0.0000   - 0.1085\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 11     - 100    -          2.5402 | 0.1146 | 9.0361 | 2.6634 -           0.5409    | 0.4994   - 15.1966\n",
      "\u001b[1;4mValidati 11     - 11     -          3.2058 | 0.0000 | 0.0000 | 3.2058 -           0.4433    | 0.0000   - 0.0985\u001b[0m\n",
      "Training 12     - 100    -          2.5391 | 0.1222 | 9.0780 | 2.6787 -           0.5420    | 0.5040   - 15.5919\n",
      "\u001b[1;4mValidati 12     - 11     -          2.9586 | 0.0000 | 0.0000 | 2.9586 -           0.4871    | 0.0000   - 0.1128\u001b[0m\n",
      "Training 13     - 100    -          2.4203 | 0.1163 | 8.9372 | 2.5723 -           0.5682    | 0.5334   - 15.1582\n",
      "\u001b[1;4mValidati 13     - 11     -          2.7412 | 0.0000 | 0.0000 | 2.7412 -           0.5240    | 0.0000   - 0.1071\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 14     - 100    -          2.3015 | 0.1266 | 9.5105 | 2.4820 -           0.5830    | 0.5380   - 15.3511\n",
      "\u001b[1;4mValidati 14     - 11     -          2.7836 | 0.0000 | 0.0000 | 2.7836 -           0.5783    | 0.0000   - 0.1058\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 15     - 100    -          2.2965 | 0.1226 | 9.2738 | 2.4916 -           0.5773    | 0.5484   - 15.1597\n",
      "\u001b[1;4mValidati 15     - 11     -          2.6652 | 0.0000 | 0.0000 | 2.6652 -           0.5818    | 0.0000   - 0.0986\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 16     - 100    -          2.1490 | 0.1181 | 9.6520 | 2.3704 -           0.6307    | 0.5682   - 15.2111\n",
      "\u001b[1;4mValidati 16     - 11     -          2.7641 | 0.0000 | 0.0000 | 2.7641 -           0.5632    | 0.0000   - 0.1010\u001b[0m\n",
      "Training 17     - 100    -          2.1404 | 0.1182 | 9.5380 | 2.3830 -           0.6205    | 0.5681   - 15.4609\n",
      "\u001b[1;4mValidati 17     - 11     -          2.6955 | 0.0000 | 0.0000 | 2.6955 -           0.5221    | 0.0000   - 0.1112\u001b[0m\n",
      "Training 18     - 100    -          1.9941 | 0.1218 | 9.7421 | 2.2682 -           0.6750    | 0.5947   - 15.2038\n",
      "\u001b[1;4mValidati 18     - 11     -          2.6305 | 0.0000 | 0.0000 | 2.6305 -           0.6038    | 0.0000   - 0.1117\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 19     - 100    -          1.8719 | 0.1204 | 9.5857 | 2.1696 -           0.6818    | 0.6037   - 15.4490\n",
      "\u001b[1;4mValidati 19     - 11     -          2.6925 | 0.0000 | 0.0000 | 2.6925 -           0.6031    | 0.0000   - 0.1085\u001b[0m\n",
      "Training 20     - 100    -          1.8887 | 0.1207 | 9.6168 | 2.2174 -           0.6955    | 0.6069   - 15.4218\n",
      "\u001b[1;4mValidati 20     - 11     -          2.5568 | 0.0000 | 0.0000 | 2.5568 -           0.5945    | 0.0000   - 0.1060\u001b[0m\n",
      "Training 21     - 100    -          1.8979 | 0.1229 | 9.0456 | 2.2433 -           0.6841    | 0.6146   - 15.2512\n",
      "\u001b[1;4mValidati 21     - 11     -          2.4924 | 0.0000 | 0.0000 | 2.4924 -           0.5922    | 0.0000   - 0.1053\u001b[0m\n",
      "Training 22     - 100    -          1.7238 | 0.1198 | 8.9724 | 2.0984 -           0.7295    | 0.6235   - 15.2491\n",
      "\u001b[1;4mValidati 22     - 11     -          2.4571 | 0.0000 | 0.0000 | 2.4571 -           0.5912    | 0.0000   - 0.0965\u001b[0m\n",
      "Training 23     - 100    -          1.7088 | 0.1206 | 8.6784 | 2.1093 -           0.7068    | 0.6451   - 15.4446\n",
      "\u001b[1;4mValidati 23     - 11     -          2.4336 | 0.0000 | 0.0000 | 2.4336 -           0.5871    | 0.0000   - 0.0990\u001b[0m\n",
      "Training 24     - 100    -          1.7334 | 0.1234 | 8.1847 | 2.1542 -           0.7420    | 0.6510   - 15.2283\n",
      "\u001b[1;4mValidati 24     - 11     -          2.5946 | 0.0000 | 0.0000 | 2.5946 -           0.6183    | 0.0000   - 0.1080\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 25     - 100    -          1.6141 | 0.1199 | 8.0344 | 2.0642 -           0.7398    | 0.6432   - 15.3330\n",
      "\u001b[1;4mValidati 25     - 11     -          2.5458 | 0.0000 | 0.0000 | 2.5458 -           0.5158    | 0.0000   - 0.0979\u001b[0m\n",
      "Training 26     - 100    -          1.5206 | 0.1261 | 7.8588 | 2.0091 -           0.7432    | 0.6483   - 15.4363\n",
      "\u001b[1;4mValidati 26     - 11     -          2.7533 | 0.0000 | 0.0000 | 2.7533 -           0.4754    | 0.0000   - 0.1015\u001b[0m\n",
      "Training 27     - 100    -          1.4564 | 0.1237 | 7.7025 | 1.9778 -           0.7966    | 0.6624   - 15.4145\n",
      "\u001b[1;4mValidati 27     - 11     -          2.7274 | 0.0000 | 0.0000 | 2.7274 -           0.4952    | 0.0000   - 0.1056\u001b[0m\n",
      "Training 28     - 100    -          1.4380 | 0.1236 | 7.3596 | 1.9857 -           0.7648    | 0.6657   - 15.2248\n",
      "\u001b[1;4mValidati 28     - 11     -          2.7541 | 0.0000 | 0.0000 | 2.7541 -           0.5359    | 0.0000   - 0.1056\u001b[0m\n",
      "Training 29     - 100    -          1.3517 | 0.1221 | 7.3790 | 1.9456 -           0.7898    | 0.6758   - 15.1369\n",
      "\u001b[1;4mValidati 29     - 11     -          2.8063 | 0.0000 | 0.0000 | 2.8063 -           0.4941    | 0.0000   - 0.1079\u001b[0m\n",
      "Training 30     - 100    -          1.4048 | 0.1224 | 7.0619 | 2.0281 -           0.7773    | 0.6767   - 15.3660\n",
      "\u001b[1;4mValidati 30     - 11     -          2.7694 | 0.0000 | 0.0000 | 2.7694 -           0.5227    | 0.0000   - 0.0972\u001b[0m\n",
      "Training 31     - 100    -          1.3374 | 0.1198 | 7.0341 | 2.0062 -           0.7886    | 0.6837   - 15.0793\n",
      "\u001b[1;4mValidati 31     - 11     -          2.6731 | 0.0000 | 0.0000 | 2.6731 -           0.5017    | 0.0000   - 0.1053\u001b[0m\n",
      "Training 32     - 100    -          1.2049 | 0.1200 | 6.7612 | 1.9068 -           0.8148    | 0.6966   - 15.1169\n",
      "\u001b[1;4mValidati 32     - 11     -          2.7260 | 0.0000 | 0.0000 | 2.7260 -           0.4774    | 0.0000   - 0.1045\u001b[0m\n",
      "Training 33     - 100    -          1.2149 | 0.1177 | 6.7331 | 1.9659 -           0.8273    | 0.7002   - 15.2828\n",
      "\u001b[1;4mValidati 33     - 11     -          3.0563 | 0.0000 | 0.0000 | 3.0563 -           0.4092    | 0.0000   - 0.0966\u001b[0m\n",
      "Training 34     - 100    -          1.1665 | 0.1163 | 6.4764 | 1.9502 -           0.8205    | 0.7018   - 15.1294\n",
      "\u001b[1;4mValidati 34     - 11     -          2.7372 | 0.0000 | 0.0000 | 2.7372 -           0.4803    | 0.0000   - 0.1162\u001b[0m\n",
      "Training 35     - 100    -          1.0686 | 0.1120 | 6.3457 | 1.8904 -           0.8341    | 0.7169   - 15.0046\n",
      "\u001b[1;4mValidati 35     - 11     -          2.6374 | 0.0000 | 0.0000 | 2.6374 -           0.5163    | 0.0000   - 0.1104\u001b[0m\n",
      "Training 36     - 100    -          1.1200 | 0.1119 | 6.1620 | 1.9833 -           0.8352    | 0.7059   - 15.1599\n",
      "\u001b[1;4mValidati 36     - 11     -          2.7602 | 0.0000 | 0.0000 | 2.7602 -           0.4537    | 0.0000   - 0.1142\u001b[0m\n",
      "Training 37     - 100    -          0.9911 | 0.1091 | 5.9394 | 1.8860 -           0.8534    | 0.7242   - 15.2152\n",
      "\u001b[1;4mValidati 37     - 11     -          2.8574 | 0.0000 | 0.0000 | 2.8574 -           0.4687    | 0.0000   - 0.1014\u001b[0m\n",
      "Training 38     - 100    -          1.0539 | 0.1070 | 5.6978 | 1.9783 -           0.8557    | 0.7238   - 14.9084\n",
      "\u001b[1;4mValidati 38     - 11     -          2.8618 | 0.0000 | 0.0000 | 2.8618 -           0.5212    | 0.0000   - 0.0968\u001b[0m\n",
      "Training 39     - 100    -          0.9023 | 0.1086 | 5.6354 | 1.8862 -           0.8716    | 0.7346   - 15.1217\n",
      "\u001b[1;4mValidati 39     - 11     -          3.1067 | 0.0000 | 0.0000 | 3.1067 -           0.4324    | 0.0000   - 0.0970\u001b[0m\n",
      "Training 40     - 100    -          0.8978 | 0.1092 | 5.3896 | 1.9163 -           0.8523    | 0.7312   - 14.9072\n",
      "\u001b[1;4mValidati 40     - 11     -          3.2056 | 0.0000 | 0.0000 | 3.2056 -           0.4882    | 0.0000   - 0.0960\u001b[0m\n",
      "Training 41     - 100    -          0.8155 | 0.1050 | 5.1776 | 1.8579 -           0.8966    | 0.7422   - 15.1200\n",
      "\u001b[1;4mValidati 41     - 11     -          3.1544 | 0.0000 | 0.0000 | 3.1544 -           0.4548    | 0.0000   - 0.1112\u001b[0m\n",
      "Training 42     - 100    -          0.7485 | 0.1003 | 4.9411 | 1.8070 -           0.8943    | 0.7403   - 14.9965\n",
      "\u001b[1;4mValidati 42     - 11     -          3.2528 | 0.0000 | 0.0000 | 3.2528 -           0.4994    | 0.0000   - 0.1128\u001b[0m\n",
      "Training 43     - 100    -          0.7168 | 0.0980 | 4.9091 | 1.8285 -           0.8943    | 0.7397   - 15.1748\n",
      "\u001b[1;4mValidati 43     - 11     -          2.8727 | 0.0000 | 0.0000 | 2.8727 -           0.4799    | 0.0000   - 0.0990\u001b[0m\n",
      "Training 44     - 100    -          0.7662 | 0.0955 | 4.9411 | 1.9416 -           0.8977    | 0.7496   - 15.1838\n",
      "\u001b[1;4mValidati 44     - 11     -          3.1421 | 0.0000 | 0.0000 | 3.1421 -           0.4508    | 0.0000   - 0.0955\u001b[0m\n",
      "Training 45     - 100    -          0.7383 | 0.0938 | 4.8858 | 1.9667 -           0.9000    | 0.7526   - 15.3010\n",
      "\u001b[1;4mValidati 45     - 11     -          3.0212 | 0.0000 | 0.0000 | 3.0212 -           0.5352    | 0.0000   - 0.1001\u001b[0m\n",
      "Training 46     - 100    -          0.6469 | 0.0873 | 4.7787 | 1.8999 -           0.9091    | 0.7552   - 15.5615\n",
      "\u001b[1;4mValidati 46     - 11     -          3.1220 | 0.0000 | 0.0000 | 3.1220 -           0.4801    | 0.0000   - 0.1000\u001b[0m\n",
      "Training 47     - 100    -          0.5962 | 0.0852 | 4.8068 | 1.9154 -           0.9136    | 0.7613   - 15.1931\n",
      "\u001b[1;4mValidati 47     - 11     -          3.6017 | 0.0000 | 0.0000 | 3.6017 -           0.4774    | 0.0000   - 0.0949\u001b[0m\n",
      "Training 48     - 100    -          0.5799 | 0.0826 | 4.7107 | 1.9385 -           0.9284    | 0.7640   - 15.2405\n",
      "\u001b[1;4mValidati 48     - 11     -          3.2427 | 0.0000 | 0.0000 | 3.2427 -           0.4385    | 0.0000   - 0.1069\u001b[0m\n",
      "Training 49     - 100    -          0.5509 | 0.0806 | 4.7002 | 1.9692 -           0.9193    | 0.7619   - 15.5352\n",
      "\u001b[1;4mValidati 49     - 11     -          3.2390 | 0.0000 | 0.0000 | 3.2390 -           0.4711    | 0.0000   - 0.1060\u001b[0m\n",
      "Training 50     - 100    -          0.4288 | 0.0774 | 4.6905 | 1.9011 -           0.9318    | 0.7708   - 15.4003\n",
      "\u001b[1;4mValidati 50     - 11     -          3.4955 | 0.0000 | 0.0000 | 3.4955 -           0.4311    | 0.0000   - 0.1082\u001b[0m\n",
      "Training 51     - 100    -          0.4853 | 0.0771 | 4.7064 | 2.0321 -           0.9341    | 0.7635   - 15.7728\n",
      "\u001b[1;4mValidati 51     - 11     -          3.6298 | 0.0000 | 0.0000 | 3.6298 -           0.3302    | 0.0000   - 0.1108\u001b[0m\n",
      "Training 52     - 100    -          0.5138 | 0.0787 | 4.6871 | 2.1368 -           0.9182    | 0.7738   - 15.3917\n",
      "\u001b[1;4mValidati 52     - 11     -          3.5637 | 0.0000 | 0.0000 | 3.5637 -           0.4594    | 0.0000   - 0.1106\u001b[0m\n",
      "Training 53     - 100    -          0.4698 | 0.0750 | 4.6528 | 2.1372 -           0.9250    | 0.7738   - 15.2637\n",
      "\u001b[1;4mValidati 53     - 11     -          3.1521 | 0.0000 | 0.0000 | 3.1521 -           0.4548    | 0.0000   - 0.0949\u001b[0m\n",
      "Training 54     - 100    -          0.4420 | 0.0702 | 4.6267 | 2.1480 -           0.9432    | 0.7824   - 15.2693\n",
      "\u001b[1;4mValidati 54     - 11     -          3.7968 | 0.0000 | 0.0000 | 3.7968 -           0.3790    | 0.0000   - 0.1062\u001b[0m\n",
      "Training 55     - 100    -          0.4242 | 0.0703 | 4.6220 | 2.2017 -           0.9466    | 0.7819   - 15.1439\n",
      "\u001b[1;4mValidati 55     - 11     -          3.3058 | 0.0000 | 0.0000 | 3.3058 -           0.3676    | 0.0000   - 0.0981\u001b[0m\n",
      "Training 56     - 100    -          0.4221 | 0.0668 | 4.6172 | 2.2489 -           0.9375    | 0.7801   - 14.9647\n",
      "\u001b[1;4mValidati 56     - 11     -          3.2437 | 0.0000 | 0.0000 | 3.2437 -           0.4520    | 0.0000   - 0.1047\u001b[0m\n",
      "Training 57     - 100    -          0.3860 | 0.0655 | 4.6098 | 2.2732 -           0.9341    | 0.7846   - 15.1597\n",
      "\u001b[1;4mValidati 57     - 11     -          3.1958 | 0.0000 | 0.0000 | 3.1958 -           0.4147    | 0.0000   - 0.1019\u001b[0m\n",
      "Training 58     - 100    -          0.3810 | 0.0618 | 4.6299 | 2.3208 -           0.9432    | 0.7871   - 15.4382\n",
      "\u001b[1;4mValidati 58     - 11     -          3.3386 | 0.0000 | 0.0000 | 3.3386 -           0.4073    | 0.0000   - 0.1099\u001b[0m\n",
      "Training 59     - 100    -          0.3199 | 0.0585 | 4.6056 | 2.2987 -           0.9591    | 0.7888   - 15.2928\n",
      "\u001b[1;4mValidati 59     - 11     -          3.2689 | 0.0000 | 0.0000 | 3.2689 -           0.4562    | 0.0000   - 0.1163\u001b[0m\n",
      "Training 60     - 100    -          0.3922 | 0.0584 | 4.6747 | 2.4624 -           0.9534    | 0.7883   - 15.5047\n",
      "\u001b[1;4mValidati 60     - 11     -          3.5326 | 0.0000 | 0.0000 | 3.5326 -           0.3932    | 0.0000   - 0.1097\u001b[0m\n",
      "Training 61     - 100    -          0.3138 | 0.0570 | 4.5949 | 2.4113 -           0.9500    | 0.7917   - 15.1560\n",
      "\u001b[1;4mValidati 61     - 11     -          3.1912 | 0.0000 | 0.0000 | 3.1912 -           0.4338    | 0.0000   - 0.0981\u001b[0m\n",
      "Training 62     - 100    -          0.3340 | 0.0542 | 4.6194 | 2.4853 -           0.9466    | 0.7911   - 15.1960\n",
      "\u001b[1;4mValidati 62     - 11     -          3.4042 | 0.0000 | 0.0000 | 3.4042 -           0.4292    | 0.0000   - 0.0970\u001b[0m\n",
      "Training 63     - 100    -          0.3162 | 0.0541 | 4.6056 | 2.5240 -           0.9511    | 0.7970   - 15.2380\n",
      "\u001b[1;4mValidati 63     - 11     -          3.5308 | 0.0000 | 0.0000 | 3.5308 -           0.3698    | 0.0000   - 0.1086\u001b[0m\n",
      "Training 64     - 100    -          0.2975 | 0.0524 | 4.6031 | 2.5524 -           0.9511    | 0.7931   - 15.1432\n",
      "\u001b[1;4mValidati 64     - 11     -          3.4155 | 0.0000 | 0.0000 | 3.4155 -           0.3828    | 0.0000   - 0.0955\u001b[0m\n",
      "Training 65     - 100    -          0.2376 | 0.0486 | 4.5866 | 2.5131 -           0.9648    | 0.7996   - 15.2112\n",
      "\u001b[1;4mValidati 65     - 11     -          3.4733 | 0.0000 | 0.0000 | 3.4733 -           0.3798    | 0.0000   - 0.1046\u001b[0m\n",
      "Training 66     - 100    -          0.3117 | 0.0494 | 4.5742 | 2.6445 -           0.9557    | 0.8030   - 15.1706\n",
      "\u001b[1;4mValidati 66     - 11     -          3.5632 | 0.0000 | 0.0000 | 3.5632 -           0.3884    | 0.0000   - 0.1069\u001b[0m\n",
      "Training 67     - 100    -          0.3033 | 0.0476 | 4.5593 | 2.6678 -           0.9500    | 0.8014   - 14.9913\n",
      "\u001b[1;4mValidati 67     - 11     -          3.4023 | 0.0000 | 0.0000 | 3.4023 -           0.4201    | 0.0000   - 0.1150\u001b[0m\n",
      "Training 68     - 100    -          0.2957 | 0.0488 | 4.5343 | 2.7101 -           0.9739    | 0.7990   - 15.1550\n",
      "\u001b[1;4mValidati 68     - 11     -          3.5577 | 0.0000 | 0.0000 | 3.5577 -           0.3627    | 0.0000   - 0.1067\u001b[0m\n",
      "Training 69     - 100    -          0.2511 | 0.0461 | 4.5552 | 2.6980 -           0.9591    | 0.8027   - 15.3426\n",
      "\u001b[1;4mValidati 69     - 11     -          3.3699 | 0.0000 | 0.0000 | 3.3699 -           0.3828    | 0.0000   - 0.1135\u001b[0m\n",
      "Training 70     - 100    -          0.2512 | 0.0426 | 4.5629 | 2.7144 -           0.9659    | 0.8070   - 15.0701\n",
      "\u001b[1;4mValidati 70     - 11     -          3.3637 | 0.0000 | 0.0000 | 3.3637 -           0.3750    | 0.0000   - 0.1020\u001b[0m\n",
      "Training 71     - 100    -          0.2315 | 0.0413 | 4.5570 | 2.7210 -           0.9693    | 0.8053   - 15.2984\n",
      "\u001b[1;4mValidati 71     - 11     -          3.3930 | 0.0000 | 0.0000 | 3.3930 -           0.3848    | 0.0000   - 0.1065\u001b[0m\n",
      "Training 72     - 100    -          0.1865 | 0.0395 | 4.5682 | 2.7012 -           0.9795    | 0.8042   - 15.2415\n",
      "\u001b[1;4mValidati 72     - 11     -          3.4771 | 0.0000 | 0.0000 | 3.4771 -           0.4120    | 0.0000   - 0.1145\u001b[0m\n",
      "Training 73     - 100    -          0.1864 | 0.0382 | 4.5536 | 2.7159 -           0.9807    | 0.8017   - 15.2879\n",
      "\u001b[1;4mValidati 73     - 11     -          3.6004 | 0.0000 | 0.0000 | 3.6004 -           0.3765    | 0.0000   - 0.0967\u001b[0m\n",
      "Training 74     - 100    -          0.2544 | 0.0385 | 4.5535 | 2.8158 -           0.9557    | 0.8115   - 15.0274\n",
      "\u001b[1;4mValidati 74     - 11     -          3.7204 | 0.0000 | 0.0000 | 3.7204 -           0.3524    | 0.0000   - 0.1171\u001b[0m\n",
      "Training 75     - 100    -          0.2313 | 0.0370 | 4.5356 | 2.7959 -           0.9705    | 0.8080   - 15.2343\n",
      "\u001b[1;4mValidati 75     - 11     -          3.6435 | 0.0000 | 0.0000 | 3.6435 -           0.3437    | 0.0000   - 0.1053\u001b[0m\n",
      "Training 76     - 100    -          0.2021 | 0.0372 | 4.5821 | 2.8142 -           0.9807    | 0.8110   - 15.2017\n",
      "\u001b[1;4mValidati 76     - 11     -          3.6927 | 0.0000 | 0.0000 | 3.6927 -           0.3611    | 0.0000   - 0.1127\u001b[0m\n",
      "Training 77     - 100    -          0.2117 | 0.0350 | 4.5792 | 2.8187 -           0.9784    | 0.8086   - 15.5145\n",
      "\u001b[1;4mValidati 77     - 11     -          3.3357 | 0.0000 | 0.0000 | 3.3357 -           0.3307    | 0.0000   - 0.1070\u001b[0m\n",
      "Training 78     - 100    -          0.1946 | 0.0344 | 4.6073 | 2.8236 -           0.9739    | 0.8062   - 15.3057\n",
      "\u001b[1;4mValidati 78     - 11     -          3.5594 | 0.0000 | 0.0000 | 3.5594 -           0.3292    | 0.0000   - 0.1062\u001b[0m\n",
      "Training 79     - 100    -          0.2066 | 0.0338 | 4.5892 | 2.8307 -           0.9716    | 0.8099   - 15.3464\n",
      "\u001b[1;4mValidati 79     - 11     -          3.6246 | 0.0000 | 0.0000 | 3.6246 -           0.3723    | 0.0000   - 0.1061\u001b[0m\n",
      "Training 80     - 100    -          0.1888 | 0.0339 | 4.5133 | 2.7828 -           0.9750    | 0.8077   - 15.2827\n",
      "\u001b[1;4mValidati 80     - 11     -          3.4778 | 0.0000 | 0.0000 | 3.4778 -           0.3623    | 0.0000   - 0.1002\u001b[0m\n",
      "Training 81     - 100    -          0.1746 | 0.0329 | 4.5374 | 2.7719 -           0.9750    | 0.8137   - 15.0153\n",
      "\u001b[1;4mValidati 81     - 11     -          3.6333 | 0.0000 | 0.0000 | 3.6333 -           0.3663    | 0.0000   - 0.0992\u001b[0m\n",
      "Training 82     - 100    -          0.1891 | 0.0306 | 4.5727 | 2.7816 -           0.9784    | 0.8100   - 15.4538\n",
      "\u001b[1;4mValidati 82     - 11     -          3.5756 | 0.0000 | 0.0000 | 3.5756 -           0.3916    | 0.0000   - 0.1091\u001b[0m\n",
      "Training 83     - 100    -          0.2254 | 0.0322 | 4.5492 | 2.8221 -           0.9739    | 0.8129   - 15.3250\n",
      "\u001b[1;4mValidati 83     - 11     -          3.6334 | 0.0000 | 0.0000 | 3.6334 -           0.3417    | 0.0000   - 0.1082\u001b[0m\n",
      "Training 84     - 100    -          0.1794 | 0.0311 | 4.5426 | 2.7619 -           0.9773    | 0.8121   - 15.3812\n",
      "\u001b[1;4mValidati 84     - 11     -          3.6337 | 0.0000 | 0.0000 | 3.6337 -           0.3071    | 0.0000   - 0.1044\u001b[0m\n",
      "Training 85     - 100    -          0.1741 | 0.0293 | 4.5782 | 2.7567 -           0.9761    | 0.8127   - 15.0085\n",
      "\u001b[1;4mValidati 85     - 11     -          3.4678 | 0.0000 | 0.0000 | 3.4678 -           0.3543    | 0.0000   - 0.0969\u001b[0m\n",
      "Training 86     - 100    -          0.1712 | 0.0301 | 4.5524 | 2.7488 -           0.9795    | 0.8164   - 15.0643\n",
      "\u001b[1;4mValidati 86     - 11     -          3.5470 | 0.0000 | 0.0000 | 3.5470 -           0.3888    | 0.0000   - 0.1023\u001b[0m\n",
      "Training 87     - 100    -          0.1443 | 0.0304 | 4.5817 | 2.7396 -           0.9784    | 0.8147   - 15.0696\n",
      "\u001b[1;4mValidati 87     - 11     -          3.5569 | 0.0000 | 0.0000 | 3.5569 -           0.3228    | 0.0000   - 0.0966\u001b[0m\n",
      "Training 88     - 3      -          0.2418 | 0.0322 | 4.5198 | 2.8240 -           0.9394    | 0.7903   - 2.4064\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
