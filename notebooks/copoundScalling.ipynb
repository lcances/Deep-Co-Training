{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from datasetManager import DatasetManager\n",
    "from generators import Dataset\n",
    "import signal_augmentations as sa \n",
    "from models import cnn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.value = 0\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.count += 1\n",
    "\n",
    "        \n",
    "class BinaryAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = (y_pred>0.5).float()\n",
    "            correct = (y_pred == y_true).float().sum()\n",
    "            self.value = correct/ (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "        \n",
    "class CategoricalAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.value = torch.mean((y_true == y_pred).float())\n",
    "            self.accumulate_value += self.value\n",
    "\n",
    "            return self.accumulate_value / self.count\n",
    "\n",
    "        \n",
    "class Ratio(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_adv_pred):\n",
    "        super().__call__()\n",
    "        \n",
    "        results = zip(y_pred, y_adv_pred)\n",
    "        results_bool = [int(r[0] != r[1]) for r in results]\n",
    "        self.value = sum(results_bool) / len(results_bool) * 100\n",
    "        self.accumulate_value += self.value\n",
    "        \n",
    "        return self.accumulate_value / self.count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## set seeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reset_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "reset_seed(1324)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class crnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(crnn, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvBNReLU(1, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,2), stride=(4,2)),\n",
    "            ConvBNReLU(64, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,2), stride=(4,2)),\n",
    "            ConvBNReLU(64, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,1), stride=(4,1)),\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.GRU(64, 64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.strong = nn.Sequential(\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        \n",
    "        x = x.squeeze(dim=-2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x, h = self.rnn(x)\n",
    "        \n",
    "        strong = self.strong(x)\n",
    "        \n",
    "        weak = strong.permute(0, 2, 1)\n",
    "        weak = F.avg_pool1d(weak, kernel_size=weak.size()[2:])\n",
    "#         max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        weak = weak.view(-1, weak.shape[1])\n",
    "        \n",
    "        \n",
    "        return weak"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultisampleDropout2d(nn.Module):\n",
    "    \"\"\"https://arxiv.org/pdf/1905.09788.pdf\"\"\"\n",
    "    def __init__(self, ratio, nb_sample):\n",
    "        super(MultisampleDropout2d, self).__init__()\n",
    "        self.nb_sample = nb_sample\n",
    "        \n",
    "        self.dropouts = [nn.Dropout2d(ratio) for _ in range(nb_sample)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d = [dropout(x) for dropout in self.dropouts]\n",
    "        return torch.mean(torch.stack(d, dim=0), dim=0)\n",
    "    \n",
    "class MultisampleDropout1d(nn.Module):\n",
    "    \"\"\"https://arxiv.org/pdf/1905.09788.pdf\"\"\"\n",
    "    def __init__(self, ratio, nb_sample):\n",
    "        super(MultisampleDropout1d, self).__init__()\n",
    "        self.nb_sample = nb_sample\n",
    "        \n",
    "        self.dropouts = [nn.Dropout(ratio) for _ in range(nb_sample)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d = [dropout(x) for dropout in self.dropouts]\n",
    "        return torch.mean(torch.stack(d, dim=0), dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_size, out_size, t, kernel_size, stride, padding):\n",
    "        super(MBConv, self).__init__()\n",
    "        expand_dim = in_size * t\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_size, expand_dim, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(expand_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(expand_dim, expand_dim, kernel_size=kernel_size, stride=stride, padding=padding, groups=expand_dim),\n",
    "            nn.BatchNorm2d(expand_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(expand_dim, out_size, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.stride == 1:\n",
    "            return x + self.conv(x)\n",
    "        return self.conv(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 conv_input_dim: tuple = (64, 431),\n",
    "                 conv_in_size: list = [1, 64, 64],\n",
    "                 conv_out_size: list = [64, 64, 64],\n",
    "                 t = [1, 6, 6],\n",
    "                 s = [1, 2, 2],\n",
    "                 n = [1, 2, 2],\n",
    "                ):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.i =0\n",
    "        \n",
    "        self.conv_input_dim = conv_input_dim\n",
    "        self.conv_in_size = conv_in_size\n",
    "        self.conv_out_size = conv_out_size\n",
    "        self.t = t\n",
    "        \n",
    "        conv_layers = []\n",
    "        for i in range(len(conv_in_size)):\n",
    "            if i == 0:\n",
    "                conv_layers.append(nn.Conv2d(conv_in_size[i], conv_out_size[i], 3, 1, 1))\n",
    "                continue\n",
    "            \n",
    "            conv_layers.append( MBConv(conv_in_size[i], conv_out_size[i], t[i], 3, s[i], 1) )\n",
    "            for j in range(n[i]-1):\n",
    "                conv_layers.append( MBConv(conv_out_size[i], conv_out_size[i], t[i], 3, 1, 1) )\n",
    "    \n",
    "        self.features = nn.Sequential(*conv_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            MultisampleDropout2d(0.2, 8),\n",
    "            nn.Conv2d(self.conv_out_size[-1], 10, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.AdaptiveMaxPool2d((1, 1)),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "#         x = x.view(-1, 1, self.conv_input_dim[0], self.conv_input_dim[1])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        x= x.view(-1, x.shape[1])\n",
    "        \n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prep model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# cnn\n",
    "model_func = cnn\n",
    "m1 = model_func()\n",
    "\n",
    "m1.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 173), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(m1, input_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prep data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "\n",
    "dataset = DatasetManager(metadata_root, audio_root, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and val loaders\n",
    "train_dataset = Dataset(dataset, train=True, val=False, augments=[], cached=False)\n",
    "val_dataset = Dataset(dataset, train=True, val=False, augments=[], cached=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prep training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "m1 = model_func()\n",
    "m1.cuda()\n",
    "\n",
    "# loss and optimizer\n",
    "criterion_bce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    m1.parameters(),\n",
    "    weight_decay=1e-3,\n",
    "    lr=0.05\n",
    ")\n",
    "\n",
    "# Augmentation to use\n",
    "augments = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epoch = 100\n",
    "batch_size = 64\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# scheduler\n",
    "lr_lambda = lambda epoch: 0.05 * (np.cos(np.pi * epoch / nb_epoch) + 1)\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "callbacks = [lr_scheduler]\n",
    "# callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"%s_%s_Cosd-lr_sgd-0.01lr-wd0.001_%de_no_augment\" % ( get_datetime(), model_func.__name__, nb_epoch )\n",
    "tensorboard = SummaryWriter(log_dir=\"tensorboard/%s\" % title, comment=model_func.__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc_func = CategoricalAccuracy()\n",
    "\n",
    "for epoch in tqdm.tqdm_notebook(range(nb_epoch)):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    acc_func.reset()\n",
    "\n",
    "    m1.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(training_loader):        \n",
    "        # Transfer to GPU\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        # predict\n",
    "        logits = m1(X)\n",
    "\n",
    "        weak_loss = criterion_bce(logits, y)\n",
    "\n",
    "        total_loss = weak_loss\n",
    "\n",
    "        # calc metrics\n",
    "#         y_pred = torch.log_softmax(logits, dim=1)\n",
    "        _, y_pred = torch.max(logits, 1)\n",
    "        acc = acc_func(y_pred, y)\n",
    "\n",
    "        # ======== back propagation ========\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== history ========\n",
    "        print(\"Epoch {}, {:d}% \\t ce: {:.4f} - acc: {:.4f} - took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            total_loss.item(),\n",
    "            acc,\n",
    "            time.time() - start_time\n",
    "        ),end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\n",
    "    tensorboard.add_scalar('train/ce', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", 100. * acc, epoch )\n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # reset metrics\n",
    "        acc_func.reset()\n",
    "        m1.eval()\n",
    "\n",
    "        for X_val, y_val in val_loader:\n",
    "            # Transfer to GPU\n",
    "            X_val = X_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "\n",
    "#             y_weak_val_pred, _ = model(X_val)\n",
    "            logits = m1(X_val)\n",
    "\n",
    "            # calc loss\n",
    "            weak_loss_val = criterion_bce(logits, y_val)\n",
    "\n",
    "            # metrics\n",
    "#             y_val_pred =torch.log_softmax(logits, dim=1)\n",
    "            _, y_val_pred = torch.max(logits, 1)\n",
    "            acc_val = acc_func(y_val_pred, y_val)\n",
    "\n",
    "            #Print statistics\n",
    "            print(\"Epoch {}, {:d}% \\t ce: {:.4f} - acc: {:.4f} - ce val: {:.4f} - acc val: {:.4f} - took: {:.2f}s\".format(\n",
    "                epoch+1,\n",
    "                int(100 * (i+1) / nb_batch),\n",
    "                total_loss.item(),\n",
    "                acc,\n",
    "                weak_loss_val.item(),\n",
    "                acc_val,\n",
    "                time.time() - start_time\n",
    "            ),end=\"\\r\")\n",
    "\n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('validation/ce', weak_loss_val.item(), epoch)\n",
    "        tensorboard.add_scalar(\"validation/acc\", 100. * acc_val, epoch )\n",
    "\n",
    "    for callback in callbacks:\n",
    "        callback.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}