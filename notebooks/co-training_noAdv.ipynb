{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from DCT.util.utils import reset_seed, get_datetime, get_model_from_name, ZipCycle\n",
    "\n",
    "from DCT.ramps import Warmup, sigmoid_rampup\n",
    "from DCT.losses import loss_cot, loss_diff, loss_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets/ubs8k\", type=str)\n",
    "parser.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
    "parser.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "parser.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "parser.add_argument(\"--model\", default=\"cnn03\", type=str)\n",
    "parser.add_argument(\"--batch_size\", default=100, type=int)\n",
    "parser.add_argument(\"--nb_epoch\", default=100, type=int)\n",
    "parser.add_argument(\"--learning_rate\", default=0.003, type=int)\n",
    "\n",
    "parser.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "parser.add_argument(\"--warmup_length\", default=80, type=int)\n",
    "\n",
    "parser.add_argument(\"--augment\", action=\"append\", help=\"augmentation. use as if python script\")\n",
    "parser.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "parser.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "\n",
    "parser.add_argument(\"--checkpoint_path\", default=\"../model_save/ubs8k/deep-co-training_noAdv\", type=str)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--tensorboard_path\", default=\"../tensorboard/ubs8k/deep-co-training_noAdv\", type=str)\n",
    "parser.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ded65d2af34671b8580b5e2e54e662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "audio_root = os.path.join(args.dataset_root, \"audio\")\n",
    "metadata_root = os.path.join(args.dataset_root, \"metadata\")\n",
    "all_folds = args.train_folds + args.val_folds\n",
    "\n",
    "manager = DatasetManager(\n",
    "    metadata_root, audio_root,\n",
    "    folds=all_folds,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the sampler with the specified number of supervised file\n",
    "train_dataset = Dataset(manager, folds=args.train_folds, cached=True)\n",
    "val_dataset = Dataset(manager, folds=args.val_folds, cached=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = get_model_from_name(args.model)\n",
    "\n",
    "m1, m2 = model_func(manager), model_func(manager)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcances/sync/Documents_sync/Projet/Datasets/UrbanSound8K/ubs8k/datasets.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.y[\"idx\"] = list(range(len(self.y)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s_idx, u_idx = train_dataset.split_s_u(args.supervised_ratio)\n",
    "\n",
    "# Calc the size of the Supervised and Unsupervised batch\n",
    "nb_s_file = len(s_idx)\n",
    "nb_u_file = len(u_idx)\n",
    "\n",
    "ratio = nb_s_file / nb_u_file\n",
    "s_batch_size = int(np.floor(args.batch_size * ratio))\n",
    "u_batch_size = int(np.ceil(args.batch_size * (1 - ratio)))\n",
    "\n",
    "\n",
    "# create the sampler, the loader and \"zip\" them\n",
    "sampler_s1 = data.SubsetRandomSampler(s_idx)\n",
    "sampler_s2 = data.SubsetRandomSampler(s_idx)\n",
    "sampler_u = data.SubsetRandomSampler(u_idx)\n",
    "\n",
    "train_loader_s1 = data.DataLoader(train_dataset, batch_size=s_batch_size, sampler=sampler_s1)\n",
    "train_loader_s2 = data.DataLoader(train_dataset, batch_size=s_batch_size, sampler=sampler_s2)\n",
    "train_loader_u = data.DataLoader(train_dataset, batch_size=u_batch_size, sampler=sampler_u)\n",
    "\n",
    "train_loader = ZipCycle([train_loader_s1, train_loader_s2, train_loader_u])\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1f\" % (get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1f\" % (model_func.__name__, args.supervised_ratio)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (args.tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "\n",
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# Optimizer\n",
    "params = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
    "grad_scaler = amp.GradScaler()\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "\n",
    "# callback\n",
    "lr_lambda = lambda epoch: (1.0 + np.cos((epoch-1) * np.pi / args.nb_epoch))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "callbacks = [lr_scheduler, lambda_cot]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint_m1 = CheckPoint(m1, optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (args.checkpoint_path, checkpoint_title))\n",
    "checkpoint_m2 = CheckPoint(m2, optimizer, mode=\"max\", name=\"%s/%s_m2.torch\" % (args.checkpoint_path, checkpoint_title))\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        x_s1, x_s2, x_u = x_s1.cuda(), x_s2.cuda(), x_u.cuda()\n",
    "        y_s1, y_s2, y_u = y_s1.cuda(), y_s2.cuda(), y_u.cuda()\n",
    "\n",
    "        logits_s1 = m1(x_s1)\n",
    "        logits_s2 = m2(x_s2)\n",
    "        logits_u1 = m1(x_u)\n",
    "        logits_u2 = m2(x_u)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # losses ----\n",
    "        l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "        l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "        total_loss = l_sup + lambda_cot() * l_cot\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        grad_scaler.scale(total_loss).backward()\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, avg_cot.mean, avg_total.mean,\n",
    "                \"\", acc_s1.mean, acc_u1.mean,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits_1 = m1(x)\n",
    "            logits_2 = m2(x)\n",
    "\n",
    "            # losses ----\n",
    "            l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(val_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_fn(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_fn(\"acc_2\", acc_2.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint_m1.step(acc_1.mean)\n",
    "    checkpoint_m2.step(acc_2.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.as_tensor(1.0000e+00)\n",
    "torch.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
      "\n",
      "Training 1      - 100    -          4.1451 | 0.0832 | 4.1451 -           0.2386    | 0.2382   - 47.9227\n",
      "\u001b[1;4mValidati 1      - 100    -          3.5694 | 0.0000 | 3.5694 -           0.2717    | 0.0000   - 4.7007\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 2      - 100    -          3.8106 | 0.0959 | 3.8179 -           0.2864    | 0.2990   - 3.2864\n",
      "\u001b[1;4mValidati 2      - 100    -          3.2841 | 0.0000 | 3.2841 -           0.3588    | 0.0000   - 0.0882\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 3      - 100    -          3.5640 | 0.1000 | 3.5726 -           0.3364    | 0.3458   - 3.1597\n",
      "\u001b[1;4mValidati 3      - 100    -          3.4906 | 0.0000 | 3.4906 -           0.3705    | 0.0000   - 0.0980\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 4      - 100    -          3.4923 | 0.1033 | 3.5023 -           0.3477    | 0.3742   - 3.1734\n",
      "\u001b[1;4mValidati 4      - 100    -          3.2116 | 0.0000 | 3.2116 -           0.3998    | 0.0000   - 0.0858\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 5      - 100    -          3.2331 | 0.0985 | 3.2439 -           0.4239    | 0.4135   - 3.1591\n",
      "\u001b[1;4mValidati 5      - 100    -          3.0990 | 0.0000 | 3.0990 -           0.4437    | 0.0000   - 0.0961\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 6      - 100    -          3.1212 | 0.1106 | 3.1349 -           0.4182    | 0.4390   - 3.1970\n",
      "\u001b[1;4mValidati 6      - 100    -          2.9277 | 0.0000 | 2.9277 -           0.4941    | 0.0000   - 0.0847\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 7      - 100    -          3.0294 | 0.1117 | 3.0449 -           0.4511    | 0.4316   - 3.3988\n",
      "\u001b[1;4mValidati 7      - 100    -          2.8423 | 0.0000 | 2.8423 -           0.4236    | 0.0000   - 0.0953\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 8      - 100    -          2.9496 | 0.1141 | 2.9674 -           0.4591    | 0.4534   - 3.4579\n",
      "\u001b[1;4mValidati 8      - 100    -          2.9999 | 0.0000 | 2.9999 -           0.4353    | 0.0000   - 0.1039\u001b[0m\n",
      "Training 9      - 100    -          2.8330 | 0.1147 | 2.8530 -           0.4773    | 0.4610   - 3.4182\n",
      "\u001b[1;4mValidati 9      - 100    -          2.9477 | 0.0000 | 2.9477 -           0.5115    | 0.0000   - 0.0935\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 10     - 100    -          2.7353 | 0.1183 | 2.7583 -           0.4773    | 0.4705   - 3.3848\n",
      "\u001b[1;4mValidati 10     - 100    -          3.0467 | 0.0000 | 3.0467 -           0.5357    | 0.0000   - 0.0929\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 11     - 100    -          2.7847 | 0.1237 | 2.8116 -           0.4932    | 0.4845   - 3.3682\n",
      "\u001b[1;4mValidati 11     - 100    -          2.8156 | 0.0000 | 2.8156 -           0.4737    | 0.0000   - 0.0953\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 12     - 100    -          2.5432 | 0.1196 | 2.5722 -           0.5125    | 0.4967   - 3.4989\n",
      "\u001b[1;4mValidati 12     - 100    -          2.6685 | 0.0000 | 2.6685 -           0.5313    | 0.0000   - 0.1057\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 13     - 100    -          2.4954 | 0.1199 | 2.5278 -           0.5330    | 0.5063   - 3.5173\n",
      "\u001b[1;4mValidati 13     - 100    -          2.8109 | 0.0000 | 2.8109 -           0.5207    | 0.0000   - 0.1062\u001b[0m\n",
      "Training 14     - 100    -          2.3754 | 0.1243 | 2.4127 -           0.5659    | 0.5239   - 3.4384\n",
      "\u001b[1;4mValidati 14     - 100    -          2.8535 | 0.0000 | 2.8535 -           0.5678    | 0.0000   - 0.1052\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 15     - 100    -          2.2013 | 0.1235 | 2.2424 -           0.5875    | 0.5445   - 3.4427\n",
      "\u001b[1;4mValidati 15     - 100    -          2.7142 | 0.0000 | 2.7142 -           0.5605    | 0.0000   - 0.0928\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 16     - 100    -          2.3303 | 0.1242 | 2.3760 -           0.5761    | 0.5562   - 3.1265\n",
      "\u001b[1;4mValidati 16     - 100    -          2.7605 | 0.0000 | 2.7605 -           0.5450    | 0.0000   - 0.0859\u001b[0m\n",
      "Training 17     - 100    -          2.1612 | 0.1193 | 2.2098 -           0.6034    | 0.5717   - 3.3312\n",
      "\u001b[1;4mValidati 17     - 100    -          2.4995 | 0.0000 | 2.4995 -           0.5767    | 0.0000   - 0.1051\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 18     - 100    -          2.0756 | 0.1218 | 2.1304 -           0.6273    | 0.5666   - 3.4715\n",
      "\u001b[1;4mValidati 18     - 100    -          2.5683 | 0.0000 | 2.5683 -           0.5591    | 0.0000   - 0.0927\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 19     - 100    -          2.0278 | 0.1180 | 2.0864 -           0.6239    | 0.5798   - 3.4419\n",
      "\u001b[1;4mValidati 19     - 100    -          2.6265 | 0.0000 | 2.6265 -           0.5750    | 0.0000   - 0.0971\u001b[0m\n",
      "Training 20     - 100    -          1.9653 | 0.1247 | 2.0335 -           0.6545    | 0.5770   - 3.4532\n",
      "\u001b[1;4mValidati 20     - 100    -          2.7336 | 0.0000 | 2.7336 -           0.5586    | 0.0000   - 0.0931\u001b[0m\n",
      "Training 21     - 100    -          1.8018 | 0.1272 | 1.8782 -           0.6568    | 0.6005   - 3.4247\n",
      "\u001b[1;4mValidati 21     - 100    -          2.6740 | 0.0000 | 2.6740 -           0.5875    | 0.0000   - 0.0929\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 22     - 100    -          1.8661 | 0.1250 | 1.9485 -           0.6466    | 0.6041   - 3.4940\n",
      "\u001b[1;4mValidati 22     - 100    -          2.5757 | 0.0000 | 2.5757 -           0.6138    | 0.0000   - 0.1050\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 23     - 100    -          1.7544 | 0.1153 | 1.8377 -           0.6795    | 0.6325   - 3.4236\n",
      "\u001b[1;4mValidati 23     - 100    -          2.5650 | 0.0000 | 2.5650 -           0.6078    | 0.0000   - 0.0927\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 24     - 100    -          1.7338 | 0.1270 | 1.8342 -           0.6830    | 0.6327   - 3.3925\n",
      "\u001b[1;4mValidati 24     - 100    -          2.5917 | 0.0000 | 2.5917 -           0.6038    | 0.0000   - 0.0924\u001b[0m\n",
      "Training 25     - 100    -          1.6723 | 0.1138 | 1.7705 -           0.7239    | 0.6436   - 3.3940\n",
      "\u001b[1;4mValidati 25     - 100    -          2.4002 | 0.0000 | 2.4002 -           0.6438    | 0.0000   - 0.0919\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 26     - 100    -          1.6168 | 0.1183 | 1.7282 -           0.6955    | 0.6345   - 3.3717\n",
      "\u001b[1;4mValidati 26     - 100    -          2.9425 | 0.0000 | 2.9425 -           0.5534    | 0.0000   - 0.0926\u001b[0m\n",
      "Training 27     - 100    -          1.4128 | 0.1165 | 1.5322 -           0.7489    | 0.6540   - 3.4513\n",
      "\u001b[1;4mValidati 27     - 100    -          2.6475 | 0.0000 | 2.6475 -           0.5872    | 0.0000   - 0.1034\u001b[0m\n",
      "Training 28     - 100    -          1.4455 | 0.1198 | 1.5790 -           0.7670    | 0.6760   - 3.4352\n",
      "\u001b[1;4mValidati 28     - 100    -          2.8363 | 0.0000 | 2.8363 -           0.6336    | 0.0000   - 0.0926\u001b[0m\n",
      "Training 29     - 100    -          1.4466 | 0.1206 | 1.5924 -           0.7614    | 0.6732   - 3.4129\n",
      "\u001b[1;4mValidati 29     - 100    -          2.6335 | 0.0000 | 2.6335 -           0.6800    | 0.0000   - 0.0933\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 30     - 100    -          1.4486 | 0.1208 | 1.6069 -           0.7580    | 0.6811   - 3.4025\n",
      "\u001b[1;4mValidati 30     - 100    -          2.6083 | 0.0000 | 2.6083 -           0.6734    | 0.0000   - 0.0928\u001b[0m\n",
      "Training 31     - 100    -          1.3672 | 0.1138 | 1.5285 -           0.7682    | 0.6794   - 3.3857\n",
      "\u001b[1;4mValidati 31     - 100    -          2.7252 | 0.0000 | 2.7252 -           0.6695    | 0.0000   - 0.0925\u001b[0m\n",
      "Training 32     - 100    -          1.4520 | 0.1203 | 1.6363 -           0.7648    | 0.6754   - 3.3624\n",
      "\u001b[1;4mValidati 32     - 100    -          2.7210 | 0.0000 | 2.7210 -           0.6592    | 0.0000   - 0.0920\u001b[0m\n",
      "Training 33     - 100    -          1.2594 | 0.1148 | 1.4492 -           0.7852    | 0.6900   - 3.4163\n",
      "\u001b[1;4mValidati 33     - 100    -          2.9217 | 0.0000 | 2.9217 -           0.6736    | 0.0000   - 0.0976\u001b[0m\n",
      "Training 34     - 100    -          1.2811 | 0.1053 | 1.4686 -           0.7648    | 0.6941   - 3.4862\n",
      "\u001b[1;4mValidati 34     - 100    -          2.7623 | 0.0000 | 2.7623 -           0.6395    | 0.0000   - 0.1014\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 35     - 100    -          1.1284 | 0.1034 | 1.3264 -           0.8205    | 0.7134   - 3.4096\n",
      "\u001b[1;4mValidati 35     - 100    -          2.8775 | 0.0000 | 2.8775 -           0.6606    | 0.0000   - 0.0932\u001b[0m\n",
      "Training 36     - 100    -          1.1402 | 0.1088 | 1.3638 -           0.8136    | 0.7118   - 3.4115\n",
      "\u001b[1;4mValidati 36     - 100    -          2.9962 | 0.0000 | 2.9962 -           0.6503    | 0.0000   - 0.0920\u001b[0m\n",
      "Training 37     - 100    -          0.9981 | 0.1088 | 1.2378 -           0.8545    | 0.7177   - 3.3909\n",
      "\u001b[1;4mValidati 37     - 100    -          2.6635 | 0.0000 | 2.6635 -           0.6843    | 0.0000   - 0.0924\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 38     - 100    -          0.9356 | 0.1005 | 1.1727 -           0.8420    | 0.7190   - 3.3993\n",
      "\u001b[1;4mValidati 38     - 100    -          3.9861 | 0.0000 | 3.9861 -           0.5495    | 0.0000   - 0.0939\u001b[0m\n",
      "Training 39     - 100    -          1.0152 | 0.1051 | 1.2801 -           0.8330    | 0.7214   - 3.4176\n",
      "\u001b[1;4mValidati 39     - 100    -          2.7321 | 0.0000 | 2.7321 -           0.6881    | 0.0000   - 0.0916\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 40     - 100    -          0.9219 | 0.1016 | 1.1952 -           0.8739    | 0.7287   - 3.4583\n",
      "\u001b[1;4mValidati 40     - 100    -          2.5222 | 0.0000 | 2.5222 -           0.6896    | 0.0000   - 0.1061\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 41     - 100    -          0.8608 | 0.0959 | 1.1355 -           0.8557    | 0.7395   - 3.3992\n",
      "\u001b[1;4mValidati 41     - 100    -          2.6301 | 0.0000 | 2.6301 -           0.7052    | 0.0000   - 0.0927\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 42     - 100    -          0.8496 | 0.0936 | 1.1348 -           0.8670    | 0.7326   - 3.3796\n",
      "\u001b[1;4mValidati 42     - 100    -          2.9812 | 0.0000 | 2.9812 -           0.6422    | 0.0000   - 0.0933\u001b[0m\n",
      "Training 43     - 100    -          0.7816 | 0.0939 | 1.0854 -           0.8682    | 0.7449   - 3.3863\n",
      "\u001b[1;4mValidati 43     - 100    -          2.9488 | 0.0000 | 2.9488 -           0.6708    | 0.0000   - 0.0932\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 44     - 100    -          0.7720 | 0.0898 | 1.0802 -           0.8784    | 0.7497   - 3.4692\n",
      "\u001b[1;4mValidati 44     - 100    -          3.0852 | 0.0000 | 3.0852 -           0.7096    | 0.0000   - 0.1059\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 45     - 100    -          0.6758 | 0.0902 | 1.0035 -           0.9045    | 0.7509   - 3.4986\n",
      "\u001b[1;4mValidati 45     - 100    -          2.7858 | 0.0000 | 2.7858 -           0.6846    | 0.0000   - 0.1036\u001b[0m\n",
      "Training 46     - 100    -          0.6547 | 0.0853 | 0.9824 -           0.8977    | 0.7605   - 3.4568\n",
      "\u001b[1;4mValidati 46     - 100    -          2.6688 | 0.0000 | 2.6688 -           0.7172    | 0.0000   - 0.0932\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 47     - 100    -          0.6541 | 0.0810 | 0.9824 -           0.8920    | 0.7587   - 3.4950\n",
      "\u001b[1;4mValidati 47     - 100    -          2.8417 | 0.0000 | 2.8417 -           0.7198    | 0.0000   - 0.1059\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 48     - 100    -          0.5879 | 0.0787 | 0.9239 -           0.9057    | 0.7665   - 3.4327\n",
      "\u001b[1;4mValidati 48     - 100    -          2.9663 | 0.0000 | 2.9663 -           0.7034    | 0.0000   - 0.0956\u001b[0m\n",
      "Training 49     - 100    -          0.5785 | 0.0801 | 0.9386 -           0.9023    | 0.7635   - 3.3989\n",
      "\u001b[1;4mValidati 49     - 100    -          3.1953 | 0.0000 | 3.1953 -           0.6743    | 0.0000   - 0.0927\u001b[0m\n",
      "Training 50     - 100    -          0.5933 | 0.0745 | 0.9451 -           0.9023    | 0.7711   - 3.4875\n",
      "\u001b[1;4mValidati 50     - 100    -          2.7874 | 0.0000 | 2.7874 -           0.7181    | 0.0000   - 0.1041\u001b[0m\n",
      "Training 51     - 100    -          0.5632 | 0.0730 | 0.9247 -           0.9125    | 0.7822   - 3.3840\n",
      "\u001b[1;4mValidati 51     - 100    -          2.8750 | 0.0000 | 2.8750 -           0.7203    | 0.0000   - 0.0930\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 52     - 100    -          0.5410 | 0.0710 | 0.9091 -           0.9034    | 0.7826   - 3.3929\n",
      "\u001b[1;4mValidati 52     - 100    -          2.8332 | 0.0000 | 2.8332 -           0.7333    | 0.0000   - 0.0938\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 53     - 100    -          0.5208 | 0.0700 | 0.9001 -           0.9125    | 0.7876   - 3.3985\n",
      "\u001b[1;4mValidati 53     - 100    -          2.9510 | 0.0000 | 2.9510 -           0.7143    | 0.0000   - 0.0929\u001b[0m\n",
      "Training 54     - 100    -          0.5293 | 0.0723 | 0.9384 -           0.9114    | 0.7838   - 3.4605\n",
      "\u001b[1;4mValidati 54     - 100    -          2.7044 | 0.0000 | 2.7044 -           0.7234    | 0.0000   - 0.1060\u001b[0m\n",
      "Training 55     - 100    -          0.5161 | 0.0656 | 0.9029 -           0.9136    | 0.7921   - 3.4127\n",
      "\u001b[1;4mValidati 55     - 100    -          2.8252 | 0.0000 | 2.8252 -           0.7190    | 0.0000   - 0.0923\u001b[0m\n",
      "Training 56     - 100    -          0.4239 | 0.0616 | 0.8021 -           0.9455    | 0.7959   - 3.4087\n",
      "\u001b[1;4mValidati 56     - 100    -          3.1055 | 0.0000 | 3.1055 -           0.7157    | 0.0000   - 0.0940\u001b[0m\n",
      "Training 57     - 100    -          0.4273 | 0.0634 | 0.8316 -           0.9341    | 0.7936   - 3.3969\n",
      "\u001b[1;4mValidati 57     - 100    -          3.2286 | 0.0000 | 3.2286 -           0.7241    | 0.0000   - 0.0942\u001b[0m\n",
      "Training 58     - 100    -          0.4139 | 0.0621 | 0.8249 -           0.9432    | 0.8011   - 3.3832\n",
      "\u001b[1;4mValidati 58     - 100    -          3.1932 | 0.0000 | 3.1932 -           0.7181    | 0.0000   - 0.0924\u001b[0m\n",
      "Training 59     - 100    -          0.4154 | 0.0578 | 0.8113 -           0.9443    | 0.8023   - 3.3928\n",
      "\u001b[1;4mValidati 59     - 100    -          3.3938 | 0.0000 | 3.3938 -           0.7165    | 0.0000   - 0.0921\u001b[0m\n",
      "Training 60     - 100    -          0.3751 | 0.0580 | 0.7863 -           0.9409    | 0.8042   - 3.4672\n",
      "\u001b[1;4mValidati 60     - 100    -          2.9563 | 0.0000 | 2.9563 -           0.7252    | 0.0000   - 0.1044\u001b[0m\n",
      "Training 61     - 100    -          0.3470 | 0.0568 | 0.7627 -           0.9420    | 0.8142   - 3.4416\n",
      "\u001b[1;4mValidati 61     - 100    -          3.4195 | 0.0000 | 3.4195 -           0.6892    | 0.0000   - 0.0940\u001b[0m\n",
      "Training 62     - 100    -          0.2751 | 0.0511 | 0.6606 -           0.9477    | 0.8106   - 3.4009\n",
      "\u001b[1;4mValidati 62     - 100    -          3.4850 | 0.0000 | 3.4850 -           0.6987    | 0.0000   - 0.0965\u001b[0m\n",
      "Training 63     - 100    -          0.3389 | 0.0525 | 0.7464 -           0.9534    | 0.8110   - 3.3796\n",
      "\u001b[1;4mValidati 63     - 100    -          3.2903 | 0.0000 | 3.2903 -           0.7314    | 0.0000   - 0.0929\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 64     - 100    -          0.3298 | 0.0506 | 0.7331 -           0.9443    | 0.8131   - 3.4138\n",
      "\u001b[1;4mValidati 64     - 100    -          3.2547 | 0.0000 | 3.2547 -           0.7281    | 0.0000   - 0.0926\u001b[0m\n",
      "Training 65     - 100    -          0.3094 | 0.0475 | 0.6979 -           0.9580    | 0.8197   - 3.3903\n",
      "\u001b[1;4mValidati 65     - 100    -          3.2623 | 0.0000 | 3.2623 -           0.7566    | 0.0000   - 0.0929\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 66     - 100    -          0.2616 | 0.0478 | 0.6626 -           0.9602    | 0.8197   - 3.4334\n",
      "\u001b[1;4mValidati 66     - 100    -          3.6113 | 0.0000 | 3.6113 -           0.7098    | 0.0000   - 0.1065\u001b[0m\n",
      "Training 67     - 100    -          0.2913 | 0.0479 | 0.7021 -           0.9568    | 0.8206   - 3.4914\n",
      "\u001b[1;4mValidati 67     - 100    -          3.4697 | 0.0000 | 3.4697 -           0.7449    | 0.0000   - 0.0942\u001b[0m\n",
      "Training 68     - 100    -          0.2635 | 0.0448 | 0.6557 -           0.9534    | 0.8211   - 3.5034\n",
      "\u001b[1;4mValidati 68     - 100    -          3.3429 | 0.0000 | 3.3429 -           0.7359    | 0.0000   - 0.1059\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 69     - 100    -          0.3017 | 0.0429 | 0.6852 -           0.9500    | 0.8213   - 3.2336\n",
      "\u001b[1;4mValidati 69     - 100    -          3.2373 | 0.0000 | 3.2373 -           0.7542    | 0.0000   - 0.0948\u001b[0m\n",
      "Training 70     - 100    -          0.2498 | 0.0411 | 0.6234 -           0.9682    | 0.8242   - 3.2249\n",
      "\u001b[1;4mValidati 70     - 100    -          3.4209 | 0.0000 | 3.4209 -           0.7572    | 0.0000   - 0.0942\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 71     - 100    -          0.2733 | 0.0406 | 0.6485 -           0.9500    | 0.8231   - 3.1993\n",
      "\u001b[1;4mValidati 71     - 100    -          3.2713 | 0.0000 | 3.2713 -           0.7373    | 0.0000   - 0.0847\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 72     - 100    -          0.2551 | 0.0372 | 0.6041 -           0.9648    | 0.8301   - 3.0601\n",
      "\u001b[1;4mValidati 72     - 100    -          3.6341 | 0.0000 | 3.6341 -           0.7368    | 0.0000   - 0.0950\u001b[0m\n",
      "Training 73     - 100    -          0.2219 | 0.0377 | 0.5801 -           0.9625    | 0.8323   - 2.9942\n",
      "\u001b[1;4mValidati 73     - 100    -          3.4693 | 0.0000 | 3.4693 -           0.7436    | 0.0000   - 0.0873\u001b[0m\n",
      "Training 74     - 100    -          0.1948 | 0.0357 | 0.5386 -           0.9750    | 0.8310   - 2.9560\n",
      "\u001b[1;4mValidati 74     - 100    -          3.2669 | 0.0000 | 3.2669 -           0.7579    | 0.0000   - 0.0846\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      " better performance: saving ...\n",
      "\n",
      "Training 75     - 100    -          0.2165 | 0.0372 | 0.5780 -           0.9636    | 0.8305   - 3.0089\n",
      "\u001b[1;4mValidati 75     - 100    -          3.2336 | 0.0000 | 3.2336 -           0.7628    | 0.0000   - 0.0930\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 76     - 100    -          0.2115 | 0.0351 | 0.5556 -           0.9659    | 0.8314   - 2.9723\n",
      "\u001b[1;4mValidati 76     - 100    -          3.3701 | 0.0000 | 3.3701 -           0.7560    | 0.0000   - 0.0850\u001b[0m\n",
      "Training 77     - 100    -          0.2707 | 0.0331 | 0.5980 -           0.9648    | 0.8321   - 2.9496\n",
      "\u001b[1;4mValidati 77     - 100    -          3.3293 | 0.0000 | 3.3293 -           0.7556    | 0.0000   - 0.0860\u001b[0m\n",
      "Training 78     - 100    -          0.2036 | 0.0342 | 0.5428 -           0.9648    | 0.8338   - 2.9665\n",
      "\u001b[1;4mValidati 78     - 100    -          3.5276 | 0.0000 | 3.5276 -           0.7493    | 0.0000   - 0.0852\u001b[0m\n",
      "Training 79     - 100    -          0.2218 | 0.0324 | 0.5447 -           0.9716    | 0.8339   - 3.0337\n",
      "\u001b[1;4mValidati 79     - 100    -          3.3439 | 0.0000 | 3.3439 -           0.7674    | 0.0000   - 0.0873\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 80     - 100    -          0.2118 | 0.0322 | 0.5331 -           0.9705    | 0.8376   - 3.0247\n",
      "\u001b[1;4mValidati 80     - 100    -          3.3862 | 0.0000 | 3.3862 -           0.7586    | 0.0000   - 0.0937\u001b[0m\n",
      "Training 81     - 100    -          0.1867 | 0.0316 | 0.5023 -           0.9693    | 0.8378   - 2.9322\n",
      "\u001b[1;4mValidati 81     - 100    -          3.6418 | 0.0000 | 3.6418 -           0.7366    | 0.0000   - 0.0856\u001b[0m\n",
      "Training 82     - 100    -          0.2093 | 0.0309 | 0.5184 -           0.9739    | 0.8354   - 2.9565\n",
      "\u001b[1;4mValidati 82     - 100    -          3.7730 | 0.0000 | 3.7730 -           0.7325    | 0.0000   - 0.0852\u001b[0m\n",
      "Training 83     - 100    -          0.1628 | 0.0306 | 0.4691 -           0.9727    | 0.8366   - 3.1195\n",
      "\u001b[1;4mValidati 83     - 100    -          3.7485 | 0.0000 | 3.7485 -           0.7265    | 0.0000   - 0.0942\u001b[0m\n",
      "Training 84     - 100    -          0.1858 | 0.0285 | 0.4710 -           0.9682    | 0.8374   - 3.4032\n",
      "\u001b[1;4mValidati 84     - 100    -          3.6596 | 0.0000 | 3.6596 -           0.7474    | 0.0000   - 0.0937\u001b[0m\n",
      "Training 85     - 100    -          0.1640 | 0.0283 | 0.4473 -           0.9784    | 0.8366   - 3.3911\n",
      "\u001b[1;4mValidati 85     - 100    -          3.5483 | 0.0000 | 3.5483 -           0.7556    | 0.0000   - 0.0939\u001b[0m\n",
      "Training 86     - 100    -          0.1853 | 0.0278 | 0.4637 -           0.9682    | 0.8393   - 3.4549\n",
      "\u001b[1;4mValidati 86     - 100    -          3.6168 | 0.0000 | 3.6168 -           0.7359    | 0.0000   - 0.1044\u001b[0m\n",
      "Training 87     - 100    -          0.1793 | 0.0286 | 0.4653 -           0.9773    | 0.8341   - 3.4043\n",
      "\u001b[1;4mValidati 87     - 100    -          3.7617 | 0.0000 | 3.7617 -           0.7419    | 0.0000   - 0.0930\u001b[0m\n",
      "Training 88     - 100    -          0.1376 | 0.0262 | 0.3999 -           0.9807    | 0.8397   - 3.6025\n",
      "\u001b[1;4mValidati 88     - 100    -          3.6328 | 0.0000 | 3.6328 -           0.7490    | 0.0000   - 0.0939\u001b[0m\n",
      "Training 89     - 100    -          0.1894 | 0.0269 | 0.4588 -           0.9739    | 0.8411   - 3.4796\n",
      "\u001b[1;4mValidati 89     - 100    -          3.5899 | 0.0000 | 3.5899 -           0.7508    | 0.0000   - 0.1093\u001b[0m\n",
      "Training 90     - 100    -          0.1570 | 0.0269 | 0.4258 -           0.9807    | 0.8374   - 3.5003\n",
      "\u001b[1;4mValidati 90     - 100    -          3.7056 | 0.0000 | 3.7056 -           0.7436    | 0.0000   - 0.1063\u001b[0m\n",
      "Training 91     - 100    -          0.1551 | 0.0275 | 0.4297 -           0.9795    | 0.8392   - 3.4683\n",
      "\u001b[1;4mValidati 91     - 100    -          3.7269 | 0.0000 | 3.7269 -           0.7312    | 0.0000   - 0.0934\u001b[0m\n",
      "Training 92     - 100    -          0.1490 | 0.0279 | 0.4275 -           0.9784    | 0.8436   - 3.4604\n",
      "\u001b[1;4mValidati 92     - 100    -          3.7778 | 0.0000 | 3.7778 -           0.7422    | 0.0000   - 0.1047\u001b[0m\n",
      "Training 93     - 100    -          0.1534 | 0.0268 | 0.4210 -           0.9784    | 0.8406   - 3.5149\n",
      "\u001b[1;4mValidati 93     - 100    -          3.8100 | 0.0000 | 3.8100 -           0.7519    | 0.0000   - 0.0922\u001b[0m\n",
      "Training 94     - 100    -          0.1423 | 0.0267 | 0.4091 -           0.9727    | 0.8383   - 3.4220\n",
      "\u001b[1;4mValidati 94     - 100    -          3.9440 | 0.0000 | 3.9440 -           0.7447    | 0.0000   - 0.0935\u001b[0m\n",
      "Training 95     - 100    -          0.1541 | 0.0263 | 0.4171 -           0.9705    | 0.8402   - 3.4767\n",
      "\u001b[1;4mValidati 95     - 100    -          3.6282 | 0.0000 | 3.6282 -           0.7561    | 0.0000   - 0.1043\u001b[0m\n",
      "Training 96     - 100    -          0.1598 | 0.0257 | 0.4169 -           0.9807    | 0.8363   - 3.4400\n",
      "\u001b[1;4mValidati 96     - 100    -          3.7331 | 0.0000 | 3.7331 -           0.7489    | 0.0000   - 0.0939\u001b[0m\n",
      "Training 97     - 100    -          0.1610 | 0.0262 | 0.4230 -           0.9682    | 0.8396   - 3.4057\n",
      "\u001b[1;4mValidati 97     - 100    -          3.8132 | 0.0000 | 3.8132 -           0.7447    | 0.0000   - 0.0935\u001b[0m\n",
      "Training 98     - 100    -          0.1529 | 0.0262 | 0.4151 -           0.9818    | 0.8411   - 3.3925\n",
      "\u001b[1;4mValidati 98     - 100    -          3.6868 | 0.0000 | 3.6868 -           0.7452    | 0.0000   - 0.0938\u001b[0m\n",
      "Training 99     - 100    -          0.1465 | 0.0262 | 0.4086 -           0.9795    | 0.8404   - 3.4313\n",
      "\u001b[1;4mValidati 99     - 100    -          3.7854 | 0.0000 | 3.7854 -           0.7460    | 0.0000   - 0.0933\u001b[0m\n",
      "Training 100    - 100    -          0.1523 | 0.0266 | 0.4180 -           0.9761    | 0.8388   - 3.5028\n",
      "\u001b[1;4mValidati 100    - 100    -          3.6799 | 0.0000 | 3.6799 -           0.7451    | 0.0000   - 0.1044\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
