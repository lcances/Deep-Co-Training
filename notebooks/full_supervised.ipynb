{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcances/sync/Documents_sync/Projet/PyTorch/audio/torchaudio/extension/extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "from DCT.util.utils import reset_seed, get_datetime\n",
    "from DCT.util.model_loader import get_model_from_name\n",
    "from DCT.util.dataset_loader import load_dataset\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"SpeechCommand\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"resnet34\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=1.0, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=256, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=50, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=256, checkpoint_path='supervised', checkpoint_root='../model_save/', dataset='SpeechCommand', dataset_root='../datasets', from_config='', learning_rate=0.001, model='resnet34', nb_epoch=200, num_classes=50, resume=False, seed=1234, supervised_ratio=1.0, tensorboard_path='supervised', tensorboard_root='../tensorboard/', tensorboard_sufix='', train_folds=[1, 2, 3, 4], val_folds=[5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadUpTo(nn.Module):\n",
    "    def __init__(self, target_length, mode: str = \"constant\", value: int = 0):\n",
    "        super().__init__()\n",
    "        self.target_length = target_length\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        actual_length = x.size()[-1]\n",
    "        return F.pad(input=x, pad=(0, (self.target_length - actual_length)), mode=self.mode, value=self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "transforms = nn.Sequential(\n",
    "    PadUpTo(target_length=16000, mode=\"constant\", value=0),\n",
    "    MelSpectrogram(sample_rate=16000, n_fft=2048, hop_length=512, n_mels=64), \n",
    "    AmplitudeToDB(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/SpeechCommands/speech_commands_v0.02\n",
      "Dataset already download and verified\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"supervised\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    transform=transforms,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 64, 32])\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(len(np.unique(y.numpy())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = get_model_from_name(args.model)\n",
    "# model_func = get_model_from_name(\"esc_wideresnet28_8\")\n",
    "model = model_func(input_shape=(64, 431), num_classes = args.num_classes)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "                                          Kernel Shape      Output Shape  \\\n",
      "Layer                                                                      \n",
      "0_conv1                                  [3, 64, 7, 7]  [1, 64, 32, 216]   \n",
      "1_bn1                                             [64]  [1, 64, 32, 216]   \n",
      "2_relu                                               -  [1, 64, 32, 216]   \n",
      "3_maxpool                                            -  [1, 64, 16, 108]   \n",
      "4_layer1.0.Conv2d_conv1                 [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "5_layer1.0.BatchNorm2d_bn1                        [64]  [1, 64, 16, 108]   \n",
      "6_layer1.0.ReLU_relu                                 -  [1, 64, 16, 108]   \n",
      "7_layer1.0.Conv2d_conv2                 [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "8_layer1.0.BatchNorm2d_bn2                        [64]  [1, 64, 16, 108]   \n",
      "9_layer1.0.ReLU_relu                                 -  [1, 64, 16, 108]   \n",
      "10_layer1.1.Conv2d_conv1                [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "11_layer1.1.BatchNorm2d_bn1                       [64]  [1, 64, 16, 108]   \n",
      "12_layer1.1.ReLU_relu                                -  [1, 64, 16, 108]   \n",
      "13_layer1.1.Conv2d_conv2                [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "14_layer1.1.BatchNorm2d_bn2                       [64]  [1, 64, 16, 108]   \n",
      "15_layer1.1.ReLU_relu                                -  [1, 64, 16, 108]   \n",
      "16_layer1.2.Conv2d_conv1                [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "17_layer1.2.BatchNorm2d_bn1                       [64]  [1, 64, 16, 108]   \n",
      "18_layer1.2.ReLU_relu                                -  [1, 64, 16, 108]   \n",
      "19_layer1.2.Conv2d_conv2                [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "20_layer1.2.BatchNorm2d_bn2                       [64]  [1, 64, 16, 108]   \n",
      "21_layer1.2.ReLU_relu                                -  [1, 64, 16, 108]   \n",
      "22_layer2.0.Conv2d_conv1               [64, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "23_layer2.0.BatchNorm2d_bn1                      [128]   [1, 128, 8, 54]   \n",
      "24_layer2.0.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "25_layer2.0.Conv2d_conv2              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "26_layer2.0.BatchNorm2d_bn2                      [128]   [1, 128, 8, 54]   \n",
      "27_layer2.0.downsample.Conv2d_0        [64, 128, 1, 1]   [1, 128, 8, 54]   \n",
      "28_layer2.0.downsample.BatchNorm2d_1             [128]   [1, 128, 8, 54]   \n",
      "29_layer2.0.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "30_layer2.1.Conv2d_conv1              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "31_layer2.1.BatchNorm2d_bn1                      [128]   [1, 128, 8, 54]   \n",
      "32_layer2.1.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "33_layer2.1.Conv2d_conv2              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "34_layer2.1.BatchNorm2d_bn2                      [128]   [1, 128, 8, 54]   \n",
      "35_layer2.1.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "36_layer2.2.Conv2d_conv1              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "37_layer2.2.BatchNorm2d_bn1                      [128]   [1, 128, 8, 54]   \n",
      "38_layer2.2.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "39_layer2.2.Conv2d_conv2              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "40_layer2.2.BatchNorm2d_bn2                      [128]   [1, 128, 8, 54]   \n",
      "41_layer2.2.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "42_layer2.3.Conv2d_conv1              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "43_layer2.3.BatchNorm2d_bn1                      [128]   [1, 128, 8, 54]   \n",
      "44_layer2.3.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "45_layer2.3.Conv2d_conv2              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "46_layer2.3.BatchNorm2d_bn2                      [128]   [1, 128, 8, 54]   \n",
      "47_layer2.3.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "48_layer3.0.Conv2d_conv1              [128, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "49_layer3.0.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "50_layer3.0.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "51_layer3.0.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "52_layer3.0.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "53_layer3.0.downsample.Conv2d_0       [128, 256, 1, 1]   [1, 256, 4, 27]   \n",
      "54_layer3.0.downsample.BatchNorm2d_1             [256]   [1, 256, 4, 27]   \n",
      "55_layer3.0.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "56_layer3.1.Conv2d_conv1              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "57_layer3.1.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "58_layer3.1.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "59_layer3.1.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "60_layer3.1.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "61_layer3.1.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "62_layer3.2.Conv2d_conv1              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "63_layer3.2.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "64_layer3.2.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "65_layer3.2.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "66_layer3.2.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "67_layer3.2.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "68_layer3.3.Conv2d_conv1              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "69_layer3.3.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "70_layer3.3.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "71_layer3.3.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "72_layer3.3.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "73_layer3.3.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "74_layer3.4.Conv2d_conv1              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "75_layer3.4.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "76_layer3.4.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "77_layer3.4.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "78_layer3.4.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "79_layer3.4.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "80_layer3.5.Conv2d_conv1              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "81_layer3.5.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "82_layer3.5.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "83_layer3.5.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "84_layer3.5.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "85_layer3.5.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "86_layer4.0.Conv2d_conv1              [256, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "87_layer4.0.BatchNorm2d_bn1                      [512]   [1, 512, 2, 14]   \n",
      "88_layer4.0.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "89_layer4.0.Conv2d_conv2              [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "90_layer4.0.BatchNorm2d_bn2                      [512]   [1, 512, 2, 14]   \n",
      "91_layer4.0.downsample.Conv2d_0       [256, 512, 1, 1]   [1, 512, 2, 14]   \n",
      "92_layer4.0.downsample.BatchNorm2d_1             [512]   [1, 512, 2, 14]   \n",
      "93_layer4.0.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "94_layer4.1.Conv2d_conv1              [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "95_layer4.1.BatchNorm2d_bn1                      [512]   [1, 512, 2, 14]   \n",
      "96_layer4.1.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "97_layer4.1.Conv2d_conv2              [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "98_layer4.1.BatchNorm2d_bn2                      [512]   [1, 512, 2, 14]   \n",
      "99_layer4.1.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "100_layer4.2.Conv2d_conv1             [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "101_layer4.2.BatchNorm2d_bn1                     [512]   [1, 512, 2, 14]   \n",
      "102_layer4.2.ReLU_relu                               -   [1, 512, 2, 14]   \n",
      "103_layer4.2.Conv2d_conv2             [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "104_layer4.2.BatchNorm2d_bn2                     [512]   [1, 512, 2, 14]   \n",
      "105_layer4.2.ReLU_relu                               -   [1, 512, 2, 14]   \n",
      "106_avgpool                                          -    [1, 512, 1, 1]   \n",
      "107_fc                                       [512, 50]           [1, 50]   \n",
      "\n",
      "                                         Params   Mult-Adds  \n",
      "Layer                                                        \n",
      "0_conv1                                  9.408k  65.028096M  \n",
      "1_bn1                                     128.0        64.0  \n",
      "2_relu                                        -           -  \n",
      "3_maxpool                                     -           -  \n",
      "4_layer1.0.Conv2d_conv1                 36.864k  63.700992M  \n",
      "5_layer1.0.BatchNorm2d_bn1                128.0        64.0  \n",
      "6_layer1.0.ReLU_relu                          -           -  \n",
      "7_layer1.0.Conv2d_conv2                 36.864k  63.700992M  \n",
      "8_layer1.0.BatchNorm2d_bn2                128.0        64.0  \n",
      "9_layer1.0.ReLU_relu                          -           -  \n",
      "10_layer1.1.Conv2d_conv1                36.864k  63.700992M  \n",
      "11_layer1.1.BatchNorm2d_bn1               128.0        64.0  \n",
      "12_layer1.1.ReLU_relu                         -           -  \n",
      "13_layer1.1.Conv2d_conv2                36.864k  63.700992M  \n",
      "14_layer1.1.BatchNorm2d_bn2               128.0        64.0  \n",
      "15_layer1.1.ReLU_relu                         -           -  \n",
      "16_layer1.2.Conv2d_conv1                36.864k  63.700992M  \n",
      "17_layer1.2.BatchNorm2d_bn1               128.0        64.0  \n",
      "18_layer1.2.ReLU_relu                         -           -  \n",
      "19_layer1.2.Conv2d_conv2                36.864k  63.700992M  \n",
      "20_layer1.2.BatchNorm2d_bn2               128.0        64.0  \n",
      "21_layer1.2.ReLU_relu                         -           -  \n",
      "22_layer2.0.Conv2d_conv1                73.728k  31.850496M  \n",
      "23_layer2.0.BatchNorm2d_bn1               256.0       128.0  \n",
      "24_layer2.0.ReLU_relu                         -           -  \n",
      "25_layer2.0.Conv2d_conv2               147.456k  63.700992M  \n",
      "26_layer2.0.BatchNorm2d_bn2               256.0       128.0  \n",
      "27_layer2.0.downsample.Conv2d_0          8.192k   3.538944M  \n",
      "28_layer2.0.downsample.BatchNorm2d_1      256.0       128.0  \n",
      "29_layer2.0.ReLU_relu                         -           -  \n",
      "30_layer2.1.Conv2d_conv1               147.456k  63.700992M  \n",
      "31_layer2.1.BatchNorm2d_bn1               256.0       128.0  \n",
      "32_layer2.1.ReLU_relu                         -           -  \n",
      "33_layer2.1.Conv2d_conv2               147.456k  63.700992M  \n",
      "34_layer2.1.BatchNorm2d_bn2               256.0       128.0  \n",
      "35_layer2.1.ReLU_relu                         -           -  \n",
      "36_layer2.2.Conv2d_conv1               147.456k  63.700992M  \n",
      "37_layer2.2.BatchNorm2d_bn1               256.0       128.0  \n",
      "38_layer2.2.ReLU_relu                         -           -  \n",
      "39_layer2.2.Conv2d_conv2               147.456k  63.700992M  \n",
      "40_layer2.2.BatchNorm2d_bn2               256.0       128.0  \n",
      "41_layer2.2.ReLU_relu                         -           -  \n",
      "42_layer2.3.Conv2d_conv1               147.456k  63.700992M  \n",
      "43_layer2.3.BatchNorm2d_bn1               256.0       128.0  \n",
      "44_layer2.3.ReLU_relu                         -           -  \n",
      "45_layer2.3.Conv2d_conv2               147.456k  63.700992M  \n",
      "46_layer2.3.BatchNorm2d_bn2               256.0       128.0  \n",
      "47_layer2.3.ReLU_relu                         -           -  \n",
      "48_layer3.0.Conv2d_conv1               294.912k  31.850496M  \n",
      "49_layer3.0.BatchNorm2d_bn1               512.0       256.0  \n",
      "50_layer3.0.ReLU_relu                         -           -  \n",
      "51_layer3.0.Conv2d_conv2               589.824k  63.700992M  \n",
      "52_layer3.0.BatchNorm2d_bn2               512.0       256.0  \n",
      "53_layer3.0.downsample.Conv2d_0         32.768k   3.538944M  \n",
      "54_layer3.0.downsample.BatchNorm2d_1      512.0       256.0  \n",
      "55_layer3.0.ReLU_relu                         -           -  \n",
      "56_layer3.1.Conv2d_conv1               589.824k  63.700992M  \n",
      "57_layer3.1.BatchNorm2d_bn1               512.0       256.0  \n",
      "58_layer3.1.ReLU_relu                         -           -  \n",
      "59_layer3.1.Conv2d_conv2               589.824k  63.700992M  \n",
      "60_layer3.1.BatchNorm2d_bn2               512.0       256.0  \n",
      "61_layer3.1.ReLU_relu                         -           -  \n",
      "62_layer3.2.Conv2d_conv1               589.824k  63.700992M  \n",
      "63_layer3.2.BatchNorm2d_bn1               512.0       256.0  \n",
      "64_layer3.2.ReLU_relu                         -           -  \n",
      "65_layer3.2.Conv2d_conv2               589.824k  63.700992M  \n",
      "66_layer3.2.BatchNorm2d_bn2               512.0       256.0  \n",
      "67_layer3.2.ReLU_relu                         -           -  \n",
      "68_layer3.3.Conv2d_conv1               589.824k  63.700992M  \n",
      "69_layer3.3.BatchNorm2d_bn1               512.0       256.0  \n",
      "70_layer3.3.ReLU_relu                         -           -  \n",
      "71_layer3.3.Conv2d_conv2               589.824k  63.700992M  \n",
      "72_layer3.3.BatchNorm2d_bn2               512.0       256.0  \n",
      "73_layer3.3.ReLU_relu                         -           -  \n",
      "74_layer3.4.Conv2d_conv1               589.824k  63.700992M  \n",
      "75_layer3.4.BatchNorm2d_bn1               512.0       256.0  \n",
      "76_layer3.4.ReLU_relu                         -           -  \n",
      "77_layer3.4.Conv2d_conv2               589.824k  63.700992M  \n",
      "78_layer3.4.BatchNorm2d_bn2               512.0       256.0  \n",
      "79_layer3.4.ReLU_relu                         -           -  \n",
      "80_layer3.5.Conv2d_conv1               589.824k  63.700992M  \n",
      "81_layer3.5.BatchNorm2d_bn1               512.0       256.0  \n",
      "82_layer3.5.ReLU_relu                         -           -  \n",
      "83_layer3.5.Conv2d_conv2               589.824k  63.700992M  \n",
      "84_layer3.5.BatchNorm2d_bn2               512.0       256.0  \n",
      "85_layer3.5.ReLU_relu                         -           -  \n",
      "86_layer4.0.Conv2d_conv1              1.179648M  33.030144M  \n",
      "87_layer4.0.BatchNorm2d_bn1              1.024k       512.0  \n",
      "88_layer4.0.ReLU_relu                         -           -  \n",
      "89_layer4.0.Conv2d_conv2              2.359296M  66.060288M  \n",
      "90_layer4.0.BatchNorm2d_bn2              1.024k       512.0  \n",
      "91_layer4.0.downsample.Conv2d_0        131.072k   3.670016M  \n",
      "92_layer4.0.downsample.BatchNorm2d_1     1.024k       512.0  \n",
      "93_layer4.0.ReLU_relu                         -           -  \n",
      "94_layer4.1.Conv2d_conv1              2.359296M  66.060288M  \n",
      "95_layer4.1.BatchNorm2d_bn1              1.024k       512.0  \n",
      "96_layer4.1.ReLU_relu                         -           -  \n",
      "97_layer4.1.Conv2d_conv2              2.359296M  66.060288M  \n",
      "98_layer4.1.BatchNorm2d_bn2              1.024k       512.0  \n",
      "99_layer4.1.ReLU_relu                         -           -  \n",
      "100_layer4.2.Conv2d_conv1             2.359296M  66.060288M  \n",
      "101_layer4.2.BatchNorm2d_bn1             1.024k       512.0  \n",
      "102_layer4.2.ReLU_relu                        -           -  \n",
      "103_layer4.2.Conv2d_conv2             2.359296M  66.060288M  \n",
      "104_layer4.2.BatchNorm2d_bn2             1.024k       512.0  \n",
      "105_layer4.2.ReLU_relu                        -           -  \n",
      "106_avgpool                                   -           -  \n",
      "107_fc                                   25.65k       25.6k  \n",
      "-----------------------------------------------------------------------------------------------\n",
      "                            Totals\n",
      "Total params            21.310322M\n",
      "Trainable params        21.310322M\n",
      "Non-trainable params           0.0\n",
      "Mult-Adds             2.031666496G\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "nb_conv = 0\n",
    "\n",
    "for layer in s.index.values:\n",
    "    if \"Conv\" in layer:\n",
    "        nb_conv += 1\n",
    "print(nb_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/SpeechCommand/supervised/resnet34/1.0S/2020-09-22_13:21:49_resnet34_1.0S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "tensorboard_title = \"%s/%sS/%s_%s_%.1fS\" % title_element\n",
    "\n",
    "title_element = (model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % title_element\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))\n",
    "\n",
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_params = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    tensorboard_params[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard.add_hparams(tensorboard_params, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar10 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    def lr_lambda(e):\n",
    "        if e < 60:\n",
    "            return 1\n",
    "\n",
    "        elif 60 <= e < 120:\n",
    "            return 0.2\n",
    "\n",
    "        elif 120 <= e < 160:\n",
    "            return 0.04\n",
    "\n",
    "        else:\n",
    "            return 0.008\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ubs8k optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"ubs8k\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n",
    "\n",
    "elif args.dataset in (\"esc10\", 'esc50'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n",
    "    \n",
    "elif args.dataset in (\"SpeechCommand\",):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# Metrics\n",
    "fscore_fn = FScore()\n",
    "acc_fn = CategoricalAccuracy()\n",
    "avg = ContinueAverage()\n",
    "\n",
    "reset_metrics = lambda : [m.reset() for m in [fscore_fn, acc_fn, avg]]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0365184e20>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, args.nb_epoch, args.nb_epoch)\n",
    "y = [lr_lambda(x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \".               \", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"metrics: \", \"acc\", \"F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        logits = model(X)        \n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_ce(logits, y)\n",
    "\n",
    "            # metrics\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc\", acc, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc\", maximum_fn(\"acc\", acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/f1\", maximum_fn(\"f1\", fscore), epoch )\n",
    "\n",
    "    checkpoint.step(acc)\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n",
      "\n",
      "Training 1      - 100    -          1.0536 -           0.6853    | 0.6694   - 118.4493\n",
      "\u001b[1;4mValidati 1      - 100    -          0.7903 -           0.7652    | 0.7779   - 24.3578\u001b[0m\n",
      "Training 2      - 100    -          0.4128 -           0.8729    | 0.8821   - 28.6948\n",
      "\u001b[1;4mValidati 2      - 100    -          0.5692 -           0.8359    | 0.8483   - 1.9061\u001b[0m\n",
      "Training 3      - 100    -          0.3127 -           0.9036    | 0.9111   - 28.2625\n",
      "\u001b[1;4mValidati 3      - 100    -          0.4139 -           0.8763    | 0.8870   - 1.9128\u001b[0m\n",
      "Training 4      - 100    -          0.2553 -           0.9196    | 0.9269   - 28.2308\n",
      "\u001b[1;4mValidati 4      - 100    -          0.4165 -           0.8832    | 0.8932   - 1.9030\u001b[0m\n",
      "Training 5      - 100    -          0.2181 -           0.9326    | 0.9386   - 28.2665\n",
      "\u001b[1;4mValidati 5      - 100    -          0.3762 -           0.8926    | 0.9039   - 1.8943\u001b[0m\n",
      "Training 6      - 100    -          0.1870 -           0.9409    | 0.9467   - 29.1985\n",
      "\u001b[1;4mValidati 6      - 100    -          0.3939 -           0.8929    | 0.9013   - 1.9075\u001b[0m\n",
      "Training 7      - 100    -          0.1661 -           0.9480    | 0.9528   - 28.9619\n",
      "\u001b[1;4mValidati 7      - 100    -          0.3600 -           0.8975    | 0.9069   - 1.9645\u001b[0m\n",
      "Training 8      - 100    -          0.1464 -           0.9535    | 0.9580   - 29.1720\n",
      "\u001b[1;4mValidati 8      - 100    -          0.3466 -           0.9039    | 0.9137   - 1.9320\u001b[0m\n",
      "Training 9      - 100    -          0.1293 -           0.9585    | 0.9625   - 29.1501\n",
      "\u001b[1;4mValidati 9      - 100    -          0.3476 -           0.9045    | 0.9142   - 1.9500\u001b[0m\n",
      "Training 10     - 100    -          0.1180 -           0.9625    | 0.9664   - 28.9228\n",
      "\u001b[1;4mValidati 10     - 100    -          0.3816 -           0.9010    | 0.9086   - 1.9163\u001b[0m\n",
      "Training 11     - 100    -          0.1083 -           0.9655    | 0.9685   - 29.3326\n",
      "\u001b[1;4mValidati 11     - 100    -          0.3792 -           0.9009    | 0.9099   - 1.9064\u001b[0m\n",
      "Training 12     - 100    -          0.0969 -           0.9693    | 0.9722   - 28.9447\n",
      "\u001b[1;4mValidati 12     - 100    -          0.4235 -           0.8938    | 0.9021   - 1.8988\u001b[0m\n",
      "Training 13     - 100    -          0.0924 -           0.9708    | 0.9734   - 28.7212\n",
      "\u001b[1;4mValidati 13     - 100    -          0.4095 -           0.8997    | 0.9074   - 1.8890\u001b[0m\n",
      "Training 14     - 100    -          0.0779 -           0.9755    | 0.9781   - 28.2420\n",
      "\u001b[1;4mValidati 14     - 100    -          0.4289 -           0.8978    | 0.9066   - 1.8362\u001b[0m\n",
      "Training 15     - 100    -          0.0719 -           0.9773    | 0.9793   - 28.2652\n",
      "\u001b[1;4mValidati 15     - 100    -          0.3865 -           0.9073    | 0.9146   - 1.9164\u001b[0m\n",
      "Training 16     - 100    -          0.0701 -           0.9779    | 0.9795   - 29.1014\n",
      "\u001b[1;4mValidati 16     - 100    -          0.3919 -           0.9128    | 0.9193   - 1.9018\u001b[0m\n",
      "Training 17     - 100    -          0.0591 -           0.9816    | 0.9830   - 28.8716\n",
      "\u001b[1;4mValidati 17     - 100    -          0.4225 -           0.9043    | 0.9113   - 1.8969\u001b[0m\n",
      "Training 18     - 100    -          0.0522 -           0.9835    | 0.9848   - 28.2553\n",
      "\u001b[1;4mValidati 18     - 100    -          0.3871 -           0.9122    | 0.9189   - 1.9124\u001b[0m\n",
      "Training 19     - 100    -          0.0510 -           0.9841    | 0.9848   - 28.6096\n",
      "\u001b[1;4mValidati 19     - 100    -          0.4462 -           0.9033    | 0.9107   - 1.9104\u001b[0m\n",
      "Training 20     - 100    -          0.0433 -           0.9863    | 0.9871   - 28.7776\n",
      "\u001b[1;4mValidati 20     - 100    -          0.4601 -           0.9027    | 0.9088   - 1.8576\u001b[0m\n",
      "Training 21     - 100    -          0.0469 -           0.9849    | 0.9857   - 28.4836\n",
      "\u001b[1;4mValidati 21     - 100    -          0.4385 -           0.9049    | 0.9116   - 1.9236\u001b[0m\n",
      "Training 22     - 100    -          0.0364 -           0.9883    | 0.9888   - 29.0305\n",
      "\u001b[1;4mValidati 22     - 100    -          0.4411 -           0.9136    | 0.9196   - 1.8951\u001b[0m\n",
      "Training 23     - 100    -          0.0303 -           0.9905    | 0.9911   - 29.6247\n",
      "\u001b[1;4mValidati 23     - 100    -          0.4713 -           0.9099    | 0.9157   - 1.9020\u001b[0m\n",
      "Training 24     - 100    -          0.0367 -           0.9879    | 0.9883   - 28.4135\n",
      "\u001b[1;4mValidati 24     - 100    -          0.4579 -           0.9056    | 0.9126   - 1.8982\u001b[0m\n",
      "Training 25     - 100    -          0.0349 -           0.9893    | 0.9895   - 28.9802\n",
      "\u001b[1;4mValidati 25     - 100    -          0.4155 -           0.9144    | 0.9197   - 1.9219\u001b[0m\n",
      "Training 26     - 100    -          0.0258 -           0.9919    | 0.9922   - 28.6220\n",
      "\u001b[1;4mValidati 26     - 100    -          0.4577 -           0.9139    | 0.9185   - 1.9280\u001b[0m\n",
      "Training 27     - 100    -          0.0241 -           0.9922    | 0.9924   - 28.4687\n",
      "\u001b[1;4mValidati 27     - 100    -          0.4674 -           0.9135    | 0.9188   - 1.8933\u001b[0m\n",
      "Training 28     - 100    -          0.0221 -           0.9933    | 0.9935   - 29.0589\n",
      "\u001b[1;4mValidati 28     - 100    -          0.5061 -           0.9084    | 0.9128   - 1.9112\u001b[0m\n",
      "Training 29     - 100    -          0.0275 -           0.9914    | 0.9913   - 29.0525\n",
      "\u001b[1;4mValidati 29     - 100    -          0.4835 -           0.9120    | 0.9170   - 1.9048\u001b[0m\n",
      "Training 30     - 100    -          0.0163 -           0.9946    | 0.9947   - 28.8013\n",
      "\u001b[1;4mValidati 30     - 100    -          0.4761 -           0.9133    | 0.9173   - 1.9081\u001b[0m\n",
      "Training 31     - 100    -          0.0179 -           0.9942    | 0.9942   - 28.8541\n",
      "\u001b[1;4mValidati 31     - 100    -          0.5200 -           0.9105    | 0.9141   - 1.9000\u001b[0m\n",
      "Training 32     - 100    -          0.0194 -           0.9939    | 0.9939   - 28.1319\n",
      "\u001b[1;4mValidati 32     - 100    -          0.5382 -           0.9061    | 0.9099   - 1.9001\u001b[0m\n",
      "Training 33     - 100    -          0.0196 -           0.9938    | 0.9938   - 29.5336\n",
      "\u001b[1;4mValidati 33     - 100    -          0.4982 -           0.9118    | 0.9167   - 1.9273\u001b[0m\n",
      "Training 34     - 100    -          0.0148 -           0.9948    | 0.9949   - 28.3372\n",
      "\u001b[1;4mValidati 34     - 100    -          0.5148 -           0.9135    | 0.9179   - 1.8801\u001b[0m\n",
      "Training 35     - 100    -          0.0168 -           0.9946    | 0.9946   - 28.5185\n",
      "\u001b[1;4mValidati 35     - 100    -          0.5580 -           0.9036    | 0.9075   - 1.8966\u001b[0m\n",
      "Training 36     - 100    -          0.0172 -           0.9945    | 0.9945   - 29.0686\n",
      "\u001b[1;4mValidati 36     - 100    -          0.4821 -           0.9151    | 0.9188   - 1.9188\u001b[0m\n",
      "Training 37     - 100    -          0.0120 -           0.9959    | 0.9959   - 29.0055\n",
      "\u001b[1;4mValidati 37     - 100    -          0.5202 -           0.9149    | 0.9183   - 1.9079\u001b[0m\n",
      "Training 38     - 100    -          0.0162 -           0.9952    | 0.9952   - 28.7991\n",
      "\u001b[1;4mValidati 38     - 100    -          0.5758 -           0.9074    | 0.9105   - 1.8517\u001b[0m\n",
      "Training 39     - 100    -          0.0156 -           0.9949    | 0.9949   - 28.5528\n",
      "\u001b[1;4mValidati 39     - 100    -          0.5462 -           0.9130    | 0.9159   - 1.9057\u001b[0m\n",
      "Training 40     - 100    -          0.0128 -           0.9956    | 0.9955   - 28.6787\n",
      "\u001b[1;4mValidati 40     - 100    -          0.5644 -           0.9114    | 0.9138   - 1.8434\u001b[0m\n",
      "Training 41     - 100    -          0.0157 -           0.9952    | 0.9952   - 28.2759\n",
      "\u001b[1;4mValidati 41     - 100    -          0.5360 -           0.9124    | 0.9159   - 1.8885\u001b[0m\n",
      "Training 42     - 100    -          0.0080 -           0.9974    | 0.9975   - 28.4182\n",
      "\u001b[1;4mValidati 42     - 100    -          0.5644 -           0.9113    | 0.9139   - 1.9096\u001b[0m\n",
      "Training 43     - 100    -          0.0111 -           0.9966    | 0.9966   - 28.9960\n",
      "\u001b[1;4mValidati 43     - 100    -          0.5715 -           0.9097    | 0.9133   - 1.9023\u001b[0m\n",
      "Training 44     - 100    -          0.0109 -           0.9965    | 0.9966   - 29.4767\n",
      "\u001b[1;4mValidati 44     - 100    -          0.5576 -           0.9126    | 0.9154   - 1.9593\u001b[0m\n",
      "Training 45     - 100    -          0.0103 -           0.9969    | 0.9969   - 29.2804\n",
      "\u001b[1;4mValidati 45     - 100    -          0.5724 -           0.9122    | 0.9152   - 1.9635\u001b[0m\n",
      "Training 46     - 100    -          0.0107 -           0.9964    | 0.9965   - 29.2432\n",
      "\u001b[1;4mValidati 46     - 100    -          0.5654 -           0.9119    | 0.9148   - 1.8980\u001b[0m\n",
      "Training 47     - 100    -          0.0091 -           0.9971    | 0.9971   - 29.7002\n",
      "\u001b[1;4mValidati 47     - 100    -          0.5192 -           0.9191    | 0.9225   - 1.9318\u001b[0m\n",
      "Training 48     - 100    -          0.0104 -           0.9964    | 0.9965   - 29.0674\n",
      "\u001b[1;4mValidati 48     - 100    -          0.5516 -           0.9158    | 0.9187   - 1.9075\u001b[0m\n",
      "Training 49     - 100    -          0.0112 -           0.9964    | 0.9964   - 28.6115\n",
      "\u001b[1;4mValidati 49     - 100    -          0.5350 -           0.9169    | 0.9203   - 1.9169\u001b[0m\n",
      "Training 50     - 100    -          0.0072 -           0.9978    | 0.9978   - 28.8588\n",
      "\u001b[1;4mValidati 50     - 100    -          0.6098 -           0.9079    | 0.9101   - 1.8887\u001b[0m\n",
      "Training 51     - 100    -          0.0110 -           0.9967    | 0.9967   - 30.2796\n",
      "\u001b[1;4mValidati 51     - 100    -          0.5633 -           0.9141    | 0.9170   - 2.0011\u001b[0m\n",
      "Training 52     - 100    -          0.0077 -           0.9977    | 0.9978   - 29.1606\n",
      "\u001b[1;4mValidati 52     - 100    -          0.6047 -           0.9088    | 0.9108   - 1.9708\u001b[0m\n",
      "Training 53     - 100    -          0.0152 -           0.9956    | 0.9956   - 29.0755\n",
      "\u001b[1;4mValidati 53     - 100    -          0.5256 -           0.9144    | 0.9187   - 1.9910\u001b[0m\n",
      "Training 54     - 100    -          0.0064 -           0.9982    | 0.9982   - 29.0809\n",
      "\u001b[1;4mValidati 54     - 100    -          0.5555 -           0.9123    | 0.9154   - 1.9162\u001b[0m\n",
      "Training 55     - 100    -          0.0068 -           0.9980    | 0.9980   - 28.2611\n",
      "\u001b[1;4mValidati 55     - 100    -          0.5636 -           0.9161    | 0.9193   - 1.9081\u001b[0m\n",
      "Training 56     - 100    -          0.0099 -           0.9968    | 0.9968   - 28.6486\n",
      "\u001b[1;4mValidati 56     - 100    -          0.5767 -           0.9119    | 0.9154   - 1.9095\u001b[0m\n",
      "Training 57     - 100    -          0.0082 -           0.9977    | 0.9977   - 28.3720\n",
      "\u001b[1;4mValidati 57     - 100    -          0.5507 -           0.9184    | 0.9212   - 1.8982\u001b[0m\n",
      "Training 58     - 100    -          0.0050 -           0.9986    | 0.9986   - 28.3837\n",
      "\u001b[1;4mValidati 58     - 100    -          0.5984 -           0.9117    | 0.9143   - 1.8933\u001b[0m\n",
      "Training 59     - 100    -          0.0084 -           0.9973    | 0.9973   - 28.2007\n",
      "\u001b[1;4mValidati 59     - 100    -          0.5597 -           0.9186    | 0.9209   - 1.8552\u001b[0m\n",
      "Training 60     - 100    -          0.0054 -           0.9982    | 0.9983   - 29.4901\n",
      "\u001b[1;4mValidati 60     - 100    -          0.6137 -           0.9122    | 0.9148   - 1.9637\u001b[0m\n",
      "Training 61     - 100    -          0.0063 -           0.9980    | 0.9980   - 29.1488\n",
      "\u001b[1;4mValidati 61     - 100    -          0.5644 -           0.9165    | 0.9190   - 1.9058\u001b[0m\n",
      "Training 62     - 100    -          0.0092 -           0.9970    | 0.9969   - 28.3127\n",
      "\u001b[1;4mValidati 62     - 100    -          0.5811 -           0.9141    | 0.9164   - 1.8953\u001b[0m\n",
      "Training 63     - 100    -          0.0057 -           0.9984    | 0.9984   - 28.2999\n",
      "\u001b[1;4mValidati 63     - 100    -          0.5397 -           0.9208    | 0.9238   - 1.9024\u001b[0m\n",
      "Training 64     - 100    -          0.0023 -           0.9995    | 0.9995   - 28.2545\n",
      "\u001b[1;4mValidati 64     - 100    -          0.5546 -           0.9207    | 0.9231   - 1.9178\u001b[0m\n",
      "Training 65     - 100    -          0.0024 -           0.9995    | 0.9995   - 28.8365\n",
      "\u001b[1;4mValidati 65     - 100    -          0.5516 -           0.9200    | 0.9233   - 1.8572\u001b[0m\n",
      "Training 66     - 100    -          0.0025 -           0.9993    | 0.9993   - 28.6043\n",
      "\u001b[1;4mValidati 66     - 100    -          0.6044 -           0.9138    | 0.9168   - 1.8981\u001b[0m\n",
      "Training 67     - 100    -          0.0171 -           0.9948    | 0.9949   - 29.1670\n",
      "\u001b[1;4mValidati 67     - 100    -          0.5510 -           0.9139    | 0.9178   - 1.9218\u001b[0m\n",
      "Training 68     - 100    -          0.0053 -           0.9983    | 0.9983   - 28.4166\n",
      "\u001b[1;4mValidati 68     - 100    -          0.5233 -           0.9226    | 0.9249   - 1.8928\u001b[0m\n",
      "Training 69     - 100    -          0.0016 -           0.9996    | 0.9995   - 28.1907\n",
      "\u001b[1;4mValidati 69     - 100    -          0.5568 -           0.9203    | 0.9231   - 1.9162\u001b[0m\n",
      "Training 70     - 100    -          0.0019 -           0.9995    | 0.9995   - 28.1651\n",
      "\u001b[1;4mValidati 70     - 100    -          0.5856 -           0.9203    | 0.9228   - 1.9119\u001b[0m\n",
      "Training 71     - 100    -          0.0035 -           0.9991    | 0.9991   - 28.3415\n",
      "\u001b[1;4mValidati 71     - 100    -          0.5887 -           0.9161    | 0.9188   - 1.8454\u001b[0m\n",
      "Training 72     - 100    -          0.0078 -           0.9975    | 0.9976   - 28.4347\n",
      "\u001b[1;4mValidati 72     - 100    -          0.5969 -           0.9150    | 0.9176   - 1.9556\u001b[0m\n",
      "Training 73     - 100    -          0.0068 -           0.9978    | 0.9978   - 29.0321\n",
      "\u001b[1;4mValidati 73     - 100    -          0.5779 -           0.9174    | 0.9199   - 1.9008\u001b[0m\n",
      "Training 74     - 100    -          0.0036 -           0.9990    | 0.9990   - 28.2402\n",
      "\u001b[1;4mValidati 74     - 100    -          0.5842 -           0.9178    | 0.9201   - 1.8955\u001b[0m\n",
      "Training 75     - 100    -          0.0030 -           0.9993    | 0.9993   - 28.6589\n",
      "\u001b[1;4mValidati 75     - 100    -          0.5792 -           0.9222    | 0.9246   - 1.9505\u001b[0m\n",
      "Training 76     - 100    -          0.0035 -           0.9990    | 0.9990   - 28.7691\n",
      "\u001b[1;4mValidati 76     - 100    -          0.6057 -           0.9162    | 0.9188   - 1.9035\u001b[0m\n",
      "Training 77     - 100    -          0.0060 -           0.9982    | 0.9982   - 28.6235\n",
      "\u001b[1;4mValidati 77     - 100    -          0.5819 -           0.9177    | 0.9203   - 1.9285\u001b[0m\n",
      "Training 78     - 100    -          0.0049 -           0.9985    | 0.9985   - 29.4929\n",
      "\u001b[1;4mValidati 78     - 100    -          0.5452 -           0.9209    | 0.9247   - 1.9278\u001b[0m\n",
      "Training 79     - 100    -          0.0039 -           0.9988    | 0.9988   - 29.1541\n",
      "\u001b[1;4mValidati 79     - 100    -          0.5542 -           0.9211    | 0.9234   - 1.8933\u001b[0m\n",
      "Training 80     - 100    -          0.0024 -           0.9993    | 0.9992   - 28.2134\n",
      "\u001b[1;4mValidati 80     - 100    -          0.5736 -           0.9223    | 0.9248   - 1.8836\u001b[0m\n",
      "Training 81     - 100    -          0.0018 -           0.9995    | 0.9995   - 28.2352\n",
      "\u001b[1;4mValidati 81     - 100    -          0.5943 -           0.9211    | 0.9235   - 1.8891\u001b[0m\n",
      "Training 82     - 100    -          0.0018 -           0.9994    | 0.9994   - 29.6149\n",
      "\u001b[1;4mValidati 82     - 100    -          0.6397 -           0.9178    | 0.9196   - 1.9646\u001b[0m\n",
      "Training 83     - 100    -          0.0044 -           0.9988    | 0.9988   - 28.8574\n",
      "\u001b[1;4mValidati 83     - 100    -          0.5798 -           0.9206    | 0.9236   - 1.8920\u001b[0m\n",
      "Training 84     - 100    -          0.0070 -           0.9978    | 0.9978   - 28.3268\n",
      "\u001b[1;4mValidati 84     - 100    -          0.6160 -           0.9124    | 0.9153   - 1.8886\u001b[0m\n",
      "Training 85     - 100    -          0.0043 -           0.9986    | 0.9986   - 28.2326\n",
      "\u001b[1;4mValidati 85     - 100    -          0.5915 -           0.9183    | 0.9205   - 1.8586\u001b[0m\n",
      "Training 86     - 100    -          0.0020 -           0.9994    | 0.9994   - 28.9388\n",
      "\u001b[1;4mValidati 86     - 100    -          0.6605 -           0.9149    | 0.9172   - 1.8997\u001b[0m\n",
      "Training 87     - 100    -          0.0028 -           0.9992    | 0.9992   - 29.0970\n",
      "\u001b[1;4mValidati 87     - 100    -          0.5861 -           0.9213    | 0.9238   - 1.8963\u001b[0m\n",
      "Training 88     - 100    -          0.0045 -           0.9987    | 0.9987   - 28.3211\n",
      "\u001b[1;4mValidati 88     - 100    -          0.5693 -           0.9229    | 0.9255   - 1.8992\u001b[0m\n",
      "Training 89     - 100    -          0.0010 -           0.9997    | 0.9997   - 28.8362\n",
      "\u001b[1;4mValidati 89     - 100    -          0.5928 -           0.9224    | 0.9247   - 1.9066\u001b[0m\n",
      "Training 90     - 100    -          0.0002 -           1.0000    | 1.0000   - 29.1043\n",
      "\u001b[1;4mValidati 90     - 100    -          0.6034 -           0.9237    | 0.9252   - 1.9314\u001b[0m\n",
      "Training 91     - 100    -          0.0011 -           0.9997    | 0.9997   - 28.2228\n",
      "\u001b[1;4mValidati 91     - 100    -          0.6109 -           0.9193    | 0.9218   - 1.8746\u001b[0m\n",
      "Training 92     - 100    -          0.0012 -           0.9998    | 0.9998   - 29.2132\n",
      "\u001b[1;4mValidati 92     - 100    -          0.6042 -           0.9243    | 0.9258   - 1.9086\u001b[0m\n",
      "Training 93     - 100    -          0.0006 -           0.9998    | 0.9999   - 28.5540\n",
      "\u001b[1;4mValidati 93     - 100    -          0.6162 -           0.9240    | 0.9261   - 1.9231\u001b[0m\n",
      "Training 94     - 100    -          0.0076 -           0.9976    | 0.9977   - 29.1800\n",
      "\u001b[1;4mValidati 94     - 100    -          0.5969 -           0.9185    | 0.9207   - 1.9291\u001b[0m\n",
      "Training 95     - 100    -          0.0051 -           0.9986    | 0.9986   - 28.3931\n",
      "\u001b[1;4mValidati 95     - 100    -          0.5893 -           0.9165    | 0.9196   - 1.8729\u001b[0m\n",
      "Training 96     - 100    -          0.0009 -           0.9998    | 0.9997   - 28.2916\n",
      "\u001b[1;4mValidati 96     - 100    -          0.5706 -           0.9229    | 0.9254   - 1.8931\u001b[0m\n",
      "Training 97     - 100    -          0.0003 -           0.9999    | 0.9999   - 28.4440\n",
      "\u001b[1;4mValidati 97     - 100    -          0.5805 -           0.9210    | 0.9237   - 1.9121\u001b[0m\n",
      "Training 98     - 100    -          0.0012 -           0.9997    | 0.9997   - 28.4053\n",
      "\u001b[1;4mValidati 98     - 100    -          0.5774 -           0.9240    | 0.9269   - 1.9161\u001b[0m\n",
      "Training 99     - 100    -          0.0052 -           0.9986    | 0.9986   - 28.7504\n",
      "\u001b[1;4mValidati 99     - 100    -          0.6271 -           0.9170    | 0.9196   - 1.9560\u001b[0m\n",
      "Training 100    - 100    -          0.0015 -           0.9996    | 0.9996   - 29.1666\n",
      "\u001b[1;4mValidati 100    - 100    -          0.6037 -           0.9244    | 0.9264   - 1.9347\u001b[0m\n",
      "Training 101    - 100    -          0.0005 -           0.9999    | 0.9999   - 28.9396\n",
      "\u001b[1;4mValidati 101    - 100    -          0.6034 -           0.9246    | 0.9269   - 1.9166\u001b[0m\n",
      "Training 102    - 100    -          0.0003 -           0.9999    | 0.9999   - 28.2284\n",
      "\u001b[1;4mValidati 102    - 100    -          0.5960 -           0.9228    | 0.9250   - 1.8930\u001b[0m\n",
      "Training 103    - 100    -          0.0001 -           1.0000    | 1.0000   - 28.2273\n",
      "\u001b[1;4mValidati 103    - 100    -          0.5967 -           0.9242    | 0.9263   - 1.9003\u001b[0m\n",
      "Training 104    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.4730\n",
      "\u001b[1;4mValidati 104    - 100    -          0.5988 -           0.9246    | 0.9265   - 1.9109\u001b[0m\n",
      "Training 105    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.0220\n",
      "\u001b[1;4mValidati 105    - 100    -          0.5999 -           0.9252    | 0.9269   - 1.9184\u001b[0m\n",
      "Training 106    - 100    -          0.0076 -           0.9978    | 0.9977   - 29.1906\n",
      "\u001b[1;4mValidati 106    - 100    -          0.6068 -           0.9181    | 0.9204   - 1.8965\u001b[0m\n",
      "Training 107    - 100    -          0.0027 -           0.9991    | 0.9991   - 29.0641\n",
      "\u001b[1;4mValidati 107    - 100    -          0.5720 -           0.9229    | 0.9251   - 1.9535\u001b[0m\n",
      "Training 108    - 100    -          0.0004 -           0.9999    | 0.9999   - 29.7903\n",
      "\u001b[1;4mValidati 108    - 100    -          0.5882 -           0.9232    | 0.9256   - 1.9064\u001b[0m\n",
      "Training 109    - 100    -          0.0001 -           1.0000    | 1.0000   - 28.5067\n",
      "\u001b[1;4mValidati 109    - 100    -          0.5921 -           0.9246    | 0.9270   - 1.9648\u001b[0m\n",
      "Training 110    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.1205\n",
      "\u001b[1;4mValidati 110    - 100    -          0.6014 -           0.9246    | 0.9266   - 1.9733\u001b[0m\n",
      "Training 111    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.3751\n",
      "\u001b[1;4mValidati 111    - 100    -          0.6020 -           0.9247    | 0.9266   - 1.9179\u001b[0m\n",
      "Training 112    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.4190\n",
      "\u001b[1;4mValidati 112    - 100    -          0.6048 -           0.9242    | 0.9261   - 1.9093\u001b[0m\n",
      "Training 113    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.7759\n",
      "\u001b[1;4mValidati 113    - 100    -          0.6110 -           0.9248    | 0.9268   - 1.8707\u001b[0m\n",
      "Training 114    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.8083\n",
      "\u001b[1;4mValidati 114    - 100    -          0.6127 -           0.9251    | 0.9268   - 1.9013\u001b[0m\n",
      "Training 115    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.0322\n",
      "\u001b[1;4mValidati 115    - 100    -          0.6267 -           0.9240    | 0.9258   - 1.9030\u001b[0m\n",
      "Training 116    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.4250\n",
      "\u001b[1;4mValidati 116    - 100    -          0.6171 -           0.9257    | 0.9273   - 2.0124\u001b[0m\n",
      "Training 117    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.8384\n",
      "\u001b[1;4mValidati 117    - 100    -          0.6185 -           0.9256    | 0.9273   - 1.9006\u001b[0m\n",
      "Training 118    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.9087\n",
      "\u001b[1;4mValidati 118    - 100    -          0.6246 -           0.9257    | 0.9274   - 1.9273\u001b[0m\n",
      "Training 119    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.3285\n",
      "\u001b[1;4mValidati 119    - 100    -          0.6368 -           0.9256    | 0.9271   - 1.9231\u001b[0m\n",
      "Training 120    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.1375\n",
      "\u001b[1;4mValidati 120    - 100    -          0.6323 -           0.9266    | 0.9279   - 1.9643\u001b[0m\n",
      "Training 121    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.1230\n",
      "\u001b[1;4mValidati 121    - 100    -          0.6387 -           0.9265    | 0.9280   - 1.9622\u001b[0m\n",
      "Training 122    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.4678\n",
      "\u001b[1;4mValidati 122    - 100    -          0.6438 -           0.9252    | 0.9266   - 1.9280\u001b[0m\n",
      "Training 123    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.3877\n",
      "\u001b[1;4mValidati 123    - 100    -          0.6437 -           0.9270    | 0.9285   - 1.9227\u001b[0m\n",
      "Training 124    - 100    -          0.0069 -           0.9980    | 0.9980   - 28.4658\n",
      "\u001b[1;4mValidati 124    - 100    -          0.6296 -           0.9186    | 0.9204   - 1.9118\u001b[0m\n",
      "Training 125    - 100    -          0.0007 -           0.9998    | 0.9998   - 29.4794\n",
      "\u001b[1;4mValidati 125    - 100    -          0.6264 -           0.9222    | 0.9241   - 1.9777\u001b[0m\n",
      "Training 126    - 100    -          0.0001 -           1.0000    | 1.0000   - 29.1866\n",
      "\u001b[1;4mValidati 126    - 100    -          0.6151 -           0.9238    | 0.9260   - 1.9366\u001b[0m\n",
      "Training 127    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.2709\n",
      "\u001b[1;4mValidati 127    - 100    -          0.6374 -           0.9231    | 0.9250   - 1.9202\u001b[0m\n",
      "Training 128    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.1626\n",
      "\u001b[1;4mValidati 128    - 100    -          0.6376 -           0.9225    | 0.9245   - 1.9068\u001b[0m\n",
      "Training 129    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.2434\n",
      "\u001b[1;4mValidati 129    - 100    -          0.6384 -           0.9230    | 0.9249   - 1.9163\u001b[0m\n",
      "Training 130    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.4630\n",
      "\u001b[1;4mValidati 130    - 100    -          0.6392 -           0.9232    | 0.9252   - 1.9221\u001b[0m\n",
      "Training 131    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.5891\n",
      "\u001b[1;4mValidati 131    - 100    -          0.6407 -           0.9233    | 0.9252   - 1.9791\u001b[0m\n",
      "Training 132    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.3325\n",
      "\u001b[1;4mValidati 132    - 100    -          0.6363 -           0.9236    | 0.9254   - 2.0209\u001b[0m\n",
      "Training 133    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.1421\n",
      "\u001b[1;4mValidati 133    - 100    -          0.6549 -           0.9235    | 0.9254   - 1.9325\u001b[0m\n",
      "Training 134    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.8233\n",
      "\u001b[1;4mValidati 134    - 100    -          0.6561 -           0.9246    | 0.9262   - 1.9004\u001b[0m\n",
      "Training 135    - 100    -          0.0015 -           0.9996    | 0.9996   - 28.5231\n",
      "\u001b[1;4mValidati 135    - 100    -          0.7142 -           0.9163    | 0.9181   - 1.9004\u001b[0m\n",
      "Training 136    - 100    -          0.0019 -           0.9994    | 0.9994   - 29.0070\n",
      "\u001b[1;4mValidati 136    - 100    -          0.6441 -           0.9223    | 0.9240   - 1.9240\u001b[0m\n",
      "Training 137    - 100    -          0.0004 -           0.9998    | 0.9998   - 28.8827\n",
      "\u001b[1;4mValidati 137    - 100    -          0.6303 -           0.9251    | 0.9268   - 1.8952\u001b[0m\n",
      "Training 138    - 100    -          0.0007 -           0.9998    | 0.9998   - 28.2768\n",
      "\u001b[1;4mValidati 138    - 100    -          0.6434 -           0.9230    | 0.9244   - 1.9064\u001b[0m\n",
      "Training 139    - 100    -          0.0003 -           0.9999    | 0.9999   - 28.2048\n",
      "\u001b[1;4mValidati 139    - 100    -          0.6521 -           0.9217    | 0.9235   - 1.8563\u001b[0m\n",
      "Training 140    - 100    -          0.0001 -           1.0000    | 1.0000   - 28.2963\n",
      "\u001b[1;4mValidati 140    - 100    -          0.6438 -           0.9240    | 0.9256   - 1.9000\u001b[0m\n",
      "Training 141    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.1290\n",
      "\u001b[1;4mValidati 141    - 100    -          0.6436 -           0.9232    | 0.9249   - 1.8861\u001b[0m\n",
      "Training 142    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.1610\n",
      "\u001b[1;4mValidati 142    - 100    -          0.6368 -           0.9247    | 0.9264   - 1.8905\u001b[0m\n",
      "Training 143    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.2154\n",
      "\u001b[1;4mValidati 143    - 100    -          0.6422 -           0.9240    | 0.9258   - 1.8881\u001b[0m\n",
      "Training 144    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.1495\n",
      "\u001b[1;4mValidati 144    - 100    -          0.6475 -           0.9238    | 0.9255   - 1.8422\u001b[0m\n",
      "Training 145    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.2019\n",
      "\u001b[1;4mValidati 145    - 100    -          0.6524 -           0.9237    | 0.9253   - 1.8805\u001b[0m\n",
      "Training 146    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.6296\n",
      "\u001b[1;4mValidati 146    - 100    -          0.6532 -           0.9245    | 0.9259   - 1.8475\u001b[0m\n",
      "Training 147    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.8273\n",
      "\u001b[1;4mValidati 147    - 100    -          0.6527 -           0.9240    | 0.9260   - 1.8919\u001b[0m\n",
      "Training 148    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.0160\n",
      "\u001b[1;4mValidati 148    - 100    -          0.6751 -           0.9235    | 0.9250   - 1.8468\u001b[0m\n",
      "Training 149    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.2521\n",
      "\u001b[1;4mValidati 149    - 100    -          0.6697 -           0.9230    | 0.9245   - 1.9006\u001b[0m\n",
      "Training 150    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.6028\n",
      "\u001b[1;4mValidati 150    - 100    -          0.6633 -           0.9244    | 0.9260   - 1.8360\u001b[0m\n",
      "Training 151    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.1160\n",
      "\u001b[1;4mValidati 151    - 100    -          0.6619 -           0.9238    | 0.9254   - 1.8899\u001b[0m\n",
      "Training 152    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.0887\n",
      "\u001b[1;4mValidati 152    - 100    -          0.6661 -           0.9240    | 0.9258   - 1.8458\u001b[0m\n",
      "Training 153    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.1932\n",
      "\u001b[1;4mValidati 153    - 100    -          0.6812 -           0.9244    | 0.9258   - 1.9615\u001b[0m\n",
      "Training 154    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.0291\n",
      "\u001b[1;4mValidati 154    - 100    -          0.6808 -           0.9247    | 0.9259   - 1.9306\u001b[0m\n",
      "Training 155    - 100    -          0.0000 -           1.0000    | 1.0000   - 29.2775\n",
      "\u001b[1;4mValidati 155    - 100    -          0.6764 -           0.9249    | 0.9264   - 1.9157\u001b[0m\n",
      "Training 156    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.7796\n",
      "\u001b[1;4mValidati 156    - 100    -          0.6785 -           0.9250    | 0.9265   - 1.9148\u001b[0m\n",
      "Training 157    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.9215\n",
      "\u001b[1;4mValidati 157    - 100    -          0.6790 -           0.9252    | 0.9264   - 1.8477\u001b[0m\n",
      "Training 158    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.2627\n",
      "\u001b[1;4mValidati 158    - 100    -          0.6948 -           0.9241    | 0.9257   - 1.8984\u001b[0m\n",
      "Training 159    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.5781\n",
      "\u001b[1;4mValidati 159    - 100    -          0.6921 -           0.9253    | 0.9269   - 1.8861\u001b[0m\n",
      "Training 160    - 100    -          0.0000 -           1.0000    | 1.0000   - 28.6876\n",
      "\u001b[1;4mValidati 160    - 100    -          0.6877 -           0.9252    | 0.9265   - 1.9106\u001b[0m\n",
      "Training 161    - 90     -          0.0000 -           1.0000    | 1.0000   - 25.4784\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .llll||=||llll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
