{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "from DCT.util.utils import reset_seed, get_datetime\n",
    "from DCT.util.model_loader import get_model_from_name\n",
    "from DCT.util.dataset_loader import load_dataset\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"SpeechCommand\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"resnet18\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=1.0, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=256, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=50, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=256, checkpoint_path='supervised', checkpoint_root='../model_save/', dataset='SpeechCommand', dataset_root='../datasets', from_config='', learning_rate=0.001, model='resnet18', nb_epoch=200, num_classes=50, resume=False, seed=1234, supervised_ratio=1.0, tensorboard_path='supervised', tensorboard_root='../tensorboard/', tensorboard_sufix='', train_folds=[1, 2, 3, 4], val_folds=[5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadUpTo(nn.Module):\n",
    "    def __init__(self, target_length, mode: str = \"constant\", value: int = 0):\n",
    "        super().__init__()\n",
    "        self.target_length = target_length\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        actual_length = x.size()[-1]\n",
    "        return F.pad(input=x, pad=(0, (self.target_length - actual_length)), mode=self.mode, value=self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "transforms = nn.Sequential(\n",
    "    PadUpTo(target_length=16000, mode=\"constant\", value=0),\n",
    "    MelSpectrogram(sample_rate=16000, n_fft=2048, hop_length=512, n_mels=64), \n",
    "    AmplitudeToDB(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/SpeechCommands/speech_commands_v0.02\n",
      "Dataset already download and verified\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"supervised\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    transform=transforms,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 64, 32])\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(len(np.unique(y.numpy())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = get_model_from_name(args.model)\n",
    "# model_func = get_model_from_name(\"esc_wideresnet28_8\")\n",
    "model = model_func(input_shape=(64, 431), num_classes = args.num_classes)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "                                          Kernel Shape      Output Shape  \\\n",
      "Layer                                                                      \n",
      "0_conv1                                  [3, 64, 7, 7]  [1, 64, 32, 216]   \n",
      "1_bn1                                             [64]  [1, 64, 32, 216]   \n",
      "2_relu                                               -  [1, 64, 32, 216]   \n",
      "3_maxpool                                            -  [1, 64, 16, 108]   \n",
      "4_layer1.0.Conv2d_conv1                 [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "5_layer1.0.BatchNorm2d_bn1                        [64]  [1, 64, 16, 108]   \n",
      "6_layer1.0.ReLU_relu                                 -  [1, 64, 16, 108]   \n",
      "7_layer1.0.Conv2d_conv2                 [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "8_layer1.0.BatchNorm2d_bn2                        [64]  [1, 64, 16, 108]   \n",
      "9_layer1.0.ReLU_relu                                 -  [1, 64, 16, 108]   \n",
      "10_layer1.1.Conv2d_conv1                [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "11_layer1.1.BatchNorm2d_bn1                       [64]  [1, 64, 16, 108]   \n",
      "12_layer1.1.ReLU_relu                                -  [1, 64, 16, 108]   \n",
      "13_layer1.1.Conv2d_conv2                [64, 64, 3, 3]  [1, 64, 16, 108]   \n",
      "14_layer1.1.BatchNorm2d_bn2                       [64]  [1, 64, 16, 108]   \n",
      "15_layer1.1.ReLU_relu                                -  [1, 64, 16, 108]   \n",
      "16_layer2.0.Conv2d_conv1               [64, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "17_layer2.0.BatchNorm2d_bn1                      [128]   [1, 128, 8, 54]   \n",
      "18_layer2.0.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "19_layer2.0.Conv2d_conv2              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "20_layer2.0.BatchNorm2d_bn2                      [128]   [1, 128, 8, 54]   \n",
      "21_layer2.0.downsample.Conv2d_0        [64, 128, 1, 1]   [1, 128, 8, 54]   \n",
      "22_layer2.0.downsample.BatchNorm2d_1             [128]   [1, 128, 8, 54]   \n",
      "23_layer2.0.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "24_layer2.1.Conv2d_conv1              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "25_layer2.1.BatchNorm2d_bn1                      [128]   [1, 128, 8, 54]   \n",
      "26_layer2.1.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "27_layer2.1.Conv2d_conv2              [128, 128, 3, 3]   [1, 128, 8, 54]   \n",
      "28_layer2.1.BatchNorm2d_bn2                      [128]   [1, 128, 8, 54]   \n",
      "29_layer2.1.ReLU_relu                                -   [1, 128, 8, 54]   \n",
      "30_layer3.0.Conv2d_conv1              [128, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "31_layer3.0.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "32_layer3.0.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "33_layer3.0.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "34_layer3.0.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "35_layer3.0.downsample.Conv2d_0       [128, 256, 1, 1]   [1, 256, 4, 27]   \n",
      "36_layer3.0.downsample.BatchNorm2d_1             [256]   [1, 256, 4, 27]   \n",
      "37_layer3.0.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "38_layer3.1.Conv2d_conv1              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "39_layer3.1.BatchNorm2d_bn1                      [256]   [1, 256, 4, 27]   \n",
      "40_layer3.1.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "41_layer3.1.Conv2d_conv2              [256, 256, 3, 3]   [1, 256, 4, 27]   \n",
      "42_layer3.1.BatchNorm2d_bn2                      [256]   [1, 256, 4, 27]   \n",
      "43_layer3.1.ReLU_relu                                -   [1, 256, 4, 27]   \n",
      "44_layer4.0.Conv2d_conv1              [256, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "45_layer4.0.BatchNorm2d_bn1                      [512]   [1, 512, 2, 14]   \n",
      "46_layer4.0.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "47_layer4.0.Conv2d_conv2              [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "48_layer4.0.BatchNorm2d_bn2                      [512]   [1, 512, 2, 14]   \n",
      "49_layer4.0.downsample.Conv2d_0       [256, 512, 1, 1]   [1, 512, 2, 14]   \n",
      "50_layer4.0.downsample.BatchNorm2d_1             [512]   [1, 512, 2, 14]   \n",
      "51_layer4.0.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "52_layer4.1.Conv2d_conv1              [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "53_layer4.1.BatchNorm2d_bn1                      [512]   [1, 512, 2, 14]   \n",
      "54_layer4.1.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "55_layer4.1.Conv2d_conv2              [512, 512, 3, 3]   [1, 512, 2, 14]   \n",
      "56_layer4.1.BatchNorm2d_bn2                      [512]   [1, 512, 2, 14]   \n",
      "57_layer4.1.ReLU_relu                                -   [1, 512, 2, 14]   \n",
      "58_avgpool                                           -    [1, 512, 1, 1]   \n",
      "59_fc                                        [512, 50]           [1, 50]   \n",
      "\n",
      "                                         Params   Mult-Adds  \n",
      "Layer                                                        \n",
      "0_conv1                                  9.408k  65.028096M  \n",
      "1_bn1                                     128.0        64.0  \n",
      "2_relu                                        -           -  \n",
      "3_maxpool                                     -           -  \n",
      "4_layer1.0.Conv2d_conv1                 36.864k  63.700992M  \n",
      "5_layer1.0.BatchNorm2d_bn1                128.0        64.0  \n",
      "6_layer1.0.ReLU_relu                          -           -  \n",
      "7_layer1.0.Conv2d_conv2                 36.864k  63.700992M  \n",
      "8_layer1.0.BatchNorm2d_bn2                128.0        64.0  \n",
      "9_layer1.0.ReLU_relu                          -           -  \n",
      "10_layer1.1.Conv2d_conv1                36.864k  63.700992M  \n",
      "11_layer1.1.BatchNorm2d_bn1               128.0        64.0  \n",
      "12_layer1.1.ReLU_relu                         -           -  \n",
      "13_layer1.1.Conv2d_conv2                36.864k  63.700992M  \n",
      "14_layer1.1.BatchNorm2d_bn2               128.0        64.0  \n",
      "15_layer1.1.ReLU_relu                         -           -  \n",
      "16_layer2.0.Conv2d_conv1                73.728k  31.850496M  \n",
      "17_layer2.0.BatchNorm2d_bn1               256.0       128.0  \n",
      "18_layer2.0.ReLU_relu                         -           -  \n",
      "19_layer2.0.Conv2d_conv2               147.456k  63.700992M  \n",
      "20_layer2.0.BatchNorm2d_bn2               256.0       128.0  \n",
      "21_layer2.0.downsample.Conv2d_0          8.192k   3.538944M  \n",
      "22_layer2.0.downsample.BatchNorm2d_1      256.0       128.0  \n",
      "23_layer2.0.ReLU_relu                         -           -  \n",
      "24_layer2.1.Conv2d_conv1               147.456k  63.700992M  \n",
      "25_layer2.1.BatchNorm2d_bn1               256.0       128.0  \n",
      "26_layer2.1.ReLU_relu                         -           -  \n",
      "27_layer2.1.Conv2d_conv2               147.456k  63.700992M  \n",
      "28_layer2.1.BatchNorm2d_bn2               256.0       128.0  \n",
      "29_layer2.1.ReLU_relu                         -           -  \n",
      "30_layer3.0.Conv2d_conv1               294.912k  31.850496M  \n",
      "31_layer3.0.BatchNorm2d_bn1               512.0       256.0  \n",
      "32_layer3.0.ReLU_relu                         -           -  \n",
      "33_layer3.0.Conv2d_conv2               589.824k  63.700992M  \n",
      "34_layer3.0.BatchNorm2d_bn2               512.0       256.0  \n",
      "35_layer3.0.downsample.Conv2d_0         32.768k   3.538944M  \n",
      "36_layer3.0.downsample.BatchNorm2d_1      512.0       256.0  \n",
      "37_layer3.0.ReLU_relu                         -           -  \n",
      "38_layer3.1.Conv2d_conv1               589.824k  63.700992M  \n",
      "39_layer3.1.BatchNorm2d_bn1               512.0       256.0  \n",
      "40_layer3.1.ReLU_relu                         -           -  \n",
      "41_layer3.1.Conv2d_conv2               589.824k  63.700992M  \n",
      "42_layer3.1.BatchNorm2d_bn2               512.0       256.0  \n",
      "43_layer3.1.ReLU_relu                         -           -  \n",
      "44_layer4.0.Conv2d_conv1              1.179648M  33.030144M  \n",
      "45_layer4.0.BatchNorm2d_bn1              1.024k       512.0  \n",
      "46_layer4.0.ReLU_relu                         -           -  \n",
      "47_layer4.0.Conv2d_conv2              2.359296M  66.060288M  \n",
      "48_layer4.0.BatchNorm2d_bn2              1.024k       512.0  \n",
      "49_layer4.0.downsample.Conv2d_0        131.072k   3.670016M  \n",
      "50_layer4.0.downsample.BatchNorm2d_1     1.024k       512.0  \n",
      "51_layer4.0.ReLU_relu                         -           -  \n",
      "52_layer4.1.Conv2d_conv1              2.359296M  66.060288M  \n",
      "53_layer4.1.BatchNorm2d_bn1              1.024k       512.0  \n",
      "54_layer4.1.ReLU_relu                         -           -  \n",
      "55_layer4.1.Conv2d_conv2              2.359296M  66.060288M  \n",
      "56_layer4.1.BatchNorm2d_bn2              1.024k       512.0  \n",
      "57_layer4.1.ReLU_relu                         -           -  \n",
      "58_avgpool                                    -           -  \n",
      "59_fc                                    25.65k       25.6k  \n",
      "-----------------------------------------------------------------------------------------------\n",
      "                           Totals\n",
      "Total params           11.202162M\n",
      "Trainable params       11.202162M\n",
      "Non-trainable params          0.0\n",
      "Mult-Adds             1.00772832G\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "nb_conv = 0\n",
    "\n",
    "for layer in s.index.values:\n",
    "    if \"Conv\" in layer:\n",
    "        nb_conv += 1\n",
    "print(nb_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/SpeechCommand/supervised/resnet18/1.0S/2020-09-22_11:35:41_resnet18_1.0S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "title_element = (args.model, args.supervised_ratio, get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "tensorboard_title = \"%s/%sS/%s_%s_%.1fS\" % title_element\n",
    "\n",
    "title_element = (model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % title_element\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))\n",
    "\n",
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_params = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    tensorboard_params[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard.add_hparams(tensorboard_params, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar10 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    def lr_lambda(e):\n",
    "        if e < 60:\n",
    "            return 1\n",
    "\n",
    "        elif 60 <= e < 120:\n",
    "            return 0.2\n",
    "\n",
    "        elif 120 <= e < 160:\n",
    "            return 0.04\n",
    "\n",
    "        else:\n",
    "            return 0.008\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ubs8k optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"ubs8k\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n",
    "\n",
    "elif args.dataset in (\"esc10\", 'esc50'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n",
    "    \n",
    "elif args.dataset in (\"SpeechCommand\",):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# Metrics\n",
    "fscore_fn = FScore()\n",
    "acc_fn = CategoricalAccuracy()\n",
    "avg = ContinueAverage()\n",
    "\n",
    "reset_metrics = lambda : [m.reset() for m in [fscore_fn, acc_fn, avg]]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2ca01b3610>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, args.nb_epoch, args.nb_epoch)\n",
    "y = [lr_lambda(x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \".               \", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"metrics: \", \"acc\", \"F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        logits = model(X)        \n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_ce(logits, y)\n",
    "\n",
    "            # metrics\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc\", acc, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc\", maximum_fn(\"acc\", acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/f1\", maximum_fn(\"f1\", fscore), epoch )\n",
    "\n",
    "    checkpoint.step(acc)\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n",
      "\n",
      "Training 1      - 100    -          0.8453 -           0.7491    | 0.7406   - 146.0288\n",
      "\u001b[1;4mValidati 1      - 100    -          0.6571 -           0.7984    | 0.8096   - 34.1217\u001b[0m\n",
      "Training 2      - 100    -          0.3660 -           0.8865    | 0.8952   - 19.9401\n",
      "\u001b[1;4mValidati 2      - 100    -          0.4334 -           0.8719    | 0.8838   - 1.2396\u001b[0m\n",
      "Training 3      - 100    -          0.2715 -           0.9147    | 0.9220   - 19.0795\n",
      "\u001b[1;4mValidati 3      - 100    -          0.4112 -           0.8797    | 0.8908   - 1.2018\u001b[0m\n",
      "Training 4      - 100    -          0.2188 -           0.9315    | 0.9374   - 18.5134\n",
      "\u001b[1;4mValidati 4      - 100    -          0.3744 -           0.8881    | 0.8972   - 1.1557\u001b[0m\n",
      "Training 5      - 100    -          0.1866 -           0.9402    | 0.9457   - 18.5119\n",
      "\u001b[1;4mValidati 5      - 100    -          0.3763 -           0.8923    | 0.9025   - 1.1573\u001b[0m\n",
      "Training 6      - 100    -          0.1571 -           0.9504    | 0.9552   - 18.8569\n",
      "\u001b[1;4mValidati 6      - 100    -          0.4257 -           0.8800    | 0.8906   - 1.1975\u001b[0m\n",
      "Training 7      - 100    -          0.1337 -           0.9573    | 0.9612   - 19.5510\n",
      "\u001b[1;4mValidati 7      - 100    -          0.4316 -           0.8854    | 0.8950   - 1.2194\u001b[0m\n",
      "Training 8      - 100    -          0.1214 -           0.9610    | 0.9648   - 19.1342\n",
      "\u001b[1;4mValidati 8      - 100    -          0.3437 -           0.9048    | 0.9144   - 1.1973\u001b[0m\n",
      "Training 9      - 100    -          0.1036 -           0.9663    | 0.9694   - 18.5495\n",
      "\u001b[1;4mValidati 9      - 100    -          0.3604 -           0.9046    | 0.9140   - 1.1738\u001b[0m\n",
      "Training 10     - 100    -          0.0924 -           0.9702    | 0.9731   - 18.5958\n",
      "\u001b[1;4mValidati 10     - 100    -          0.4285 -           0.8936    | 0.9033   - 1.1535\u001b[0m\n",
      "Training 11     - 100    -          0.0894 -           0.9714    | 0.9738   - 19.1654\n",
      "\u001b[1;4mValidati 11     - 100    -          0.3972 -           0.9042    | 0.9127   - 1.1984\u001b[0m\n",
      "Training 12     - 100    -          0.0687 -           0.9780    | 0.9799   - 18.6299\n",
      "\u001b[1;4mValidati 12     - 100    -          0.3897 -           0.9063    | 0.9146   - 1.1133\u001b[0m\n",
      "Training 13     - 100    -          0.0686 -           0.9785    | 0.9802   - 19.2246\n",
      "\u001b[1;4mValidati 13     - 100    -          0.4433 -           0.8975    | 0.9052   - 1.1826\u001b[0m\n",
      "Training 14     - 100    -          0.0625 -           0.9800    | 0.9816   - 18.6205\n",
      "\u001b[1;4mValidati 14     - 100    -          0.5233 -           0.8885    | 0.8959   - 1.1667\u001b[0m\n",
      "Training 15     - 100    -          0.0560 -           0.9820    | 0.9831   - 18.8641\n",
      "\u001b[1;4mValidati 15     - 100    -          0.4113 -           0.9082    | 0.9149   - 1.1830\u001b[0m\n",
      "Training 16     - 100    -          0.0442 -           0.9859    | 0.9868   - 18.6069\n",
      "\u001b[1;4mValidati 16     - 100    -          0.4683 -           0.9027    | 0.9094   - 1.1607\u001b[0m\n",
      "Training 17     - 100    -          0.0520 -           0.9831    | 0.9840   - 18.5991\n",
      "\u001b[1;4mValidati 17     - 100    -          0.5099 -           0.8957    | 0.9018   - 1.1751\u001b[0m\n",
      "Training 18     - 100    -          0.0423 -           0.9861    | 0.9867   - 18.5423\n",
      "\u001b[1;4mValidati 18     - 100    -          0.4596 -           0.9080    | 0.9140   - 1.1526\u001b[0m\n",
      "Training 19     - 100    -          0.0399 -           0.9872    | 0.9873   - 18.5108\n",
      "\u001b[1;4mValidati 19     - 100    -          0.4445 -           0.9111    | 0.9164   - 1.1118\u001b[0m\n",
      "Training 20     - 100    -          0.0294 -           0.9907    | 0.9911   - 18.5980\n",
      "\u001b[1;4mValidati 20     - 100    -          0.4901 -           0.9022    | 0.9078   - 1.1618\u001b[0m\n",
      "Training 21     - 100    -          0.0304 -           0.9906    | 0.9907   - 18.4424\n",
      "\u001b[1;4mValidati 21     - 100    -          0.4670 -           0.9099    | 0.9151   - 1.1122\u001b[0m\n",
      "Training 22     - 100    -          0.0332 -           0.9888    | 0.9890   - 19.8437\n",
      "\u001b[1;4mValidati 22     - 100    -          0.5455 -           0.8963    | 0.9015   - 1.2519\u001b[0m\n",
      "Training 23     - 100    -          0.0309 -           0.9903    | 0.9904   - 19.0300\n",
      "\u001b[1;4mValidati 23     - 100    -          0.5157 -           0.9025    | 0.9075   - 1.1735\u001b[0m\n",
      "Training 24     - 100    -          0.0279 -           0.9907    | 0.9908   - 18.9736\n",
      "\u001b[1;4mValidati 24     - 100    -          0.4699 -           0.9117    | 0.9171   - 1.1703\u001b[0m\n",
      "Training 25     - 100    -          0.0155 -           0.9948    | 0.9949   - 19.1762\n",
      "\u001b[1;4mValidati 25     - 100    -          0.5531 -           0.9019    | 0.9064   - 1.2214\u001b[0m\n",
      "Training 26     - 100    -          0.0234 -           0.9924    | 0.9924   - 19.2709\n",
      "\u001b[1;4mValidati 26     - 100    -          0.5143 -           0.9062    | 0.9104   - 1.1999\u001b[0m\n",
      "Training 27     - 100    -          0.0228 -           0.9927    | 0.9926   - 19.2375\n",
      "\u001b[1;4mValidati 27     - 100    -          0.5172 -           0.9100    | 0.9143   - 1.1946\u001b[0m\n",
      "Training 28     - 100    -          0.0154 -           0.9950    | 0.9950   - 19.2428\n",
      "\u001b[1;4mValidati 28     - 100    -          0.5174 -           0.9088    | 0.9134   - 1.1937\u001b[0m\n",
      "Training 29     - 100    -          0.0230 -           0.9927    | 0.9927   - 18.5910\n",
      "\u001b[1;4mValidati 29     - 100    -          0.5147 -           0.9123    | 0.9166   - 1.1651\u001b[0m\n",
      "Training 30     - 100    -          0.0113 -           0.9965    | 0.9966   - 19.0475\n",
      "\u001b[1;4mValidati 30     - 100    -          0.5408 -           0.9100    | 0.9140   - 1.2323\u001b[0m\n",
      "Training 31     - 100    -          0.0184 -           0.9939    | 0.9940   - 18.6336\n",
      "\u001b[1;4mValidati 31     - 100    -          0.5671 -           0.9022    | 0.9065   - 1.1822\u001b[0m\n",
      "Training 32     - 100    -          0.0205 -           0.9934    | 0.9934   - 18.6394\n",
      "\u001b[1;4mValidati 32     - 100    -          0.5600 -           0.9044    | 0.9078   - 1.1670\u001b[0m\n",
      "Training 33     - 100    -          0.0114 -           0.9967    | 0.9967   - 18.5889\n",
      "\u001b[1;4mValidati 33     - 100    -          0.5538 -           0.9095    | 0.9133   - 1.2141\u001b[0m\n",
      "Training 34     - 100    -          0.0140 -           0.9956    | 0.9956   - 18.5282\n",
      "\u001b[1;4mValidati 34     - 100    -          0.5482 -           0.9120    | 0.9156   - 1.1647\u001b[0m\n",
      "Training 35     - 100    -          0.0143 -           0.9955    | 0.9955   - 18.5069\n",
      "\u001b[1;4mValidati 35     - 100    -          0.5776 -           0.9054    | 0.9093   - 1.1169\u001b[0m\n",
      "Training 36     - 100    -          0.0121 -           0.9959    | 0.9959   - 19.1734\n",
      "\u001b[1;4mValidati 36     - 100    -          0.5404 -           0.9110    | 0.9143   - 1.2101\u001b[0m\n",
      "Training 37     - 100    -          0.0083 -           0.9973    | 0.9973   - 19.8638\n",
      "\u001b[1;4mValidati 37     - 100    -          0.6135 -           0.9033    | 0.9068   - 1.2562\u001b[0m\n",
      "Training 38     - 100    -          0.0189 -           0.9943    | 0.9942   - 19.1692\n",
      "\u001b[1;4mValidati 38     - 100    -          0.5505 -           0.9062    | 0.9106   - 1.2269\u001b[0m\n",
      "Training 39     - 100    -          0.0171 -           0.9947    | 0.9946   - 19.1573\n",
      "\u001b[1;4mValidati 39     - 100    -          0.5135 -           0.9113    | 0.9156   - 1.2223\u001b[0m\n",
      "Training 40     - 100    -          0.0067 -           0.9978    | 0.9978   - 19.1456\n",
      "\u001b[1;4mValidati 40     - 100    -          0.5989 -           0.9044    | 0.9087   - 1.2224\u001b[0m\n",
      "Training 41     - 100    -          0.0093 -           0.9972    | 0.9972   - 19.0587\n",
      "\u001b[1;4mValidati 41     - 100    -          0.5122 -           0.9179    | 0.9215   - 1.1908\u001b[0m\n",
      "Training 42     - 100    -          0.0091 -           0.9970    | 0.9969   - 19.1722\n",
      "\u001b[1;4mValidati 42     - 100    -          0.5821 -           0.9093    | 0.9123   - 1.2321\u001b[0m\n",
      "Training 43     - 100    -          0.0108 -           0.9964    | 0.9964   - 20.1305\n",
      "\u001b[1;4mValidati 43     - 100    -          0.5834 -           0.9097    | 0.9131   - 1.2574\u001b[0m\n",
      "Training 44     - 100    -          0.0127 -           0.9959    | 0.9959   - 19.2178\n",
      "\u001b[1;4mValidati 44     - 100    -          0.5456 -           0.9103    | 0.9143   - 1.1953\u001b[0m\n",
      "Training 45     - 100    -          0.0127 -           0.9962    | 0.9962   - 18.5924\n",
      "\u001b[1;4mValidati 45     - 100    -          0.5652 -           0.9070    | 0.9107   - 1.1846\u001b[0m\n",
      "Training 46     - 100    -          0.0053 -           0.9983    | 0.9983   - 18.5849\n",
      "\u001b[1;4mValidati 46     - 100    -          0.5219 -           0.9175    | 0.9205   - 1.1631\u001b[0m\n",
      "Training 47     - 100    -          0.0062 -           0.9980    | 0.9980   - 18.5832\n",
      "\u001b[1;4mValidati 47     - 100    -          0.5839 -           0.9104    | 0.9137   - 1.1721\u001b[0m\n",
      "Training 48     - 100    -          0.0049 -           0.9984    | 0.9984   - 18.5387\n",
      "\u001b[1;4mValidati 48     - 100    -          0.5755 -           0.9134    | 0.9165   - 1.2355\u001b[0m\n",
      "Training 49     - 100    -          0.0129 -           0.9962    | 0.9963   - 19.2480\n",
      "\u001b[1;4mValidati 49     - 100    -          0.5846 -           0.9061    | 0.9102   - 1.2107\u001b[0m\n",
      "Training 50     - 100    -          0.0087 -           0.9975    | 0.9975   - 18.8286\n",
      "\u001b[1;4mValidati 50     - 100    -          0.5701 -           0.9119    | 0.9152   - 1.1743\u001b[0m\n",
      "Training 51     - 100    -          0.0041 -           0.9989    | 0.9989   - 18.5954\n",
      "\u001b[1;4mValidati 51     - 100    -          0.5781 -           0.9146    | 0.9179   - 1.1737\u001b[0m\n",
      "Training 52     - 100    -          0.0091 -           0.9971    | 0.9972   - 18.5907\n",
      "\u001b[1;4mValidati 52     - 100    -          0.5857 -           0.9111    | 0.9147   - 1.1763\u001b[0m\n",
      "Training 53     - 100    -          0.0099 -           0.9970    | 0.9970   - 18.6513\n",
      "\u001b[1;4mValidati 53     - 100    -          0.5577 -           0.9143    | 0.9178   - 1.2442\u001b[0m\n",
      "Training 54     - 100    -          0.0061 -           0.9981    | 0.9981   - 19.3565\n",
      "\u001b[1;4mValidati 54     - 100    -          0.5981 -           0.9124    | 0.9156   - 1.2026\u001b[0m\n",
      "Training 55     - 100    -          0.0041 -           0.9988    | 0.9988   - 19.4511\n",
      "\u001b[1;4mValidati 55     - 100    -          0.5574 -           0.9164    | 0.9185   - 1.2200\u001b[0m\n",
      "Training 56     - 100    -          0.0077 -           0.9975    | 0.9975   - 19.6483\n",
      "\u001b[1;4mValidati 56     - 100    -          0.5940 -           0.9110    | 0.9139   - 1.2080\u001b[0m\n",
      "Training 57     - 100    -          0.0062 -           0.9983    | 0.9983   - 19.9389\n",
      "\u001b[1;4mValidati 57     - 100    -          0.5662 -           0.9152    | 0.9188   - 1.2207\u001b[0m\n",
      "Training 58     - 100    -          0.0069 -           0.9980    | 0.9981   - 19.3850\n",
      "\u001b[1;4mValidati 58     - 100    -          0.6078 -           0.9076    | 0.9114   - 1.1925\u001b[0m\n",
      "Training 59     - 100    -          0.0085 -           0.9973    | 0.9974   - 18.6082\n",
      "\u001b[1;4mValidati 59     - 100    -          0.5790 -           0.9119    | 0.9154   - 1.1625\u001b[0m\n",
      "Training 60     - 100    -          0.0051 -           0.9985    | 0.9984   - 18.6191\n",
      "\u001b[1;4mValidati 60     - 100    -          0.5664 -           0.9152    | 0.9182   - 1.1589\u001b[0m\n",
      "Training 61     - 100    -          0.0035 -           0.9990    | 0.9990   - 18.6090\n",
      "\u001b[1;4mValidati 61     - 100    -          0.5807 -           0.9133    | 0.9167   - 1.1363\u001b[0m\n",
      "Training 62     - 100    -          0.0029 -           0.9991    | 0.9991   - 18.6413\n",
      "\u001b[1;4mValidati 62     - 100    -          0.5963 -           0.9130    | 0.9157   - 1.1631\u001b[0m\n",
      "Training 63     - 100    -          0.0078 -           0.9976    | 0.9976   - 18.4522\n",
      "\u001b[1;4mValidati 63     - 100    -          0.6154 -           0.9078    | 0.9112   - 1.2084\u001b[0m\n",
      "Training 64     - 100    -          0.0087 -           0.9973    | 0.9974   - 19.6076\n",
      "\u001b[1;4mValidati 64     - 100    -          0.5806 -           0.9123    | 0.9158   - 1.2082\u001b[0m\n",
      "Training 65     - 100    -          0.0038 -           0.9988    | 0.9988   - 18.6302\n",
      "\u001b[1;4mValidati 65     - 100    -          0.5579 -           0.9177    | 0.9210   - 1.1903\u001b[0m\n",
      "Training 66     - 100    -          0.0008 -           0.9998    | 0.9998   - 19.3234\n",
      "\u001b[1;4mValidati 66     - 100    -          0.5471 -           0.9213    | 0.9243   - 1.2074\u001b[0m\n",
      "Training 67     - 100    -          0.0009 -           0.9999    | 0.9999   - 18.5415\n",
      "\u001b[1;4mValidati 67     - 100    -          0.5562 -           0.9219    | 0.9247   - 1.1149\u001b[0m\n",
      "Training 68     - 100    -          0.0002 -           1.0000    | 1.0000   - 18.5950\n",
      "\u001b[1;4mValidati 68     - 100    -          0.5606 -           0.9227    | 0.9252   - 1.1581\u001b[0m\n",
      "Training 69     - 100    -          0.0002 -           1.0000    | 1.0000   - 19.3383\n",
      "\u001b[1;4mValidati 69     - 100    -          0.5599 -           0.9228    | 0.9249   - 1.2349\u001b[0m\n",
      "Training 70     - 100    -          0.0001 -           1.0000    | 1.0000   - 19.2263\n",
      "\u001b[1;4mValidati 70     - 100    -          0.5673 -           0.9237    | 0.9259   - 1.1916\u001b[0m\n",
      "Training 71     - 100    -          0.0152 -           0.9956    | 0.9956   - 18.5894\n",
      "\u001b[1;4mValidati 71     - 100    -          0.5627 -           0.9071    | 0.9109   - 1.1658\u001b[0m\n",
      "Training 72     - 100    -          0.0123 -           0.9962    | 0.9962   - 18.6010\n",
      "\u001b[1;4mValidati 72     - 100    -          0.5412 -           0.9156    | 0.9187   - 1.1564\u001b[0m\n",
      "Training 73     - 100    -          0.0022 -           0.9994    | 0.9994   - 18.8029\n",
      "\u001b[1;4mValidati 73     - 100    -          0.5324 -           0.9184    | 0.9217   - 1.1885\u001b[0m\n",
      "Training 74     - 100    -          0.0005 -           0.9999    | 0.9999   - 18.6239\n",
      "\u001b[1;4mValidati 74     - 100    -          0.5377 -           0.9216    | 0.9246   - 1.1582\u001b[0m\n",
      "Training 75     - 100    -          0.0001 -           1.0000    | 1.0000   - 18.5641\n",
      "\u001b[1;4mValidati 75     - 100    -          0.5391 -           0.9223    | 0.9251   - 1.1679\u001b[0m\n",
      "Training 76     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5621\n",
      "\u001b[1;4mValidati 76     - 100    -          0.5506 -           0.9234    | 0.9257   - 1.1594\u001b[0m\n",
      "Training 77     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5436\n",
      "\u001b[1;4mValidati 77     - 100    -          0.5462 -           0.9242    | 0.9264   - 1.1608\u001b[0m\n",
      "Training 78     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4681\n",
      "\u001b[1;4mValidati 78     - 100    -          0.5606 -           0.9234    | 0.9251   - 1.1443\u001b[0m\n",
      "Training 79     - 100    -          0.0000 -           1.0000    | 1.0000   - 19.4387\n",
      "\u001b[1;4mValidati 79     - 100    -          0.5577 -           0.9243    | 0.9264   - 1.2213\u001b[0m\n",
      "Training 80     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.9909\n",
      "\u001b[1;4mValidati 80     - 100    -          0.5728 -           0.9230    | 0.9254   - 1.1812\u001b[0m\n",
      "Training 81     - 100    -          0.0000 -           1.0000    | 1.0000   - 19.0936\n",
      "\u001b[1;4mValidati 81     - 100    -          0.5636 -           0.9238    | 0.9261   - 1.1856\u001b[0m\n",
      "Training 82     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.3552\n",
      "\u001b[1;4mValidati 82     - 100    -          0.5725 -           0.9243    | 0.9262   - 1.1075\u001b[0m\n",
      "Training 83     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4748\n",
      "\u001b[1;4mValidati 83     - 100    -          0.5771 -           0.9245    | 0.9264   - 1.1192\u001b[0m\n",
      "Training 84     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4859\n",
      "\u001b[1;4mValidati 84     - 100    -          0.5851 -           0.9250    | 0.9271   - 1.1567\u001b[0m\n",
      "Training 85     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.8931\n",
      "\u001b[1;4mValidati 85     - 100    -          0.5811 -           0.9254    | 0.9270   - 1.1375\u001b[0m\n",
      "Training 86     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.3830\n",
      "\u001b[1;4mValidati 86     - 100    -          0.5848 -           0.9244    | 0.9265   - 1.1159\u001b[0m\n",
      "Training 87     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5215\n",
      "\u001b[1;4mValidati 87     - 100    -          0.6023 -           0.9252    | 0.9268   - 1.1586\u001b[0m\n",
      "Training 88     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.3950\n",
      "\u001b[1;4mValidati 88     - 100    -          0.5997 -           0.9249    | 0.9266   - 1.1094\u001b[0m\n",
      "Training 89     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4158\n",
      "\u001b[1;4mValidati 89     - 100    -          0.6077 -           0.9249    | 0.9266   - 1.1150\u001b[0m\n",
      "Training 90     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5655\n",
      "\u001b[1;4mValidati 90     - 100    -          0.6176 -           0.9243    | 0.9260   - 1.1578\u001b[0m\n",
      "Training 91     - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3406\n",
      "\u001b[1;4mValidati 91     - 100    -          0.6157 -           0.9256    | 0.9271   - 1.2418\u001b[0m\n",
      "Training 92     - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1808\n",
      "\u001b[1;4mValidati 92     - 100    -          0.6288 -           0.9246    | 0.9256   - 1.2289\u001b[0m\n",
      "Training 93     - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3420\n",
      "\u001b[1;4mValidati 93     - 100    -          0.6215 -           0.9258    | 0.9274   - 1.2360\u001b[0m\n",
      "Training 94     - 100    -          0.0000 -           1.0000    | 1.0000   - 19.2312\n",
      "\u001b[1;4mValidati 94     - 100    -          0.6271 -           0.9254    | 0.9270   - 1.1898\u001b[0m\n",
      "Training 95     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5583\n",
      "\u001b[1;4mValidati 95     - 100    -          0.6300 -           0.9264    | 0.9281   - 1.1571\u001b[0m\n",
      "Training 96     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5539\n",
      "\u001b[1;4mValidati 96     - 100    -          0.6368 -           0.9257    | 0.9274   - 1.1614\u001b[0m\n",
      "Training 97     - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5506\n",
      "\u001b[1;4mValidati 97     - 100    -          0.6383 -           0.9256    | 0.9272   - 1.1705\u001b[0m\n",
      "Training 98     - 100    -          0.0228 -           0.9936    | 0.9937   - 18.4688\n",
      "\u001b[1;4mValidati 98     - 100    -          0.5666 -           0.9101    | 0.9133   - 1.1989\u001b[0m\n",
      "Training 99     - 100    -          0.0037 -           0.9989    | 0.9989   - 19.1685\n",
      "\u001b[1;4mValidati 99     - 100    -          0.5553 -           0.9167    | 0.9190   - 1.1842\u001b[0m\n",
      "Training 100    - 100    -          0.0007 -           0.9999    | 0.9999   - 18.3907\n",
      "\u001b[1;4mValidati 100    - 100    -          0.5500 -           0.9192    | 0.9220   - 1.1342\u001b[0m\n",
      "Training 101    - 100    -          0.0001 -           1.0000    | 1.0000   - 18.5303\n",
      "\u001b[1;4mValidati 101    - 100    -          0.5491 -           0.9210    | 0.9235   - 1.1553\u001b[0m\n",
      "Training 102    - 100    -          0.0001 -           1.0000    | 1.0000   - 18.4441\n",
      "\u001b[1;4mValidati 102    - 100    -          0.5597 -           0.9209    | 0.9237   - 1.1152\u001b[0m\n",
      "Training 103    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6039\n",
      "\u001b[1;4mValidati 103    - 100    -          0.5648 -           0.9218    | 0.9243   - 1.1589\u001b[0m\n",
      "Training 104    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5610\n",
      "\u001b[1;4mValidati 104    - 100    -          0.5729 -           0.9208    | 0.9229   - 1.1718\u001b[0m\n",
      "Training 105    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3916\n",
      "\u001b[1;4mValidati 105    - 100    -          0.5751 -           0.9214    | 0.9238   - 1.1922\u001b[0m\n",
      "Training 106    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1844\n",
      "\u001b[1;4mValidati 106    - 100    -          0.5760 -           0.9225    | 0.9248   - 1.2245\u001b[0m\n",
      "Training 107    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.2109\n",
      "\u001b[1;4mValidati 107    - 100    -          0.5862 -           0.9222    | 0.9247   - 1.2226\u001b[0m\n",
      "Training 108    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.4248\n",
      "\u001b[1;4mValidati 108    - 100    -          0.5974 -           0.9218    | 0.9238   - 1.1915\u001b[0m\n",
      "Training 109    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3993\n",
      "\u001b[1;4mValidati 109    - 100    -          0.5980 -           0.9216    | 0.9235   - 1.1971\u001b[0m\n",
      "Training 110    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.7738\n",
      "\u001b[1;4mValidati 110    - 100    -          0.6010 -           0.9223    | 0.9241   - 1.1922\u001b[0m\n",
      "Training 111    - 100    -          0.0115 -           0.9966    | 0.9966   - 18.5701\n",
      "\u001b[1;4mValidati 111    - 100    -          0.5765 -           0.9133    | 0.9164   - 1.1557\u001b[0m\n",
      "Training 112    - 100    -          0.0035 -           0.9990    | 0.9989   - 18.6571\n",
      "\u001b[1;4mValidati 112    - 100    -          0.5749 -           0.9177    | 0.9208   - 1.1852\u001b[0m\n",
      "Training 113    - 100    -          0.0003 -           1.0000    | 0.9999   - 18.5990\n",
      "\u001b[1;4mValidati 113    - 100    -          0.5713 -           0.9187    | 0.9211   - 1.1597\u001b[0m\n",
      "Training 114    - 100    -          0.0001 -           1.0000    | 1.0000   - 18.6499\n",
      "\u001b[1;4mValidati 114    - 100    -          0.5663 -           0.9212    | 0.9236   - 1.1697\u001b[0m\n",
      "Training 115    - 100    -          0.0000 -           1.0000    | 1.0000   - 20.2215\n",
      "\u001b[1;4mValidati 115    - 100    -          0.5825 -           0.9214    | 0.9233   - 1.2514\u001b[0m\n",
      "Training 116    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3194\n",
      "\u001b[1;4mValidati 116    - 100    -          0.5876 -           0.9210    | 0.9230   - 1.1897\u001b[0m\n",
      "Training 117    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6263\n",
      "\u001b[1;4mValidati 117    - 100    -          0.5836 -           0.9219    | 0.9238   - 1.1612\u001b[0m\n",
      "Training 118    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6083\n",
      "\u001b[1;4mValidati 118    - 100    -          0.5896 -           0.9212    | 0.9231   - 1.1562\u001b[0m\n",
      "Training 119    - 100    -          0.0000 -           1.0000    | 1.0000   - 20.0479\n",
      "\u001b[1;4mValidati 119    - 100    -          0.6027 -           0.9218    | 0.9236   - 1.2560\u001b[0m\n",
      "Training 120    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.0981\n",
      "\u001b[1;4mValidati 120    - 100    -          0.6005 -           0.9224    | 0.9239   - 1.1401\u001b[0m\n",
      "Training 121    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4147\n",
      "\u001b[1;4mValidati 121    - 100    -          0.6146 -           0.9216    | 0.9232   - 1.1157\u001b[0m\n",
      "Training 122    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6355\n",
      "\u001b[1;4mValidati 122    - 100    -          0.6072 -           0.9226    | 0.9245   - 1.1683\u001b[0m\n",
      "Training 123    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.9754\n",
      "\u001b[1;4mValidati 123    - 100    -          0.6037 -           0.9223    | 0.9239   - 1.1419\u001b[0m\n",
      "Training 124    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3496\n",
      "\u001b[1;4mValidati 124    - 100    -          0.6176 -           0.9219    | 0.9233   - 1.1963\u001b[0m\n",
      "Training 125    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6084\n",
      "\u001b[1;4mValidati 125    - 100    -          0.6277 -           0.9220    | 0.9234   - 1.1754\u001b[0m\n",
      "Training 126    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5926\n",
      "\u001b[1;4mValidati 126    - 100    -          0.6294 -           0.9226    | 0.9239   - 1.1548\u001b[0m\n",
      "Training 127    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.7264\n",
      "\u001b[1;4mValidati 127    - 100    -          0.6298 -           0.9221    | 0.9235   - 1.1430\u001b[0m\n",
      "Training 128    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6080\n",
      "\u001b[1;4mValidati 128    - 100    -          0.6243 -           0.9233    | 0.9246   - 1.1658\u001b[0m\n",
      "Training 129    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4110\n",
      "\u001b[1;4mValidati 129    - 100    -          0.6337 -           0.9234    | 0.9247   - 1.1118\u001b[0m\n",
      "Training 130    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5920\n",
      "\u001b[1;4mValidati 130    - 100    -          0.6397 -           0.9229    | 0.9246   - 1.1561\u001b[0m\n",
      "Training 131    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4652\n",
      "\u001b[1;4mValidati 131    - 100    -          0.6475 -           0.9230    | 0.9244   - 1.1285\u001b[0m\n",
      "Training 132    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6302\n",
      "\u001b[1;4mValidati 132    - 100    -          0.6486 -           0.9228    | 0.9244   - 1.1515\u001b[0m\n",
      "Training 133    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6007\n",
      "\u001b[1;4mValidati 133    - 100    -          0.6666 -           0.9232    | 0.9246   - 1.1677\u001b[0m\n",
      "Training 134    - 100    -          0.0044 -           0.9989    | 0.9988   - 18.4383\n",
      "\u001b[1;4mValidati 134    - 100    -          0.6620 -           0.9161    | 0.9184   - 1.1108\u001b[0m\n",
      "Training 135    - 100    -          0.0008 -           0.9998    | 0.9998   - 18.4572\n",
      "\u001b[1;4mValidati 135    - 100    -          0.6022 -           0.9199    | 0.9226   - 1.1098\u001b[0m\n",
      "Training 136    - 100    -          0.0002 -           1.0000    | 1.0000   - 18.6452\n",
      "\u001b[1;4mValidati 136    - 100    -          0.6078 -           0.9204    | 0.9227   - 1.1599\u001b[0m\n",
      "Training 137    - 100    -          0.0001 -           1.0000    | 1.0000   - 18.5970\n",
      "\u001b[1;4mValidati 137    - 100    -          0.6137 -           0.9197    | 0.9220   - 1.1552\u001b[0m\n",
      "Training 138    - 100    -          0.0001 -           1.0000    | 1.0000   - 18.5935\n",
      "\u001b[1;4mValidati 138    - 100    -          0.6128 -           0.9208    | 0.9235   - 1.1604\u001b[0m\n",
      "Training 139    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5174\n",
      "\u001b[1;4mValidati 139    - 100    -          0.6184 -           0.9211    | 0.9234   - 1.1793\u001b[0m\n",
      "Training 140    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3926\n",
      "\u001b[1;4mValidati 140    - 100    -          0.6114 -           0.9214    | 0.9235   - 1.2135\u001b[0m\n",
      "Training 141    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.0702\n",
      "\u001b[1;4mValidati 141    - 100    -          0.6247 -           0.9211    | 0.9229   - 1.2251\u001b[0m\n",
      "Training 142    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.0567\n",
      "\u001b[1;4mValidati 142    - 100    -          0.6176 -           0.9218    | 0.9237   - 1.1531\u001b[0m\n",
      "Training 143    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.3648\n",
      "\u001b[1;4mValidati 143    - 100    -          0.6286 -           0.9214    | 0.9230   - 1.1094\u001b[0m\n",
      "Training 144    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5372\n",
      "\u001b[1;4mValidati 144    - 100    -          0.6258 -           0.9219    | 0.9238   - 1.1499\u001b[0m\n",
      "Training 145    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.9068\n",
      "\u001b[1;4mValidati 145    - 100    -          0.6391 -           0.9214    | 0.9233   - 1.1404\u001b[0m\n",
      "Training 146    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.0107\n",
      "\u001b[1;4mValidati 146    - 100    -          0.6239 -           0.9225    | 0.9244   - 1.1882\u001b[0m\n",
      "Training 147    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5755\n",
      "\u001b[1;4mValidati 147    - 100    -          0.6348 -           0.9225    | 0.9245   - 1.1575\u001b[0m\n",
      "Training 148    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5827\n",
      "\u001b[1;4mValidati 148    - 100    -          0.6362 -           0.9230    | 0.9249   - 1.1778\u001b[0m\n",
      "Training 149    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.2809\n",
      "\u001b[1;4mValidati 149    - 100    -          0.6363 -           0.9231    | 0.9251   - 1.2086\u001b[0m\n",
      "Training 150    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6710\n",
      "\u001b[1;4mValidati 150    - 100    -          0.6444 -           0.9227    | 0.9244   - 1.1726\u001b[0m\n",
      "Training 151    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6086\n",
      "\u001b[1;4mValidati 151    - 100    -          0.6473 -           0.9232    | 0.9248   - 1.1964\u001b[0m\n",
      "Training 152    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3280\n",
      "\u001b[1;4mValidati 152    - 100    -          0.6468 -           0.9231    | 0.9249   - 1.1864\u001b[0m\n",
      "Training 153    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5923\n",
      "\u001b[1;4mValidati 153    - 100    -          0.6572 -           0.9234    | 0.9249   - 1.1557\u001b[0m\n",
      "Training 154    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.2148\n",
      "\u001b[1;4mValidati 154    - 100    -          0.6621 -           0.9240    | 0.9256   - 1.1869\u001b[0m\n",
      "Training 155    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3108\n",
      "\u001b[1;4mValidati 155    - 100    -          0.6646 -           0.9238    | 0.9253   - 1.2580\u001b[0m\n",
      "Training 156    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.4827\n",
      "\u001b[1;4mValidati 156    - 100    -          0.6638 -           0.9232    | 0.9249   - 1.1864\u001b[0m\n",
      "Training 157    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.9468\n",
      "\u001b[1;4mValidati 157    - 100    -          0.6583 -           0.9232    | 0.9250   - 1.2445\u001b[0m\n",
      "Training 158    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.2082\n",
      "\u001b[1;4mValidati 158    - 100    -          0.6724 -           0.9234    | 0.9249   - 1.2168\u001b[0m\n",
      "Training 159    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3059\n",
      "\u001b[1;4mValidati 159    - 100    -          0.6812 -           0.9241    | 0.9257   - 1.1891\u001b[0m\n",
      "Training 160    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.4541\n",
      "\u001b[1;4mValidati 160    - 100    -          0.6760 -           0.9244    | 0.9261   - 1.1927\u001b[0m\n",
      "Training 161    - 100    -          0.0000 -           1.0000    | 1.0000   - 20.2979\n",
      "\u001b[1;4mValidati 161    - 100    -          0.6723 -           0.9240    | 0.9259   - 1.2420\u001b[0m\n",
      "Training 162    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1786\n",
      "\u001b[1;4mValidati 162    - 100    -          0.6696 -           0.9247    | 0.9264   - 1.2164\u001b[0m\n",
      "Training 163    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1437\n",
      "\u001b[1;4mValidati 163    - 100    -          0.6867 -           0.9239    | 0.9255   - 1.1799\u001b[0m\n",
      "Training 164    - 100    -          0.0003 -           1.0000    | 1.0000   - 19.1997\n",
      "\u001b[1;4mValidati 164    - 100    -          0.6961 -           0.9215    | 0.9229   - 1.1467\u001b[0m\n",
      "Training 165    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5531\n",
      "\u001b[1;4mValidati 165    - 100    -          0.7045 -           0.9226    | 0.9237   - 1.1813\u001b[0m\n",
      "Training 166    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6393\n",
      "\u001b[1;4mValidati 166    - 100    -          0.7115 -           0.9221    | 0.9235   - 1.1858\u001b[0m\n",
      "Training 167    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6470\n",
      "\u001b[1;4mValidati 167    - 100    -          0.6922 -           0.9228    | 0.9244   - 1.1602\u001b[0m\n",
      "Training 168    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6411\n",
      "\u001b[1;4mValidati 168    - 100    -          0.7119 -           0.9220    | 0.9233   - 1.1606\u001b[0m\n",
      "Training 169    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5561\n",
      "\u001b[1;4mValidati 169    - 100    -          0.6973 -           0.9227    | 0.9241   - 1.1739\u001b[0m\n",
      "Training 170    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5132\n",
      "\u001b[1;4mValidati 170    - 100    -          0.6997 -           0.9234    | 0.9250   - 1.1227\u001b[0m\n",
      "Training 171    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6045\n",
      "\u001b[1;4mValidati 171    - 100    -          0.6959 -           0.9237    | 0.9250   - 1.1549\u001b[0m\n",
      "Training 172    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.4190\n",
      "\u001b[1;4mValidati 172    - 100    -          0.6999 -           0.9238    | 0.9249   - 1.1903\u001b[0m\n",
      "Training 173    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5641\n",
      "\u001b[1;4mValidati 173    - 100    -          0.7011 -           0.9228    | 0.9244   - 1.1546\u001b[0m\n",
      "Training 174    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.8469\n",
      "\u001b[1;4mValidati 174    - 100    -          0.7051 -           0.9232    | 0.9244   - 1.1928\u001b[0m\n",
      "Training 175    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.9468\n",
      "\u001b[1;4mValidati 175    - 100    -          0.7061 -           0.9233    | 0.9246   - 1.1869\u001b[0m\n",
      "Training 176    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1108\n",
      "\u001b[1;4mValidati 176    - 100    -          0.7170 -           0.9233    | 0.9246   - 1.2178\u001b[0m\n",
      "Training 177    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1632\n",
      "\u001b[1;4mValidati 177    - 100    -          0.7113 -           0.9232    | 0.9244   - 1.2366\u001b[0m\n",
      "Training 178    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.2867\n",
      "\u001b[1;4mValidati 178    - 100    -          0.7143 -           0.9231    | 0.9242   - 1.1921\u001b[0m\n",
      "Training 179    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.4667\n",
      "\u001b[1;4mValidati 179    - 100    -          0.7131 -           0.9232    | 0.9243   - 1.1873\u001b[0m\n",
      "Training 180    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5771\n",
      "\u001b[1;4mValidati 180    - 100    -          0.7066 -           0.9245    | 0.9256   - 1.1559\u001b[0m\n",
      "Training 181    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6286\n",
      "\u001b[1;4mValidati 181    - 100    -          0.7135 -           0.9226    | 0.9242   - 1.2098\u001b[0m\n",
      "Training 182    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1227\n",
      "\u001b[1;4mValidati 182    - 100    -          0.7134 -           0.9231    | 0.9244   - 1.2164\u001b[0m\n",
      "Training 183    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.0892\n",
      "\u001b[1;4mValidati 183    - 100    -          0.7130 -           0.9233    | 0.9246   - 1.2170\u001b[0m\n",
      "Training 184    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1437\n",
      "\u001b[1;4mValidati 184    - 100    -          0.7075 -           0.9239    | 0.9251   - 1.1877\u001b[0m\n",
      "Training 185    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5834\n",
      "\u001b[1;4mValidati 185    - 100    -          0.7218 -           0.9242    | 0.9256   - 1.1566\u001b[0m\n",
      "Training 186    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.6583\n",
      "\u001b[1;4mValidati 186    - 100    -          0.7172 -           0.9233    | 0.9245   - 1.1730\u001b[0m\n",
      "Training 187    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5941\n",
      "\u001b[1;4mValidati 187    - 100    -          0.7315 -           0.9231    | 0.9244   - 1.1769\u001b[0m\n",
      "Training 188    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5404\n",
      "\u001b[1;4mValidati 188    - 100    -          0.7251 -           0.9238    | 0.9248   - 1.1640\u001b[0m\n",
      "Training 189    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3944\n",
      "\u001b[1;4mValidati 189    - 100    -          0.7396 -           0.9233    | 0.9246   - 1.1841\u001b[0m\n",
      "Training 190    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.8227\n",
      "\u001b[1;4mValidati 190    - 100    -          0.7285 -           0.9226    | 0.9239   - 1.2616\u001b[0m\n",
      "Training 191    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1061\n",
      "\u001b[1;4mValidati 191    - 100    -          0.7288 -           0.9241    | 0.9253   - 1.2276\u001b[0m\n",
      "Training 192    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3908\n",
      "\u001b[1;4mValidati 192    - 100    -          0.7241 -           0.9241    | 0.9253   - 1.1975\u001b[0m\n",
      "Training 193    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5606\n",
      "\u001b[1;4mValidati 193    - 100    -          0.7287 -           0.9236    | 0.9246   - 1.1580\u001b[0m\n",
      "Training 194    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.1879\n",
      "\u001b[1;4mValidati 194    - 100    -          0.7250 -           0.9242    | 0.9256   - 1.1848\u001b[0m\n",
      "Training 195    - 100    -          0.0000 -           1.0000    | 1.0000   - 19.3743\n",
      "\u001b[1;4mValidati 195    - 100    -          0.7351 -           0.9233    | 0.9246   - 1.1953\u001b[0m\n",
      "Training 196    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5631\n",
      "\u001b[1;4mValidati 196    - 100    -          0.7382 -           0.9235    | 0.9249   - 1.1950\u001b[0m\n",
      "Training 197    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5441\n",
      "\u001b[1;4mValidati 197    - 100    -          0.7291 -           0.9240    | 0.9253   - 1.1636\u001b[0m\n",
      "Training 198    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5182\n",
      "\u001b[1;4mValidati 198    - 100    -          0.7199 -           0.9242    | 0.9254   - 1.1162\u001b[0m\n",
      "Training 199    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.5629\n",
      "\u001b[1;4mValidati 199    - 100    -          0.7245 -           0.9247    | 0.9259   - 1.1575\u001b[0m\n",
      "Training 200    - 100    -          0.0000 -           1.0000    | 1.0000   - 18.3743\n",
      "\u001b[1;4mValidati 200    - 100    -          0.7173 -           0.9244    | 0.9259   - 1.1137\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .llll||=||llll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
