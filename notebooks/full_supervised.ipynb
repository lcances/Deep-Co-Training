{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "from DCT.util.utils import reset_seed, get_datetime\n",
    "from DCT.util.model_loader import get_model_from_name\n",
    "from DCT.util.dataset_loader import load_dataset\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"cifar10\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.08, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=128, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"UrbanSound8k parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=128, checkpoint_path='supervised', checkpoint_root='../model_save/', dataset='cifar10', dataset_root='../datasets', from_config='', learning_rate=0.1, model='wideresnet28_2', nb_epoch=200, resume=False, seed=1234, supervised_ratio=0.08, tensorboard_path='supervised', tensorboard_root='../tensorboard/', tensorboard_sufix='', train_folds=[1, 2, 3, 4, 5, 6, 7, 8, 9], val_folds=[10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "we pre-processed the images using ZCA and augmented the dataset using horizontal flips and random translations. The translations\n",
    "were drawn from [−2, 2] pixels,\n",
    "\"\"\"\n",
    "extra_train_transforms = [\n",
    "    transforms.Pad(4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "]\n",
    "\n",
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"supervised\",\n",
    "    \n",
    "    extra_train_transform = extra_train_transforms,\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_func = get_model_from_name(args.model)\n",
    "model = model_func()\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "                                          Kernel Shape      Output Shape  \\\n",
      "Layer                                                                      \n",
      "0_conv1                                  [3, 32, 3, 3]  [64, 32, 32, 32]   \n",
      "1_bn1                                             [32]  [64, 32, 32, 32]   \n",
      "2_relu                                               -  [64, 32, 32, 32]   \n",
      "3_maxpool                                            -  [64, 32, 16, 16]   \n",
      "4_layer1.0.Conv2d_conv1                 [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "5_layer1.0.BatchNorm2d_bn1                        [32]  [64, 32, 16, 16]   \n",
      "6_layer1.0.ReLU_relu                                 -  [64, 32, 16, 16]   \n",
      "7_layer1.0.Conv2d_conv2                 [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "8_layer1.0.BatchNorm2d_bn2                        [32]  [64, 32, 16, 16]   \n",
      "9_layer1.0.ReLU_relu                                 -  [64, 32, 16, 16]   \n",
      "10_layer1.1.Conv2d_conv1                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "11_layer1.1.BatchNorm2d_bn1                       [32]  [64, 32, 16, 16]   \n",
      "12_layer1.1.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "13_layer1.1.Conv2d_conv2                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "14_layer1.1.BatchNorm2d_bn2                       [32]  [64, 32, 16, 16]   \n",
      "15_layer1.1.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "16_layer1.2.Conv2d_conv1                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "17_layer1.2.BatchNorm2d_bn1                       [32]  [64, 32, 16, 16]   \n",
      "18_layer1.2.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "19_layer1.2.Conv2d_conv2                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "20_layer1.2.BatchNorm2d_bn2                       [32]  [64, 32, 16, 16]   \n",
      "21_layer1.2.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "22_layer1.3.Conv2d_conv1                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "23_layer1.3.BatchNorm2d_bn1                       [32]  [64, 32, 16, 16]   \n",
      "24_layer1.3.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "25_layer1.3.Conv2d_conv2                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "26_layer1.3.BatchNorm2d_bn2                       [32]  [64, 32, 16, 16]   \n",
      "27_layer1.3.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "28_layer2.0.Conv2d_conv1                [32, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "29_layer2.0.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "30_layer2.0.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "31_layer2.0.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "32_layer2.0.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "33_layer2.0.downsample.Conv2d_0         [32, 64, 1, 1]    [64, 64, 8, 8]   \n",
      "34_layer2.0.downsample.BatchNorm2d_1              [64]    [64, 64, 8, 8]   \n",
      "35_layer2.0.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "36_layer2.1.Conv2d_conv1                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "37_layer2.1.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "38_layer2.1.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "39_layer2.1.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "40_layer2.1.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "41_layer2.1.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "42_layer2.2.Conv2d_conv1                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "43_layer2.2.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "44_layer2.2.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "45_layer2.2.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "46_layer2.2.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "47_layer2.2.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "48_layer2.3.Conv2d_conv1                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "49_layer2.3.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "50_layer2.3.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "51_layer2.3.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "52_layer2.3.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "53_layer2.3.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "54_layer3.0.Conv2d_conv1               [64, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "55_layer3.0.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "56_layer3.0.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "57_layer3.0.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "58_layer3.0.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "59_layer3.0.downsample.Conv2d_0        [64, 128, 1, 1]   [64, 128, 4, 4]   \n",
      "60_layer3.0.downsample.BatchNorm2d_1             [128]   [64, 128, 4, 4]   \n",
      "61_layer3.0.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "62_layer3.1.Conv2d_conv1              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "63_layer3.1.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "64_layer3.1.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "65_layer3.1.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "66_layer3.1.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "67_layer3.1.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "68_layer3.2.Conv2d_conv1              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "69_layer3.2.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "70_layer3.2.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "71_layer3.2.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "72_layer3.2.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "73_layer3.2.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "74_layer3.3.Conv2d_conv1              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "75_layer3.3.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "76_layer3.3.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "77_layer3.3.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "78_layer3.3.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "79_layer3.3.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "80_avgpool                                           -   [64, 128, 1, 1]   \n",
      "81_fc                                        [128, 10]          [64, 10]   \n",
      "\n",
      "                                        Params  Mult-Adds  \n",
      "Layer                                                      \n",
      "0_conv1                                  864.0   884.736k  \n",
      "1_bn1                                     64.0       32.0  \n",
      "2_relu                                       -          -  \n",
      "3_maxpool                                    -          -  \n",
      "4_layer1.0.Conv2d_conv1                 9.216k  2.359296M  \n",
      "5_layer1.0.BatchNorm2d_bn1                64.0       32.0  \n",
      "6_layer1.0.ReLU_relu                         -          -  \n",
      "7_layer1.0.Conv2d_conv2                 9.216k  2.359296M  \n",
      "8_layer1.0.BatchNorm2d_bn2                64.0       32.0  \n",
      "9_layer1.0.ReLU_relu                         -          -  \n",
      "10_layer1.1.Conv2d_conv1                9.216k  2.359296M  \n",
      "11_layer1.1.BatchNorm2d_bn1               64.0       32.0  \n",
      "12_layer1.1.ReLU_relu                        -          -  \n",
      "13_layer1.1.Conv2d_conv2                9.216k  2.359296M  \n",
      "14_layer1.1.BatchNorm2d_bn2               64.0       32.0  \n",
      "15_layer1.1.ReLU_relu                        -          -  \n",
      "16_layer1.2.Conv2d_conv1                9.216k  2.359296M  \n",
      "17_layer1.2.BatchNorm2d_bn1               64.0       32.0  \n",
      "18_layer1.2.ReLU_relu                        -          -  \n",
      "19_layer1.2.Conv2d_conv2                9.216k  2.359296M  \n",
      "20_layer1.2.BatchNorm2d_bn2               64.0       32.0  \n",
      "21_layer1.2.ReLU_relu                        -          -  \n",
      "22_layer1.3.Conv2d_conv1                9.216k  2.359296M  \n",
      "23_layer1.3.BatchNorm2d_bn1               64.0       32.0  \n",
      "24_layer1.3.ReLU_relu                        -          -  \n",
      "25_layer1.3.Conv2d_conv2                9.216k  2.359296M  \n",
      "26_layer1.3.BatchNorm2d_bn2               64.0       32.0  \n",
      "27_layer1.3.ReLU_relu                        -          -  \n",
      "28_layer2.0.Conv2d_conv1               18.432k  1.179648M  \n",
      "29_layer2.0.BatchNorm2d_bn1              128.0       64.0  \n",
      "30_layer2.0.ReLU_relu                        -          -  \n",
      "31_layer2.0.Conv2d_conv2               36.864k  2.359296M  \n",
      "32_layer2.0.BatchNorm2d_bn2              128.0       64.0  \n",
      "33_layer2.0.downsample.Conv2d_0         2.048k   131.072k  \n",
      "34_layer2.0.downsample.BatchNorm2d_1     128.0       64.0  \n",
      "35_layer2.0.ReLU_relu                        -          -  \n",
      "36_layer2.1.Conv2d_conv1               36.864k  2.359296M  \n",
      "37_layer2.1.BatchNorm2d_bn1              128.0       64.0  \n",
      "38_layer2.1.ReLU_relu                        -          -  \n",
      "39_layer2.1.Conv2d_conv2               36.864k  2.359296M  \n",
      "40_layer2.1.BatchNorm2d_bn2              128.0       64.0  \n",
      "41_layer2.1.ReLU_relu                        -          -  \n",
      "42_layer2.2.Conv2d_conv1               36.864k  2.359296M  \n",
      "43_layer2.2.BatchNorm2d_bn1              128.0       64.0  \n",
      "44_layer2.2.ReLU_relu                        -          -  \n",
      "45_layer2.2.Conv2d_conv2               36.864k  2.359296M  \n",
      "46_layer2.2.BatchNorm2d_bn2              128.0       64.0  \n",
      "47_layer2.2.ReLU_relu                        -          -  \n",
      "48_layer2.3.Conv2d_conv1               36.864k  2.359296M  \n",
      "49_layer2.3.BatchNorm2d_bn1              128.0       64.0  \n",
      "50_layer2.3.ReLU_relu                        -          -  \n",
      "51_layer2.3.Conv2d_conv2               36.864k  2.359296M  \n",
      "52_layer2.3.BatchNorm2d_bn2              128.0       64.0  \n",
      "53_layer2.3.ReLU_relu                        -          -  \n",
      "54_layer3.0.Conv2d_conv1               73.728k  1.179648M  \n",
      "55_layer3.0.BatchNorm2d_bn1              256.0      128.0  \n",
      "56_layer3.0.ReLU_relu                        -          -  \n",
      "57_layer3.0.Conv2d_conv2              147.456k  2.359296M  \n",
      "58_layer3.0.BatchNorm2d_bn2              256.0      128.0  \n",
      "59_layer3.0.downsample.Conv2d_0         8.192k   131.072k  \n",
      "60_layer3.0.downsample.BatchNorm2d_1     256.0      128.0  \n",
      "61_layer3.0.ReLU_relu                        -          -  \n",
      "62_layer3.1.Conv2d_conv1              147.456k  2.359296M  \n",
      "63_layer3.1.BatchNorm2d_bn1              256.0      128.0  \n",
      "64_layer3.1.ReLU_relu                        -          -  \n",
      "65_layer3.1.Conv2d_conv2              147.456k  2.359296M  \n",
      "66_layer3.1.BatchNorm2d_bn2              256.0      128.0  \n",
      "67_layer3.1.ReLU_relu                        -          -  \n",
      "68_layer3.2.Conv2d_conv1              147.456k  2.359296M  \n",
      "69_layer3.2.BatchNorm2d_bn1              256.0      128.0  \n",
      "70_layer3.2.ReLU_relu                        -          -  \n",
      "71_layer3.2.Conv2d_conv2              147.456k  2.359296M  \n",
      "72_layer3.2.BatchNorm2d_bn2              256.0      128.0  \n",
      "73_layer3.2.ReLU_relu                        -          -  \n",
      "74_layer3.3.Conv2d_conv1              147.456k  2.359296M  \n",
      "75_layer3.3.BatchNorm2d_bn1              256.0      128.0  \n",
      "76_layer3.3.ReLU_relu                        -          -  \n",
      "77_layer3.3.Conv2d_conv2              147.456k  2.359296M  \n",
      "78_layer3.3.BatchNorm2d_bn2              256.0      128.0  \n",
      "79_layer3.3.ReLU_relu                        -          -  \n",
      "80_avgpool                                   -          -  \n",
      "81_fc                                    1.29k      1.28k  \n",
      "---------------------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params           1.472554M\n",
      "Trainable params       1.472554M\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             55.413984M\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((64, 3, 32, 32), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "nb_conv = 0\n",
    "\n",
    "for layer in s.index.values:\n",
    "    if \"Conv\" in layer:\n",
    "        nb_conv += 1\n",
    "print(nb_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = model_func()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/cifar10/supervised/2020-08-27_11:16:39_wideresnet28_2_0.1S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1fS\" % (get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % (model_func.__name__, args.supervised_ratio)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))\n",
    "\n",
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar10 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    def lr_lambda(e):\n",
    "        if e < 60:\n",
    "            return 1\n",
    "\n",
    "        elif 60 <= e < 120:\n",
    "            return 0.2\n",
    "\n",
    "        elif 120 <= e < 160:\n",
    "            return 0.04\n",
    "\n",
    "        else:\n",
    "            return 0.008\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ubs8k optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"ubs8k\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# Metrics\n",
    "fscore_fn = FScore()\n",
    "acc_fn = CategoricalAccuracy()\n",
    "avg = ContinueAverage()\n",
    "\n",
    "reset_metrics = lambda : [m.reset() for m in [fscore_fn, acc_fn, avg]]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73d487e580>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, 300, 300)\n",
    "y = [lr_lambda(x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"metrics: \", \"acc\", \"F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        logits = model(X)        \n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=10)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_ce(logits, y)\n",
    "\n",
    "            # metrics\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=10)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc\", acc, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc\", maximum_fn(\"acc\", acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/f1\", maximum_fn(\"f1\", fscore), epoch )\n",
    "\n",
    "    checkpoint.step(acc)\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n",
      "\n",
      "Training 1      - 100    -          2.3678 -           0.1930    | 0.0469   - 1.4349\n",
      "\u001b[1;4mValidati 1      - 100    -          2.0204 -           0.2221    | 0.0302   - 0.9056\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 2      - 100    -          1.8990 -           0.2826    | 0.0502   - 1.4163\n",
      "\u001b[1;4mValidati 2      - 100    -          1.9741 -           0.2881    | 0.1063   - 0.9638\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 3      - 100    -          1.8141 -           0.3010    | 0.1142   - 1.3792\n",
      "\u001b[1;4mValidati 3      - 100    -          1.9296 -           0.2724    | 0.1468   - 1.0014\u001b[0m\n",
      "Training 4      - 100    -          1.7142 -           0.3477    | 0.1660   - 1.4609\n",
      "\u001b[1;4mValidati 4      - 100    -          1.6497 -           0.3822    | 0.2446   - 0.9784\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 5      - 100    -          1.6160 -           0.3826    | 0.2246   - 1.3560\n",
      "\u001b[1;4mValidati 5      - 100    -          1.8536 -           0.3352    | 0.1475   - 0.9583\u001b[0m\n",
      "Training 6      - 100    -          1.5799 -           0.3955    | 0.2557   - 1.2282\n",
      "\u001b[1;4mValidati 6      - 100    -          1.5908 -           0.4251    | 0.3257   - 0.9216\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 7      - 100    -          1.5406 -           0.4402    | 0.3211   - 1.2915\n",
      "\u001b[1;4mValidati 7      - 100    -          1.7277 -           0.3616    | 0.2434   - 0.9079\u001b[0m\n",
      "Training 8      - 100    -          1.5491 -           0.4180    | 0.2589   - 1.2978\n",
      "\u001b[1;4mValidati 8      - 100    -          1.5867 -           0.4201    | 0.3251   - 0.9033\u001b[0m\n",
      "Training 9      - 100    -          1.4405 -           0.4695    | 0.3597   - 1.3849\n",
      "\u001b[1;4mValidati 9      - 100    -          1.4599 -           0.4686    | 0.3805   - 0.9015\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 10     - 100    -          1.4172 -           0.4756    | 0.3919   - 1.4310\n",
      "\u001b[1;4mValidati 10     - 100    -          1.6459 -           0.4142    | 0.3554   - 0.9268\u001b[0m\n",
      "Training 11     - 100    -          1.4834 -           0.4367    | 0.3316   - 1.3787\n",
      "\u001b[1;4mValidati 11     - 100    -          1.5172 -           0.4679    | 0.4016   - 0.9141\u001b[0m\n",
      "Training 12     - 100    -          1.3640 -           0.4943    | 0.4141   - 1.1605\n",
      "\u001b[1;4mValidati 12     - 100    -          1.5144 -           0.4460    | 0.3884   - 0.8519\u001b[0m\n",
      "Training 13     - 100    -          1.3449 -           0.4965    | 0.4086   - 1.2082\n",
      "\u001b[1;4mValidati 13     - 100    -          1.3787 -           0.5062    | 0.4553   - 0.8173\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 14     - 100    -          1.3079 -           0.5242    | 0.4567   - 1.3493\n",
      "\u001b[1;4mValidati 14     - 100    -          1.5633 -           0.4587    | 0.4043   - 0.8182\u001b[0m\n",
      "Training 15     - 100    -          1.2802 -           0.5293    | 0.4583   - 1.5474\n",
      "\u001b[1;4mValidati 15     - 100    -          1.3567 -           0.5032    | 0.4424   - 0.9904\u001b[0m\n",
      "Training 16     - 100    -          1.2389 -           0.5377    | 0.4903   - 1.3885\n",
      "\u001b[1;4mValidati 16     - 100    -          1.2764 -           0.5399    | 0.4952   - 0.9642\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 17     - 100    -          1.2498 -           0.5523    | 0.4897   - 1.3102\n",
      "\u001b[1;4mValidati 17     - 100    -          1.3536 -           0.5230    | 0.4912   - 0.9309\u001b[0m\n",
      "Training 18     - 100    -          1.2487 -           0.5400    | 0.4826   - 1.3768\n",
      "\u001b[1;4mValidati 18     - 100    -          1.4442 -           0.4899    | 0.4529   - 0.8720\u001b[0m\n",
      "Training 19     - 100    -          1.2480 -           0.5582    | 0.4901   - 1.2882\n",
      "\u001b[1;4mValidati 19     - 100    -          1.4696 -           0.4828    | 0.4469   - 0.8433\u001b[0m\n",
      "Training 20     - 100    -          1.1226 -           0.6053    | 0.5510   - 1.5065\n",
      "\u001b[1;4mValidati 20     - 100    -          1.3616 -           0.5181    | 0.4851   - 1.0240\u001b[0m\n",
      "Training 21     - 100    -          1.1018 -           0.6088    | 0.5708   - 1.4294\n",
      "\u001b[1;4mValidati 21     - 100    -          1.2717 -           0.5589    | 0.5240   - 0.8807\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 22     - 100    -          1.1098 -           0.6133    | 0.5687   - 1.2791\n",
      "\u001b[1;4mValidati 22     - 100    -          1.3381 -           0.5266    | 0.4887   - 0.8959\u001b[0m\n",
      "Training 23     - 100    -          1.0569 -           0.6229    | 0.5959   - 1.3825\n",
      "\u001b[1;4mValidati 23     - 100    -          1.3244 -           0.5436    | 0.5287   - 0.8919\u001b[0m\n",
      "Training 24     - 100    -          1.0492 -           0.6318    | 0.5977   - 1.4244\n",
      "\u001b[1;4mValidati 24     - 100    -          1.2158 -           0.5715    | 0.5535   - 0.9092\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 25     - 100    -          1.0375 -           0.6287    | 0.6116   - 1.3409\n",
      "\u001b[1;4mValidati 25     - 100    -          1.3554 -           0.5304    | 0.5048   - 0.9126\u001b[0m\n",
      "Training 26     - 100    -          1.1211 -           0.6002    | 0.5634   - 1.3556\n",
      "\u001b[1;4mValidati 26     - 100    -          1.2303 -           0.5665    | 0.5382   - 0.8917\u001b[0m\n",
      "Training 27     - 100    -          1.0144 -           0.6496    | 0.6216   - 1.3903\n",
      "\u001b[1;4mValidati 27     - 100    -          1.4518 -           0.5003    | 0.4766   - 0.9028\u001b[0m\n",
      "Training 28     - 100    -          1.1185 -           0.5971    | 0.5576   - 1.3895\n",
      "\u001b[1;4mValidati 28     - 100    -          1.1526 -           0.5870    | 0.5712   - 0.8977\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 29     - 100    -          0.9775 -           0.6514    | 0.6221   - 1.4888\n",
      "\u001b[1;4mValidati 29     - 100    -          1.2358 -           0.5827    | 0.5584   - 0.9071\u001b[0m\n",
      "Training 30     - 100    -          0.9807 -           0.6525    | 0.6388   - 1.3465\n",
      "\u001b[1;4mValidati 30     - 100    -          1.1748 -           0.5940    | 0.5702   - 0.9398\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 31     - 100    -          0.9227 -           0.6773    | 0.6554   - 1.4110\n",
      "\u001b[1;4mValidati 31     - 100    -          1.4138 -           0.5581    | 0.5514   - 0.9352\u001b[0m\n",
      "Training 32     - 100    -          1.0249 -           0.6393    | 0.6098   - 1.4409\n",
      "\u001b[1;4mValidati 32     - 100    -          1.1857 -           0.5820    | 0.5617   - 0.8750\u001b[0m\n",
      "Training 33     - 100    -          0.9950 -           0.6525    | 0.6253   - 1.3768\n",
      "\u001b[1;4mValidati 33     - 100    -          1.1968 -           0.5843    | 0.5763   - 0.9028\u001b[0m\n",
      "Training 34     - 100    -          0.8995 -           0.6869    | 0.6709   - 1.3419\n",
      "\u001b[1;4mValidati 34     - 100    -          1.2543 -           0.5606    | 0.5494   - 0.8819\u001b[0m\n",
      "Training 35     - 100    -          0.9816 -           0.6488    | 0.6297   - 1.3830\n",
      "\u001b[1;4mValidati 35     - 100    -          1.2263 -           0.5752    | 0.5684   - 0.9585\u001b[0m\n",
      "Training 36     - 100    -          0.9665 -           0.6643    | 0.6413   - 1.3452\n",
      "\u001b[1;4mValidati 36     - 100    -          1.2154 -           0.5788    | 0.5674   - 0.9078\u001b[0m\n",
      "Training 37     - 100    -          0.9753 -           0.6527    | 0.6286   - 1.4129\n",
      "\u001b[1;4mValidati 37     - 100    -          1.0128 -           0.6394    | 0.6207   - 0.9098\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 38     - 100    -          0.9209 -           0.6721    | 0.6514   - 1.3601\n",
      "\u001b[1;4mValidati 38     - 100    -          1.1722 -           0.5999    | 0.5921   - 0.8221\u001b[0m\n",
      "Training 39     - 100    -          0.8630 -           0.6988    | 0.6838   - 1.3614\n",
      "\u001b[1;4mValidati 39     - 100    -          1.3522 -           0.5650    | 0.5531   - 0.9445\u001b[0m\n",
      "Training 40     - 100    -          0.8433 -           0.7021    | 0.6894   - 1.3998\n",
      "\u001b[1;4mValidati 40     - 100    -          1.2587 -           0.5776    | 0.5764   - 0.8830\u001b[0m\n",
      "Training 41     - 100    -          0.7807 -           0.7205    | 0.7122   - 1.2770\n",
      "\u001b[1;4mValidati 41     - 100    -          1.1654 -           0.6053    | 0.6040   - 0.8608\u001b[0m\n",
      "Training 42     - 100    -          0.9085 -           0.6814    | 0.6686   - 1.4619\n",
      "\u001b[1;4mValidati 42     - 100    -          1.0848 -           0.6289    | 0.6176   - 0.9365\u001b[0m\n",
      "Training 43     - 100    -          0.9880 -           0.6600    | 0.6332   - 1.4576\n",
      "\u001b[1;4mValidati 43     - 100    -          1.4048 -           0.5350    | 0.5213   - 0.9084\u001b[0m\n",
      "Training 44     - 100    -          1.1478 -           0.5977    | 0.5539   - 1.3791\n",
      "\u001b[1;4mValidati 44     - 100    -          1.1005 -           0.6123    | 0.5874   - 0.9164\u001b[0m\n",
      "Training 45     - 100    -          0.8681 -           0.6834    | 0.6717   - 1.4089\n",
      "\u001b[1;4mValidati 45     - 100    -          1.1295 -           0.6148    | 0.6051   - 0.8810\u001b[0m\n",
      "Training 46     - 100    -          0.8907 -           0.6830    | 0.6695   - 1.3490\n",
      "\u001b[1;4mValidati 46     - 100    -          0.9969 -           0.6510    | 0.6395   - 0.8752\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 47     - 100    -          0.7522 -           0.7383    | 0.7251   - 1.3361\n",
      "\u001b[1;4mValidati 47     - 100    -          1.2913 -           0.5851    | 0.5768   - 1.0055\u001b[0m\n",
      "Training 48     - 100    -          0.7402 -           0.7373    | 0.7275   - 1.4108\n",
      "\u001b[1;4mValidati 48     - 100    -          1.0140 -           0.6531    | 0.6529   - 0.8992\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 49     - 100    -          0.7422 -           0.7355    | 0.7262   - 1.3629\n",
      "\u001b[1;4mValidati 49     - 100    -          1.1468 -           0.6196    | 0.6145   - 0.9953\u001b[0m\n",
      "Training 50     - 100    -          0.7542 -           0.7318    | 0.7219   - 1.4067\n",
      "\u001b[1;4mValidati 50     - 100    -          1.2805 -           0.6001    | 0.6022   - 0.9535\u001b[0m\n",
      "Training 51     - 100    -          0.7471 -           0.7410    | 0.7316   - 1.4124\n",
      "\u001b[1;4mValidati 51     - 100    -          1.5700 -           0.5324    | 0.5300   - 0.9469\u001b[0m\n",
      "Training 52     - 100    -          0.8554 -           0.6986    | 0.6896   - 1.4187\n",
      "\u001b[1;4mValidati 52     - 100    -          1.1696 -           0.6204    | 0.6140   - 0.8710\u001b[0m\n",
      "Training 53     - 100    -          0.7585 -           0.7354    | 0.7226   - 1.5740\n",
      "\u001b[1;4mValidati 53     - 100    -          1.0191 -           0.6501    | 0.6423   - 0.9309\u001b[0m\n",
      "Training 54     - 100    -          0.6870 -           0.7563    | 0.7491   - 1.3374\n",
      "\u001b[1;4mValidati 54     - 100    -          1.0901 -           0.6421    | 0.6373   - 0.8376\u001b[0m\n",
      "Training 55     - 100    -          0.7252 -           0.7438    | 0.7445   - 1.2776\n",
      "\u001b[1;4mValidati 55     - 100    -          1.1427 -           0.6417    | 0.6354   - 0.8692\u001b[0m\n",
      "Training 56     - 100    -          0.6898 -           0.7596    | 0.7481   - 1.4487\n",
      "\u001b[1;4mValidati 56     - 100    -          1.0932 -           0.6414    | 0.6380   - 0.8489\u001b[0m\n",
      "Training 57     - 100    -          0.7546 -           0.7432    | 0.7369   - 1.4261\n",
      "\u001b[1;4mValidati 57     - 100    -          1.0660 -           0.6470    | 0.6430   - 0.8827\u001b[0m\n",
      "Training 58     - 100    -          0.6353 -           0.7727    | 0.7713   - 1.4663\n",
      "\u001b[1;4mValidati 58     - 100    -          1.0902 -           0.6479    | 0.6455   - 1.0061\u001b[0m\n",
      "Training 59     - 100    -          0.5944 -           0.7811    | 0.7797   - 1.3693\n",
      "\u001b[1;4mValidati 59     - 100    -          1.1796 -           0.6303    | 0.6251   - 0.9847\u001b[0m\n",
      "Training 60     - 100    -          0.7083 -           0.7436    | 0.7441   - 1.3933\n",
      "\u001b[1;4mValidati 60     - 100    -          1.1809 -           0.6260    | 0.6272   - 0.8747\u001b[0m\n",
      "Training 61     - 100    -          0.5187 -           0.8141    | 0.8110   - 1.4482\n",
      "\u001b[1;4mValidati 61     - 100    -          0.8086 -           0.7247    | 0.7258   - 0.9655\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 62     - 100    -          0.4302 -           0.8508    | 0.8498   - 1.4364\n",
      "\u001b[1;4mValidati 62     - 100    -          0.8578 -           0.7177    | 0.7208   - 0.9079\u001b[0m\n",
      "Training 63     - 100    -          0.4089 -           0.8594    | 0.8595   - 1.4592\n",
      "\u001b[1;4mValidati 63     - 100    -          0.8388 -           0.7298    | 0.7312   - 0.9790\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 64     - 100    -          0.3781 -           0.8676    | 0.8707   - 1.3845\n",
      "\u001b[1;4mValidati 64     - 100    -          0.8587 -           0.7300    | 0.7340   - 0.9525\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 65     - 100    -          0.3730 -           0.8686    | 0.8688   - 1.4198\n",
      "\u001b[1;4mValidati 65     - 100    -          0.8670 -           0.7309    | 0.7330   - 0.8965\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 66     - 100    -          0.3719 -           0.8695    | 0.8702   - 1.3973\n",
      "\u001b[1;4mValidati 66     - 100    -          0.8886 -           0.7247    | 0.7275   - 0.9808\u001b[0m\n",
      "Training 67     - 100    -          0.3699 -           0.8699    | 0.8744   - 1.4370\n",
      "\u001b[1;4mValidati 67     - 100    -          0.8766 -           0.7316    | 0.7340   - 0.9289\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 68     - 100    -          0.3437 -           0.8793    | 0.8767   - 1.4163\n",
      "\u001b[1;4mValidati 68     - 100    -          0.8939 -           0.7306    | 0.7321   - 0.9390\u001b[0m\n",
      "Training 69     - 100    -          0.3131 -           0.8916    | 0.8931   - 1.4961\n",
      "\u001b[1;4mValidati 69     - 100    -          0.9637 -           0.7215    | 0.7243   - 0.8873\u001b[0m\n",
      "Training 70     - 100    -          0.3390 -           0.8828    | 0.8803   - 1.2114\n",
      "\u001b[1;4mValidati 70     - 100    -          0.9292 -           0.7180    | 0.7208   - 0.9328\u001b[0m\n",
      "Training 71     - 100    -          0.3678 -           0.8662    | 0.8689   - 1.3788\n",
      "\u001b[1;4mValidati 71     - 100    -          0.9405 -           0.7248    | 0.7269   - 0.9463\u001b[0m\n",
      "Training 72     - 100    -          0.2987 -           0.8988    | 0.8974   - 1.4115\n",
      "\u001b[1;4mValidati 72     - 100    -          0.9480 -           0.7222    | 0.7259   - 0.9306\u001b[0m\n",
      "Training 73     - 100    -          0.3052 -           0.8871    | 0.8866   - 1.2932\n",
      "\u001b[1;4mValidati 73     - 100    -          0.9418 -           0.7233    | 0.7285   - 0.9500\u001b[0m\n",
      "Training 74     - 100    -          0.3485 -           0.8771    | 0.8777   - 1.3700\n",
      "\u001b[1;4mValidati 74     - 100    -          0.9163 -           0.7272    | 0.7295   - 0.8599\u001b[0m\n",
      "Training 75     - 100    -          0.3759 -           0.8648    | 0.8645   - 1.5150\n",
      "\u001b[1;4mValidati 75     - 100    -          0.9593 -           0.7127    | 0.7149   - 1.0464\u001b[0m\n",
      "Training 76     - 100    -          0.3221 -           0.8893    | 0.8888   - 1.3598\n",
      "\u001b[1;4mValidati 76     - 100    -          1.0226 -           0.7036    | 0.7070   - 0.9131\u001b[0m\n",
      "Training 77     - 100    -          0.2994 -           0.8961    | 0.8923   - 1.3927\n",
      "\u001b[1;4mValidati 77     - 100    -          0.9925 -           0.7098    | 0.7152   - 0.9213\u001b[0m\n",
      "Training 78     - 100    -          0.3078 -           0.8953    | 0.8954   - 1.3419\n",
      "\u001b[1;4mValidati 78     - 100    -          0.9981 -           0.7107    | 0.7133   - 0.9498\u001b[0m\n",
      "Training 79     - 100    -          0.2873 -           0.9035    | 0.9023   - 1.4332\n",
      "\u001b[1;4mValidati 79     - 100    -          0.9349 -           0.7320    | 0.7334   - 0.8848\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 80     - 100    -          0.2557 -           0.9084    | 0.9090   - 1.2678\n",
      "\u001b[1;4mValidati 80     - 100    -          1.0219 -           0.7152    | 0.7203   - 0.9008\u001b[0m\n",
      "Training 81     - 100    -          0.2108 -           0.9283    | 0.9278   - 1.4447\n",
      "\u001b[1;4mValidati 81     - 100    -          0.9781 -           0.7225    | 0.7271   - 0.9024\u001b[0m\n",
      "Training 82     - 100    -          0.2506 -           0.9176    | 0.9172   - 1.3614\n",
      "\u001b[1;4mValidati 82     - 100    -          1.2070 -           0.6909    | 0.6950   - 0.9103\u001b[0m\n",
      "Training 83     - 100    -          0.3981 -           0.8680    | 0.8657   - 1.4188\n",
      "\u001b[1;4mValidati 83     - 100    -          1.0179 -           0.7062    | 0.7088   - 0.8567\u001b[0m\n",
      "Training 84     - 100    -          0.2851 -           0.9045    | 0.9021   - 1.2814\n",
      "\u001b[1;4mValidati 84     - 100    -          1.0467 -           0.7083    | 0.7090   - 0.9165\u001b[0m\n",
      "Training 85     - 100    -          0.4083 -           0.8611    | 0.8587   - 1.4326\n",
      "\u001b[1;4mValidati 85     - 100    -          0.9094 -           0.7261    | 0.7302   - 0.9859\u001b[0m\n",
      "Training 86     - 100    -          0.3251 -           0.8891    | 0.8850   - 1.3722\n",
      "\u001b[1;4mValidati 86     - 100    -          0.9309 -           0.7234    | 0.7262   - 0.9336\u001b[0m\n",
      "Training 87     - 100    -          0.2463 -           0.9127    | 0.9116   - 1.4326\n",
      "\u001b[1;4mValidati 87     - 100    -          0.9989 -           0.7189    | 0.7224   - 0.8868\u001b[0m\n",
      "Training 88     - 100    -          0.2683 -           0.9105    | 0.9101   - 1.3640\n",
      "\u001b[1;4mValidati 88     - 100    -          0.9716 -           0.7260    | 0.7299   - 0.8705\u001b[0m\n",
      "Training 89     - 100    -          0.2529 -           0.9205    | 0.9188   - 1.6038\n",
      "\u001b[1;4mValidati 89     - 100    -          0.9875 -           0.7283    | 0.7306   - 0.8760\u001b[0m\n",
      "Training 90     - 100    -          0.2546 -           0.9049    | 0.9038   - 1.5420\n",
      "\u001b[1;4mValidati 90     - 100    -          1.0123 -           0.7227    | 0.7267   - 0.9755\u001b[0m\n",
      "Training 91     - 100    -          0.3356 -           0.8797    | 0.8809   - 1.3113\n",
      "\u001b[1;4mValidati 91     - 100    -          0.9788 -           0.7189    | 0.7223   - 0.9702\u001b[0m\n",
      "Training 92     - 100    -          0.2403 -           0.9176    | 0.9173   - 1.4079\n",
      "\u001b[1;4mValidati 92     - 100    -          0.9931 -           0.7246    | 0.7280   - 0.8828\u001b[0m\n",
      "Training 93     - 100    -          0.2210 -           0.9191    | 0.9189   - 1.3759\n",
      "\u001b[1;4mValidati 93     - 100    -          1.0721 -           0.7108    | 0.7157   - 1.0185\u001b[0m\n",
      "Training 94     - 100    -          0.2574 -           0.9074    | 0.9086   - 1.3956\n",
      "\u001b[1;4mValidati 94     - 100    -          1.0756 -           0.7188    | 0.7220   - 0.9294\u001b[0m\n",
      "Training 95     - 100    -          0.3020 -           0.8883    | 0.8882   - 1.4534\n",
      "\u001b[1;4mValidati 95     - 100    -          1.0125 -           0.7173    | 0.7210   - 0.8374\u001b[0m\n",
      "Training 96     - 100    -          0.2384 -           0.9234    | 0.9207   - 1.3763\n",
      "\u001b[1;4mValidati 96     - 100    -          0.9928 -           0.7255    | 0.7283   - 0.9804\u001b[0m\n",
      "Training 97     - 100    -          0.1884 -           0.9357    | 0.9334   - 1.3626\n",
      "\u001b[1;4mValidati 97     - 100    -          1.0493 -           0.7248    | 0.7280   - 0.8983\u001b[0m\n",
      "Training 98     - 100    -          0.1851 -           0.9383    | 0.9380   - 1.3738\n",
      "\u001b[1;4mValidati 98     - 100    -          1.2396 -           0.6949    | 0.6972   - 0.9052\u001b[0m\n",
      "Training 99     - 100    -          0.3472 -           0.8779    | 0.8793   - 1.3867\n",
      "\u001b[1;4mValidati 99     - 100    -          1.0731 -           0.7111    | 0.7137   - 0.9350\u001b[0m\n",
      "Training 100    - 100    -          0.2672 -           0.9078    | 0.9089   - 1.4369\n",
      "\u001b[1;4mValidati 100    - 100    -          1.1466 -           0.7007    | 0.7041   - 1.0173\u001b[0m\n",
      "Training 101    - 100    -          0.3639 -           0.8758    | 0.8728   - 1.3411\n",
      "\u001b[1;4mValidati 101    - 100    -          1.0112 -           0.7154    | 0.7210   - 0.8716\u001b[0m\n",
      "Training 102    - 100    -          0.2245 -           0.9230    | 0.9231   - 1.4333\n",
      "\u001b[1;4mValidati 102    - 100    -          1.0001 -           0.7220    | 0.7266   - 0.9734\u001b[0m\n",
      "Training 103    - 100    -          0.2176 -           0.9258    | 0.9253   - 1.6832\n",
      "\u001b[1;4mValidati 103    - 100    -          1.0998 -           0.7146    | 0.7172   - 0.9188\u001b[0m\n",
      "Training 104    - 100    -          0.1802 -           0.9355    | 0.9360   - 1.4839\n",
      "\u001b[1;4mValidati 104    - 100    -          1.0934 -           0.7170    | 0.7226   - 0.9459\u001b[0m\n",
      "Training 105    - 100    -          0.2732 -           0.9088    | 0.9092   - 1.5343\n",
      "\u001b[1;4mValidati 105    - 100    -          1.1053 -           0.7089    | 0.7112   - 0.9214\u001b[0m\n",
      "Training 106    - 100    -          0.2836 -           0.9021    | 0.9020   - 1.9302\n",
      "\u001b[1;4mValidati 106    - 100    -          0.9977 -           0.7277    | 0.7309   - 1.3281\u001b[0m\n",
      "Training 107    - 100    -          0.2081 -           0.9348    | 0.9322   - 1.4610\n",
      "\u001b[1;4mValidati 107    - 100    -          1.1107 -           0.7098    | 0.7124   - 0.9451\u001b[0m\n",
      "Training 108    - 100    -          0.2317 -           0.9234    | 0.9240   - 1.3135\n",
      "\u001b[1;4mValidati 108    - 100    -          1.0145 -           0.7334    | 0.7354   - 0.9527\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 109    - 100    -          0.2041 -           0.9244    | 0.9260   - 1.4227\n",
      "\u001b[1;4mValidati 109    - 100    -          1.0556 -           0.7211    | 0.7253   - 0.8928\u001b[0m\n",
      "Training 110    - 100    -          0.2214 -           0.9242    | 0.9211   - 1.4408\n",
      "\u001b[1;4mValidati 110    - 100    -          1.1510 -           0.7143    | 0.7172   - 0.9500\u001b[0m\n",
      "Training 111    - 100    -          0.2953 -           0.8865    | 0.8901   - 1.3887\n",
      "\u001b[1;4mValidati 111    - 100    -          1.0904 -           0.7081    | 0.7090   - 0.8970\u001b[0m\n",
      "Training 112    - 100    -          0.2269 -           0.9236    | 0.9226   - 1.4139\n",
      "\u001b[1;4mValidati 112    - 100    -          1.0061 -           0.7279    | 0.7312   - 0.8873\u001b[0m\n",
      "Training 113    - 100    -          0.1549 -           0.9486    | 0.9476   - 1.4329\n",
      "\u001b[1;4mValidati 113    - 100    -          1.0547 -           0.7285    | 0.7305   - 0.9530\u001b[0m\n",
      "Training 114    - 100    -          0.1368 -           0.9576    | 0.9571   - 1.5261\n",
      "\u001b[1;4mValidati 114    - 100    -          1.1163 -           0.7177    | 0.7197   - 0.9857\u001b[0m\n",
      "Training 115    - 100    -          0.2745 -           0.9092    | 0.9067   - 1.4235\n",
      "\u001b[1;4mValidati 115    - 100    -          1.1080 -           0.7146    | 0.7173   - 0.9615\u001b[0m\n",
      "Training 116    - 100    -          0.2818 -           0.9029    | 0.9014   - 1.5093\n",
      "\u001b[1;4mValidati 116    - 100    -          1.1885 -           0.7047    | 0.7076   - 1.0151\u001b[0m\n",
      "Training 117    - 100    -          0.2122 -           0.9350    | 0.9322   - 1.4043\n",
      "\u001b[1;4mValidati 117    - 100    -          1.0061 -           0.7303    | 0.7326   - 0.9192\u001b[0m\n",
      "Training 118    - 100    -          0.1972 -           0.9311    | 0.9322   - 1.3996\n",
      "\u001b[1;4mValidati 118    - 100    -          1.0329 -           0.7276    | 0.7287   - 0.9346\u001b[0m\n",
      "Training 119    - 100    -          0.2133 -           0.9248    | 0.9251   - 1.4181\n",
      "\u001b[1;4mValidati 119    - 100    -          1.0935 -           0.7245    | 0.7271   - 0.9356\u001b[0m\n",
      "Training 120    - 100    -          0.2679 -           0.9109    | 0.9099   - 1.3151\n",
      "\u001b[1;4mValidati 120    - 100    -          1.0770 -           0.7117    | 0.7161   - 0.9627\u001b[0m\n",
      "Training 121    - 100    -          0.1532 -           0.9479    | 0.9457   - 1.4285\n",
      "\u001b[1;4mValidati 121    - 100    -          0.9665 -           0.7387    | 0.7421   - 0.9886\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 122    - 100    -          0.1444 -           0.9547    | 0.9550   - 1.2665\n",
      "\u001b[1;4mValidati 122    - 100    -          0.9761 -           0.7381    | 0.7420   - 0.9518\u001b[0m\n",
      "Training 123    - 100    -          0.1143 -           0.9631    | 0.9638   - 1.3369\n",
      "\u001b[1;4mValidati 123    - 100    -          0.9691 -           0.7402    | 0.7437   - 0.8736\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 124    - 100    -          0.0959 -           0.9709    | 0.9700   - 1.3038\n",
      "\u001b[1;4mValidati 124    - 100    -          0.9725 -           0.7451    | 0.7476   - 0.8784\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 125    - 100    -          0.0903 -           0.9688    | 0.9685   - 1.2823\n",
      "\u001b[1;4mValidati 125    - 100    -          0.9663 -           0.7486    | 0.7521   - 0.9243\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 126    - 100    -          0.0991 -           0.9695    | 0.9683   - 1.3452\n",
      "\u001b[1;4mValidati 126    - 100    -          1.0006 -           0.7457    | 0.7471   - 0.9752\u001b[0m\n",
      "Training 127    - 100    -          0.0893 -           0.9750    | 0.9743   - 1.3982\n",
      "\u001b[1;4mValidati 127    - 100    -          1.0043 -           0.7431    | 0.7459   - 0.9214\u001b[0m\n",
      "Training 128    - 100    -          0.0874 -           0.9707    | 0.9696   - 1.2327\n",
      "\u001b[1;4mValidati 128    - 100    -          1.0111 -           0.7442    | 0.7468   - 0.9054\u001b[0m\n",
      "Training 129    - 100    -          0.0887 -           0.9719    | 0.9714   - 1.4631\n",
      "\u001b[1;4mValidati 129    - 100    -          1.0206 -           0.7433    | 0.7469   - 0.9273\u001b[0m\n",
      "Training 130    - 100    -          0.1191 -           0.9744    | 0.9729   - 1.4105\n",
      "\u001b[1;4mValidati 130    - 100    -          1.0199 -           0.7481    | 0.7506   - 0.9368\u001b[0m\n",
      "Training 131    - 100    -          0.0904 -           0.9732    | 0.9718   - 1.3988\n",
      "\u001b[1;4mValidati 131    - 100    -          1.0366 -           0.7395    | 0.7447   - 0.9352\u001b[0m\n",
      "Training 132    - 100    -          0.0812 -           0.9775    | 0.9778   - 1.3967\n",
      "\u001b[1;4mValidati 132    - 100    -          1.0387 -           0.7436    | 0.7475   - 0.9360\u001b[0m\n",
      "Training 133    - 100    -          0.0807 -           0.9725    | 0.9732   - 1.4309\n",
      "\u001b[1;4mValidati 133    - 100    -          1.0576 -           0.7431    | 0.7457   - 0.9577\u001b[0m\n",
      "Training 134    - 100    -          0.0769 -           0.9783    | 0.9782   - 1.3971\n",
      "\u001b[1;4mValidati 134    - 100    -          1.0763 -           0.7390    | 0.7427   - 0.8820\u001b[0m\n",
      "Training 135    - 100    -          0.0800 -           0.9756    | 0.9750   - 1.3190\n",
      "\u001b[1;4mValidati 135    - 100    -          1.0532 -           0.7419    | 0.7456   - 0.8950\u001b[0m\n",
      "Training 136    - 100    -          0.0672 -           0.9801    | 0.9788   - 1.2336\n",
      "\u001b[1;4mValidati 136    - 100    -          1.0783 -           0.7408    | 0.7422   - 0.8717\u001b[0m\n",
      "Training 137    - 100    -          0.0648 -           0.9814    | 0.9822   - 1.3978\n",
      "\u001b[1;4mValidati 137    - 100    -          1.0606 -           0.7430    | 0.7452   - 0.9550\u001b[0m\n",
      "Training 138    - 100    -          0.0584 -           0.9842    | 0.9832   - 1.3985\n",
      "\u001b[1;4mValidati 138    - 100    -          1.0704 -           0.7443    | 0.7472   - 0.9625\u001b[0m\n",
      "Training 139    - 100    -          0.0555 -           0.9842    | 0.9841   - 1.4264\n",
      "\u001b[1;4mValidati 139    - 100    -          1.0515 -           0.7456    | 0.7483   - 0.9508\u001b[0m\n",
      "Training 140    - 100    -          0.0646 -           0.9807    | 0.9798   - 1.3291\n",
      "\u001b[1;4mValidati 140    - 100    -          1.0622 -           0.7481    | 0.7508   - 1.0090\u001b[0m\n",
      "Training 141    - 100    -          0.0500 -           0.9869    | 0.9862   - 1.4430\n",
      "\u001b[1;4mValidati 141    - 100    -          1.0787 -           0.7466    | 0.7496   - 0.9175\u001b[0m\n",
      "Training 142    - 100    -          0.0437 -           0.9893    | 0.9892   - 1.4361\n",
      "\u001b[1;4mValidati 142    - 100    -          1.0784 -           0.7450    | 0.7465   - 0.8720\u001b[0m\n",
      "Training 143    - 100    -          0.0611 -           0.9871    | 0.9862   - 1.3666\n",
      "\u001b[1;4mValidati 143    - 100    -          1.0851 -           0.7498    | 0.7530   - 0.8405\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 144    - 100    -          0.0632 -           0.9801    | 0.9808   - 1.4569\n",
      "\u001b[1;4mValidati 144    - 100    -          1.1159 -           0.7416    | 0.7464   - 0.8974\u001b[0m\n",
      "Training 145    - 100    -          0.0618 -           0.9809    | 0.9803   - 1.4142\n",
      "\u001b[1;4mValidati 145    - 100    -          1.0916 -           0.7457    | 0.7490   - 0.8533\u001b[0m\n",
      "Training 146    - 100    -          0.0601 -           0.9797    | 0.9805   - 1.6289\n",
      "\u001b[1;4mValidati 146    - 100    -          1.0955 -           0.7470    | 0.7487   - 1.0255\u001b[0m\n",
      "Training 147    - 100    -          0.0477 -           0.9879    | 0.9875   - 1.4253\n",
      "\u001b[1;4mValidati 147    - 100    -          1.1292 -           0.7420    | 0.7446   - 0.8910\u001b[0m\n",
      "Training 148    - 100    -          0.0730 -           0.9828    | 0.9830   - 1.3962\n",
      "\u001b[1;4mValidati 148    - 100    -          1.1243 -           0.7450    | 0.7472   - 0.9288\u001b[0m\n",
      "Training 149    - 100    -          0.0712 -           0.9752    | 0.9762   - 1.4343\n",
      "\u001b[1;4mValidati 149    - 100    -          1.1634 -           0.7420    | 0.7452   - 0.8509\u001b[0m\n",
      "Training 150    - 100    -          0.0605 -           0.9812    | 0.9817   - 1.3895\n",
      "\u001b[1;4mValidati 150    - 100    -          1.1350 -           0.7413    | 0.7436   - 0.8452\u001b[0m\n",
      "Training 151    - 100    -          0.0646 -           0.9814    | 0.9812   - 1.2310\n",
      "\u001b[1;4mValidati 151    - 100    -          1.1281 -           0.7459    | 0.7489   - 0.8310\u001b[0m\n",
      "Training 152    - 100    -          0.0681 -           0.9785    | 0.9779   - 1.3758\n",
      "\u001b[1;4mValidati 152    - 100    -          1.1510 -           0.7391    | 0.7427   - 0.8916\u001b[0m\n",
      "Training 153    - 100    -          0.0637 -           0.9799    | 0.9806   - 1.4021\n",
      "\u001b[1;4mValidati 153    - 100    -          1.1610 -           0.7366    | 0.7393   - 0.8889\u001b[0m\n",
      "Training 154    - 100    -          0.0498 -           0.9861    | 0.9861   - 1.4414\n",
      "\u001b[1;4mValidati 154    - 100    -          1.1560 -           0.7411    | 0.7435   - 1.0008\u001b[0m\n",
      "Training 155    - 100    -          0.0732 -           0.9785    | 0.9780   - 1.4016\n",
      "\u001b[1;4mValidati 155    - 100    -          1.1417 -           0.7397    | 0.7420   - 0.9176\u001b[0m\n",
      "Training 156    - 100    -          0.0564 -           0.9846    | 0.9846   - 1.3971\n",
      "\u001b[1;4mValidati 156    - 100    -          1.1342 -           0.7437    | 0.7450   - 0.9072\u001b[0m\n",
      "Training 157    - 100    -          0.0495 -           0.9834    | 0.9846   - 1.2953\n",
      "\u001b[1;4mValidati 157    - 100    -          1.1466 -           0.7420    | 0.7449   - 0.8662\u001b[0m\n",
      "Training 158    - 100    -          0.0509 -           0.9873    | 0.9867   - 1.3319\n",
      "\u001b[1;4mValidati 158    - 100    -          1.1571 -           0.7407    | 0.7424   - 0.9604\u001b[0m\n",
      "Training 159    - 100    -          0.0498 -           0.9812    | 0.9820   - 1.4524\n",
      "\u001b[1;4mValidati 159    - 100    -          1.1565 -           0.7436    | 0.7461   - 0.8989\u001b[0m\n",
      "Training 160    - 100    -          0.0584 -           0.9785    | 0.9776   - 1.3868\n",
      "\u001b[1;4mValidati 160    - 100    -          1.1860 -           0.7385    | 0.7409   - 0.9656\u001b[0m\n",
      "Training 161    - 100    -          0.0395 -           0.9879    | 0.9883   - 1.4967\n",
      "\u001b[1;4mValidati 161    - 100    -          1.1635 -           0.7371    | 0.7397   - 1.0224\u001b[0m\n",
      "Training 162    - 100    -          0.0423 -           0.9885    | 0.9886   - 1.4005\n",
      "\u001b[1;4mValidati 162    - 100    -          1.1597 -           0.7426    | 0.7446   - 0.9428\u001b[0m\n",
      "Training 163    - 100    -          0.0527 -           0.9826    | 0.9821   - 1.4426\n",
      "\u001b[1;4mValidati 163    - 100    -          1.1484 -           0.7421    | 0.7439   - 0.9204\u001b[0m\n",
      "Training 164    - 100    -          0.0511 -           0.9828    | 0.9821   - 1.3585\n",
      "\u001b[1;4mValidati 164    - 100    -          1.1598 -           0.7430    | 0.7466   - 0.9365\u001b[0m\n",
      "Training 165    - 100    -          0.0376 -           0.9885    | 0.9884   - 1.3958\n",
      "\u001b[1;4mValidati 165    - 100    -          1.1391 -           0.7427    | 0.7437   - 0.8899\u001b[0m\n",
      "Training 166    - 100    -          0.0378 -           0.9902    | 0.9902   - 1.4341\n",
      "\u001b[1;4mValidati 166    - 100    -          1.1522 -           0.7432    | 0.7448   - 0.9420\u001b[0m\n",
      "Training 167    - 100    -          0.0321 -           0.9914    | 0.9909   - 1.3684\n",
      "\u001b[1;4mValidati 167    - 100    -          1.1224 -           0.7477    | 0.7508   - 0.9376\u001b[0m\n",
      "Training 168    - 100    -          0.0593 -           0.9871    | 0.9872   - 1.4851\n",
      "\u001b[1;4mValidati 168    - 100    -          1.1349 -           0.7464    | 0.7489   - 0.8886\u001b[0m\n",
      "Training 169    - 100    -          0.0367 -           0.9889    | 0.9880   - 1.3214\n",
      "\u001b[1;4mValidati 169    - 100    -          1.1666 -           0.7413    | 0.7439   - 0.9494\u001b[0m\n",
      "Training 170    - 100    -          0.0289 -           0.9932    | 0.9931   - 1.4706\n",
      "\u001b[1;4mValidati 170    - 100    -          1.1365 -           0.7440    | 0.7454   - 0.8861\u001b[0m\n",
      "Training 171    - 100    -          0.0446 -           0.9898    | 0.9895   - 1.3094\n",
      "\u001b[1;4mValidati 171    - 100    -          1.1452 -           0.7477    | 0.7503   - 0.8344\u001b[0m\n",
      "Training 172    - 100    -          0.0378 -           0.9914    | 0.9908   - 1.4915\n",
      "\u001b[1;4mValidati 172    - 100    -          1.1467 -           0.7475    | 0.7501   - 0.9616\u001b[0m\n",
      "Training 173    - 100    -          0.0505 -           0.9885    | 0.9890   - 1.4652\n",
      "\u001b[1;4mValidati 173    - 100    -          1.1521 -           0.7467    | 0.7493   - 0.9260\u001b[0m\n",
      "Training 174    - 100    -          0.0589 -           0.9869    | 0.9871   - 1.4561\n",
      "\u001b[1;4mValidati 174    - 100    -          1.1480 -           0.7452    | 0.7476   - 0.9136\u001b[0m\n",
      "Training 175    - 100    -          0.0369 -           0.9900    | 0.9893   - 1.3571\n",
      "\u001b[1;4mValidati 175    - 100    -          1.1532 -           0.7442    | 0.7475   - 0.8996\u001b[0m\n",
      "Training 176    - 100    -          0.0362 -           0.9918    | 0.9896   - 1.2686\n",
      "\u001b[1;4mValidati 176    - 100    -          1.1437 -           0.7490    | 0.7509   - 0.9017\u001b[0m\n",
      "Training 177    - 100    -          0.0530 -           0.9857    | 0.9857   - 1.4363\n",
      "\u001b[1;4mValidati 177    - 100    -          1.1559 -           0.7474    | 0.7489   - 0.9101\u001b[0m\n",
      "Training 178    - 100    -          0.0481 -           0.9908    | 0.9903   - 1.4569\n",
      "\u001b[1;4mValidati 178    - 100    -          1.1458 -           0.7444    | 0.7467   - 1.0220\u001b[0m\n",
      "Training 179    - 100    -          0.0301 -           0.9912    | 0.9912   - 1.4280\n",
      "\u001b[1;4mValidati 179    - 100    -          1.1329 -           0.7461    | 0.7486   - 0.9174\u001b[0m\n",
      "Training 180    - 100    -          0.0424 -           0.9873    | 0.9883   - 1.4660\n",
      "\u001b[1;4mValidati 180    - 100    -          1.1467 -           0.7463    | 0.7497   - 0.8672\u001b[0m\n",
      "Training 181    - 100    -          0.0287 -           0.9941    | 0.9921   - 1.4423\n",
      "\u001b[1;4mValidati 181    - 100    -          1.1607 -           0.7437    | 0.7456   - 0.9439\u001b[0m\n",
      "Training 182    - 100    -          0.0250 -           0.9951    | 0.9942   - 1.4821\n",
      "\u001b[1;4mValidati 182    - 100    -          1.1583 -           0.7448    | 0.7452   - 0.8494\u001b[0m\n",
      "Training 183    - 100    -          0.0400 -           0.9881    | 0.9879   - 1.1876\n",
      "\u001b[1;4mValidati 183    - 100    -          1.1335 -           0.7491    | 0.7514   - 0.8571\u001b[0m\n",
      "Training 184    - 100    -          0.0305 -           0.9914    | 0.9907   - 1.3440\n",
      "\u001b[1;4mValidati 184    - 100    -          1.1495 -           0.7482    | 0.7495   - 0.9423\u001b[0m\n",
      "Training 185    - 100    -          0.0275 -           0.9939    | 0.9942   - 1.2200\n",
      "\u001b[1;4mValidati 185    - 100    -          1.1498 -           0.7486    | 0.7506   - 0.9278\u001b[0m\n",
      "Training 186    - 100    -          0.0401 -           0.9873    | 0.9874   - 1.3474\n",
      "\u001b[1;4mValidati 186    - 100    -          1.1607 -           0.7457    | 0.7478   - 0.9293\u001b[0m\n",
      "Training 187    - 100    -          0.0289 -           0.9934    | 0.9934   - 1.3903\n",
      "\u001b[1;4mValidati 187    - 100    -          1.1531 -           0.7443    | 0.7457   - 0.8591\u001b[0m\n",
      "Training 188    - 100    -          0.0344 -           0.9879    | 0.9891   - 1.3642\n",
      "\u001b[1;4mValidati 188    - 100    -          1.1600 -           0.7454    | 0.7477   - 0.8720\u001b[0m\n",
      "Training 189    - 100    -          0.0312 -           0.9922    | 0.9922   - 1.3321\n",
      "\u001b[1;4mValidati 189    - 100    -          1.1659 -           0.7462    | 0.7479   - 0.8790\u001b[0m\n",
      "Training 190    - 100    -          0.0507 -           0.9865    | 0.9874   - 1.6106\n",
      "\u001b[1;4mValidati 190    - 100    -          1.1598 -           0.7419    | 0.7452   - 0.9081\u001b[0m\n",
      "Training 191    - 100    -          0.0357 -           0.9910    | 0.9907   - 1.4305\n",
      "\u001b[1;4mValidati 191    - 100    -          1.1630 -           0.7460    | 0.7479   - 0.8775\u001b[0m\n",
      "Training 192    - 100    -          0.0315 -           0.9885    | 0.9892   - 1.3887\n",
      "\u001b[1;4mValidati 192    - 100    -          1.1335 -           0.7476    | 0.7495   - 0.8483\u001b[0m\n",
      "Training 193    - 100    -          0.0243 -           0.9943    | 0.9944   - 1.4077\n",
      "\u001b[1;4mValidati 193    - 100    -          1.1462 -           0.7480    | 0.7490   - 0.8948\u001b[0m\n",
      "Training 194    - 100    -          0.0437 -           0.9852    | 0.9849   - 1.4744\n",
      "\u001b[1;4mValidati 194    - 100    -          1.1588 -           0.7446    | 0.7482   - 0.9871\u001b[0m\n",
      "Training 195    - 100    -          0.0366 -           0.9906    | 0.9906   - 1.4436\n",
      "\u001b[1;4mValidati 195    - 100    -          1.1550 -           0.7465    | 0.7489   - 0.8996\u001b[0m\n",
      "Training 196    - 100    -          0.0324 -           0.9908    | 0.9896   - 1.4515\n",
      "\u001b[1;4mValidati 196    - 100    -          1.1872 -           0.7439    | 0.7461   - 0.8485\u001b[0m\n",
      "Training 197    - 100    -          0.0371 -           0.9877    | 0.9887   - 1.4857\n",
      "\u001b[1;4mValidati 197    - 100    -          1.1762 -           0.7451    | 0.7479   - 0.9734\u001b[0m\n",
      "Training 198    - 100    -          0.0302 -           0.9896    | 0.9899   - 1.2534\n",
      "\u001b[1;4mValidati 198    - 100    -          1.1734 -           0.7453    | 0.7480   - 0.8831\u001b[0m\n",
      "Training 199    - 100    -          0.0378 -           0.9879    | 0.9880   - 1.5009\n",
      "\u001b[1;4mValidati 199    - 100    -          1.1563 -           0.7462    | 0.7496   - 1.0019\u001b[0m\n",
      "Training 200    - 100    -          0.0350 -           0.9924    | 0.9917   - 1.3398\n",
      "\u001b[1;4mValidati 200    - 100    -          1.1764 -           0.7455    | 0.7461   - 0.8461\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
