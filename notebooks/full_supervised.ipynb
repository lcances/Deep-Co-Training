{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcances/sync/Documents_sync/Projet/PyTorch/audio/torchaudio/extension/extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "from DCT.util.utils import reset_seed, get_datetime\n",
    "from DCT.util.model_loader import get_model_from_name\n",
    "from DCT.util.dataset_loader import load_dataset\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"esc50\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"esc_wideresnet28_8\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=1.0, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=64, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=200, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.001, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=50, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"Datasets parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, checkpoint_path='supervised', checkpoint_root='../model_save/', dataset='esc50', dataset_root='../datasets', from_config='', learning_rate=0.001, model='esc_wideresnet28_8', nb_epoch=200, num_classes=50, resume=False, seed=1234, supervised_ratio=1.0, tensorboard_path='supervised', tensorboard_root='../tensorboard/', tensorboard_sufix='', train_folds=[1, 2, 3, 4], val_folds=[5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "transforms = nn.Sequential(\n",
    "    MelSpectrogram(sample_rate=44100, n_fft=2048, hop_length=512, n_mels=64), \n",
    "    AmplitudeToDB(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and verified.\n",
      "Dataset already downloaded and verified.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "we pre-processed the images using ZCA and augmented the dataset using horizontal flips and random translations. The translations\n",
    "were drawn from [âˆ’2, 2] pixels,\n",
    "\"\"\"\n",
    "\n",
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"supervised\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    transform=transforms,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 431])\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(len(np.unique(y.numpy())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# model_func = get_model_from_name(args.model)\n",
    "model_func = get_model_from_name(\"esc_wideresnet28_8\")\n",
    "model = model_func(input_shape=(64, 431), num_classes = args.num_classes)\n",
    "# model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "                                          Kernel Shape       Output Shape  \\\n",
      "Layer                                                                       \n",
      "0_conv1                                 [3, 128, 3, 3]  [1, 128, 64, 431]   \n",
      "1_bn1                                            [128]  [1, 128, 64, 431]   \n",
      "2_relu                                               -  [1, 128, 64, 431]   \n",
      "3_maxpool                                            -  [1, 128, 32, 216]   \n",
      "4_layer1.0.Conv2d_conv1               [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "5_layer1.0.BatchNorm2d_bn1                       [128]  [1, 128, 32, 216]   \n",
      "6_layer1.0.ReLU_relu                                 -  [1, 128, 32, 216]   \n",
      "7_layer1.0.Conv2d_conv2               [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "8_layer1.0.BatchNorm2d_bn2                       [128]  [1, 128, 32, 216]   \n",
      "9_layer1.0.ReLU_relu                                 -  [1, 128, 32, 216]   \n",
      "10_layer1.1.Conv2d_conv1              [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "11_layer1.1.BatchNorm2d_bn1                      [128]  [1, 128, 32, 216]   \n",
      "12_layer1.1.ReLU_relu                                -  [1, 128, 32, 216]   \n",
      "13_layer1.1.Conv2d_conv2              [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "14_layer1.1.BatchNorm2d_bn2                      [128]  [1, 128, 32, 216]   \n",
      "15_layer1.1.ReLU_relu                                -  [1, 128, 32, 216]   \n",
      "16_layer1.2.Conv2d_conv1              [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "17_layer1.2.BatchNorm2d_bn1                      [128]  [1, 128, 32, 216]   \n",
      "18_layer1.2.ReLU_relu                                -  [1, 128, 32, 216]   \n",
      "19_layer1.2.Conv2d_conv2              [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "20_layer1.2.BatchNorm2d_bn2                      [128]  [1, 128, 32, 216]   \n",
      "21_layer1.2.ReLU_relu                                -  [1, 128, 32, 216]   \n",
      "22_layer1.3.Conv2d_conv1              [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "23_layer1.3.BatchNorm2d_bn1                      [128]  [1, 128, 32, 216]   \n",
      "24_layer1.3.ReLU_relu                                -  [1, 128, 32, 216]   \n",
      "25_layer1.3.Conv2d_conv2              [128, 128, 3, 3]  [1, 128, 32, 216]   \n",
      "26_layer1.3.BatchNorm2d_bn2                      [128]  [1, 128, 32, 216]   \n",
      "27_layer1.3.ReLU_relu                                -  [1, 128, 32, 216]   \n",
      "28_layer2.0.Conv2d_conv1              [128, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "29_layer2.0.BatchNorm2d_bn1                      [256]  [1, 256, 16, 108]   \n",
      "30_layer2.0.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "31_layer2.0.Conv2d_conv2              [256, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "32_layer2.0.BatchNorm2d_bn2                      [256]  [1, 256, 16, 108]   \n",
      "33_layer2.0.downsample.Conv2d_0       [128, 256, 1, 1]  [1, 256, 16, 108]   \n",
      "34_layer2.0.downsample.BatchNorm2d_1             [256]  [1, 256, 16, 108]   \n",
      "35_layer2.0.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "36_layer2.1.Conv2d_conv1              [256, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "37_layer2.1.BatchNorm2d_bn1                      [256]  [1, 256, 16, 108]   \n",
      "38_layer2.1.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "39_layer2.1.Conv2d_conv2              [256, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "40_layer2.1.BatchNorm2d_bn2                      [256]  [1, 256, 16, 108]   \n",
      "41_layer2.1.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "42_layer2.2.Conv2d_conv1              [256, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "43_layer2.2.BatchNorm2d_bn1                      [256]  [1, 256, 16, 108]   \n",
      "44_layer2.2.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "45_layer2.2.Conv2d_conv2              [256, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "46_layer2.2.BatchNorm2d_bn2                      [256]  [1, 256, 16, 108]   \n",
      "47_layer2.2.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "48_layer2.3.Conv2d_conv1              [256, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "49_layer2.3.BatchNorm2d_bn1                      [256]  [1, 256, 16, 108]   \n",
      "50_layer2.3.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "51_layer2.3.Conv2d_conv2              [256, 256, 3, 3]  [1, 256, 16, 108]   \n",
      "52_layer2.3.BatchNorm2d_bn2                      [256]  [1, 256, 16, 108]   \n",
      "53_layer2.3.ReLU_relu                                -  [1, 256, 16, 108]   \n",
      "54_layer3.0.Conv2d_conv1              [256, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "55_layer3.0.BatchNorm2d_bn1                      [512]    [1, 512, 8, 54]   \n",
      "56_layer3.0.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "57_layer3.0.Conv2d_conv2              [512, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "58_layer3.0.BatchNorm2d_bn2                      [512]    [1, 512, 8, 54]   \n",
      "59_layer3.0.downsample.Conv2d_0       [256, 512, 1, 1]    [1, 512, 8, 54]   \n",
      "60_layer3.0.downsample.BatchNorm2d_1             [512]    [1, 512, 8, 54]   \n",
      "61_layer3.0.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "62_layer3.1.Conv2d_conv1              [512, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "63_layer3.1.BatchNorm2d_bn1                      [512]    [1, 512, 8, 54]   \n",
      "64_layer3.1.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "65_layer3.1.Conv2d_conv2              [512, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "66_layer3.1.BatchNorm2d_bn2                      [512]    [1, 512, 8, 54]   \n",
      "67_layer3.1.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "68_layer3.2.Conv2d_conv1              [512, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "69_layer3.2.BatchNorm2d_bn1                      [512]    [1, 512, 8, 54]   \n",
      "70_layer3.2.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "71_layer3.2.Conv2d_conv2              [512, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "72_layer3.2.BatchNorm2d_bn2                      [512]    [1, 512, 8, 54]   \n",
      "73_layer3.2.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "74_layer3.3.Conv2d_conv1              [512, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "75_layer3.3.BatchNorm2d_bn1                      [512]    [1, 512, 8, 54]   \n",
      "76_layer3.3.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "77_layer3.3.Conv2d_conv2              [512, 512, 3, 3]    [1, 512, 8, 54]   \n",
      "78_layer3.3.BatchNorm2d_bn2                      [512]    [1, 512, 8, 54]   \n",
      "79_layer3.3.ReLU_relu                                -    [1, 512, 8, 54]   \n",
      "80_avgpool                                           -     [1, 512, 1, 1]   \n",
      "81_fc                                        [512, 50]            [1, 50]   \n",
      "\n",
      "                                         Params     Mult-Adds  \n",
      "Layer                                                          \n",
      "0_conv1                                  3.456k    95.330304M  \n",
      "1_bn1                                     256.0         128.0  \n",
      "2_relu                                        -             -  \n",
      "3_maxpool                                     -             -  \n",
      "4_layer1.0.Conv2d_conv1                147.456k  1.019215872G  \n",
      "5_layer1.0.BatchNorm2d_bn1                256.0         128.0  \n",
      "6_layer1.0.ReLU_relu                          -             -  \n",
      "7_layer1.0.Conv2d_conv2                147.456k  1.019215872G  \n",
      "8_layer1.0.BatchNorm2d_bn2                256.0         128.0  \n",
      "9_layer1.0.ReLU_relu                          -             -  \n",
      "10_layer1.1.Conv2d_conv1               147.456k  1.019215872G  \n",
      "11_layer1.1.BatchNorm2d_bn1               256.0         128.0  \n",
      "12_layer1.1.ReLU_relu                         -             -  \n",
      "13_layer1.1.Conv2d_conv2               147.456k  1.019215872G  \n",
      "14_layer1.1.BatchNorm2d_bn2               256.0         128.0  \n",
      "15_layer1.1.ReLU_relu                         -             -  \n",
      "16_layer1.2.Conv2d_conv1               147.456k  1.019215872G  \n",
      "17_layer1.2.BatchNorm2d_bn1               256.0         128.0  \n",
      "18_layer1.2.ReLU_relu                         -             -  \n",
      "19_layer1.2.Conv2d_conv2               147.456k  1.019215872G  \n",
      "20_layer1.2.BatchNorm2d_bn2               256.0         128.0  \n",
      "21_layer1.2.ReLU_relu                         -             -  \n",
      "22_layer1.3.Conv2d_conv1               147.456k  1.019215872G  \n",
      "23_layer1.3.BatchNorm2d_bn1               256.0         128.0  \n",
      "24_layer1.3.ReLU_relu                         -             -  \n",
      "25_layer1.3.Conv2d_conv2               147.456k  1.019215872G  \n",
      "26_layer1.3.BatchNorm2d_bn2               256.0         128.0  \n",
      "27_layer1.3.ReLU_relu                         -             -  \n",
      "28_layer2.0.Conv2d_conv1               294.912k   509.607936M  \n",
      "29_layer2.0.BatchNorm2d_bn1               512.0         256.0  \n",
      "30_layer2.0.ReLU_relu                         -             -  \n",
      "31_layer2.0.Conv2d_conv2               589.824k  1.019215872G  \n",
      "32_layer2.0.BatchNorm2d_bn2               512.0         256.0  \n",
      "33_layer2.0.downsample.Conv2d_0         32.768k    56.623104M  \n",
      "34_layer2.0.downsample.BatchNorm2d_1      512.0         256.0  \n",
      "35_layer2.0.ReLU_relu                         -             -  \n",
      "36_layer2.1.Conv2d_conv1               589.824k  1.019215872G  \n",
      "37_layer2.1.BatchNorm2d_bn1               512.0         256.0  \n",
      "38_layer2.1.ReLU_relu                         -             -  \n",
      "39_layer2.1.Conv2d_conv2               589.824k  1.019215872G  \n",
      "40_layer2.1.BatchNorm2d_bn2               512.0         256.0  \n",
      "41_layer2.1.ReLU_relu                         -             -  \n",
      "42_layer2.2.Conv2d_conv1               589.824k  1.019215872G  \n",
      "43_layer2.2.BatchNorm2d_bn1               512.0         256.0  \n",
      "44_layer2.2.ReLU_relu                         -             -  \n",
      "45_layer2.2.Conv2d_conv2               589.824k  1.019215872G  \n",
      "46_layer2.2.BatchNorm2d_bn2               512.0         256.0  \n",
      "47_layer2.2.ReLU_relu                         -             -  \n",
      "48_layer2.3.Conv2d_conv1               589.824k  1.019215872G  \n",
      "49_layer2.3.BatchNorm2d_bn1               512.0         256.0  \n",
      "50_layer2.3.ReLU_relu                         -             -  \n",
      "51_layer2.3.Conv2d_conv2               589.824k  1.019215872G  \n",
      "52_layer2.3.BatchNorm2d_bn2               512.0         256.0  \n",
      "53_layer2.3.ReLU_relu                         -             -  \n",
      "54_layer3.0.Conv2d_conv1              1.179648M   509.607936M  \n",
      "55_layer3.0.BatchNorm2d_bn1              1.024k         512.0  \n",
      "56_layer3.0.ReLU_relu                         -             -  \n",
      "57_layer3.0.Conv2d_conv2              2.359296M  1.019215872G  \n",
      "58_layer3.0.BatchNorm2d_bn2              1.024k         512.0  \n",
      "59_layer3.0.downsample.Conv2d_0        131.072k    56.623104M  \n",
      "60_layer3.0.downsample.BatchNorm2d_1     1.024k         512.0  \n",
      "61_layer3.0.ReLU_relu                         -             -  \n",
      "62_layer3.1.Conv2d_conv1              2.359296M  1.019215872G  \n",
      "63_layer3.1.BatchNorm2d_bn1              1.024k         512.0  \n",
      "64_layer3.1.ReLU_relu                         -             -  \n",
      "65_layer3.1.Conv2d_conv2              2.359296M  1.019215872G  \n",
      "66_layer3.1.BatchNorm2d_bn2              1.024k         512.0  \n",
      "67_layer3.1.ReLU_relu                         -             -  \n",
      "68_layer3.2.Conv2d_conv1              2.359296M  1.019215872G  \n",
      "69_layer3.2.BatchNorm2d_bn1              1.024k         512.0  \n",
      "70_layer3.2.ReLU_relu                         -             -  \n",
      "71_layer3.2.Conv2d_conv2              2.359296M  1.019215872G  \n",
      "72_layer3.2.BatchNorm2d_bn2              1.024k         512.0  \n",
      "73_layer3.2.ReLU_relu                         -             -  \n",
      "74_layer3.3.Conv2d_conv1              2.359296M  1.019215872G  \n",
      "75_layer3.3.BatchNorm2d_bn1              1.024k         512.0  \n",
      "76_layer3.3.ReLU_relu                         -             -  \n",
      "77_layer3.3.Conv2d_conv2              2.359296M  1.019215872G  \n",
      "78_layer3.3.BatchNorm2d_bn2              1.024k         512.0  \n",
      "79_layer3.3.ReLU_relu                         -             -  \n",
      "80_avgpool                                    -             -  \n",
      "81_fc                                    25.65k         25.6k  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "                             Totals\n",
      "Total params             23.507122M\n",
      "Trainable params         23.507122M\n",
      "Non-trainable params            0.0\n",
      "Mult-Adds             23.650575232G\n",
      "==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "# input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "nb_conv = 0\n",
    "\n",
    "for layer in s.index.values:\n",
    "    if \"Conv\" in layer:\n",
    "        nb_conv += 1\n",
    "print(nb_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/esc50/supervised/2020-09-16_10:39:59_esc_wideresnet28_8_1.0S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1fS\" % (get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % (model_func.__name__, args.supervised_ratio)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))\n",
    "\n",
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_params = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    tensorboard_params[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard.add_hparams(tensorboard_params, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar10 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    def lr_lambda(e):\n",
    "        if e < 60:\n",
    "            return 1\n",
    "\n",
    "        elif 60 <= e < 120:\n",
    "            return 0.2\n",
    "\n",
    "        elif 120 <= e < 160:\n",
    "            return 0.04\n",
    "\n",
    "        else:\n",
    "            return 0.008\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ubs8k optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"ubs8k\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n",
    "\n",
    "elif args.dataset in (\"esc10\", 'esc50'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# Metrics\n",
    "fscore_fn = FScore()\n",
    "acc_fn = CategoricalAccuracy()\n",
    "avg = ContinueAverage()\n",
    "\n",
    "reset_metrics = lambda : [m.reset() for m in [fscore_fn, acc_fn, avg]]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f51ae780700>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, args.nb_epoch, args.nb_epoch)\n",
    "y = [lr_lambda(x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \".               \", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"metrics: \", \"acc\", \"F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        logits = model(X)        \n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_ce(logits, y)\n",
    "\n",
    "            # metrics\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=args.num_classes)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc\", acc, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc\", maximum_fn(\"acc\", acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/f1\", maximum_fn(\"f1\", fscore), epoch )\n",
    "\n",
    "    checkpoint.step(acc)\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".        Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n",
      "\n",
      "Training 1      - 100    -          3.5694 -           0.1000    | 0.0049   - 37.8175\n",
      "\u001b[1;4mValidati 1      - 100    -          30.2475 -           0.0469    | 0.0330   - 4.2603\u001b[0m\n",
      "Training 2      - 100    -          2.8543 -           0.1931    | 0.0408   - 31.2400\n",
      "\u001b[1;4mValidati 2      - 100    -          3.1391 -           0.2121    | 0.1149   - 2.3593\u001b[0m\n",
      "Training 3      - 100    -          2.4170 -           0.2906    | 0.1211   - 31.3548\n",
      "\u001b[1;4mValidati 3      - 100    -          7.3994 -           0.1183    | 0.0955   - 2.3466\u001b[0m\n",
      "Training 4      - 100    -          2.3202 -           0.3100    | 0.1670   - 31.3702\n",
      "\u001b[1;4mValidati 4      - 100    -          5.3933 -           0.2411    | 0.1942   - 2.3457\u001b[0m\n",
      "Training 5      - 100    -          2.0965 -           0.3738    | 0.2272   - 31.4858\n",
      "\u001b[1;4mValidati 5      - 100    -          9.3291 -           0.2522    | 0.1886   - 2.3429\u001b[0mm\n",
      "Training 6      - 100    -          1.8542 -           0.4325    | 0.2985   - 31.4608\n",
      "\u001b[1;4mValidati 6      - 100    -          3.5140 -           0.2701    | 0.2325   - 2.3502\u001b[0m\n",
      "Training 7      - 100    -          1.6175 -           0.5087    | 0.3887   - 31.3767\n",
      "\u001b[1;4mValidati 7      - 100    -          2.5702 -           0.3415    | 0.2768   - 2.3474\u001b[0m\n",
      "Training 8      - 100    -          1.4423 -           0.5494    | 0.4618   - 31.3326\n",
      "\u001b[1;4mValidati 8      - 100    -          2.4031 -           0.3795    | 0.3633   - 2.3492\u001b[0m\n",
      "Training 9      - 100    -          1.3638 -           0.5788    | 0.5155   - 31.3286\n",
      "\u001b[1;4mValidati 9      - 100    -          2.2866 -           0.4107    | 0.3639   - 2.3551\u001b[0m\n",
      "Training 10     - 100    -          1.2842 -           0.6250    | 0.5506   - 31.4093\n",
      "\u001b[1;4mValidati 10     - 100    -          2.4990 -           0.3571    | 0.3542   - 2.3489\u001b[0m\n",
      "Training 11     - 100    -          1.1776 -           0.6306    | 0.5888   - 31.3947\n",
      "\u001b[1;4mValidati 11     - 100    -          2.1502 -           0.4598    | 0.4338   - 2.3527\u001b[0m\n",
      "Training 12     - 100    -          1.1681 -           0.6350    | 0.6024   - 31.4922\n",
      "\u001b[1;4mValidati 12     - 100    -          12.5431 -           0.3527    | 0.3343   - 2.3602\u001b[0m\n",
      "Training 13     - 100    -          1.0156 -           0.6662    | 0.6350   - 31.3220\n",
      "\u001b[1;4mValidati 13     - 100    -          1.6524 -           0.5156    | 0.5018   - 2.3462\u001b[0m\n",
      "Training 14     - 100    -          0.9287 -           0.6969    | 0.6752   - 31.3365\n",
      "\u001b[1;4mValidati 14     - 100    -          2.1416 -           0.4821    | 0.4724   - 2.3563\u001b[0m\n",
      "Training 15     - 100    -          0.8050 -           0.7369    | 0.7168   - 31.3369\n",
      "\u001b[1;4mValidati 15     - 100    -          1.4941 -           0.6116    | 0.6066   - 2.3563\u001b[0m\n",
      "Training 16     - 100    -          0.6825 -           0.7825    | 0.7694   - 31.3317\n",
      "\u001b[1;4mValidati 16     - 100    -          1.7661 -           0.5469    | 0.5595   - 2.3556\u001b[0m\n",
      "Training 17     - 100    -          0.6345 -           0.7987    | 0.7713   - 31.3578\n",
      "\u001b[1;4mValidati 17     - 100    -          3.3293 -           0.4821    | 0.4811   - 2.3439\u001b[0m\n",
      "Training 18     - 100    -          0.6238 -           0.8062    | 0.7936   - 31.3975\n",
      "\u001b[1;4mValidati 18     - 100    -          1.6032 -           0.5871    | 0.5738   - 2.3422\u001b[0m\n",
      "Training 19     - 100    -          0.6405 -           0.7937    | 0.7805   - 31.2636\n",
      "\u001b[1;4mValidati 19     - 100    -          1.5243 -           0.6161    | 0.5913   - 2.3476\u001b[0m\n",
      "Training 20     - 100    -          0.5082 -           0.8381    | 0.8292   - 31.2865\n",
      "\u001b[1;4mValidati 20     - 100    -          1.5368 -           0.6250    | 0.6267   - 2.3365\u001b[0m\n",
      "Training 21     - 100    -          0.4766 -           0.8575    | 0.8434   - 31.3582\n",
      "\u001b[1;4mValidati 21     - 100    -          1.6259 -           0.5982    | 0.5828   - 2.3539\u001b[0m\n",
      "Training 22     - 100    -          0.3627 -           0.8950    | 0.8840   - 31.3642\n",
      "\u001b[1;4mValidati 22     - 100    -          1.4023 -           0.6250    | 0.6398   - 2.3539\u001b[0m\n",
      "Training 23     - 100    -          0.3248 -           0.9106    | 0.9039   - 31.3481\n",
      "\u001b[1;4mValidati 23     - 100    -          1.4417 -           0.6652    | 0.6665   - 2.3527\u001b[0m\n",
      "Training 24     - 100    -          0.3144 -           0.9056    | 0.8974   - 31.3425\n",
      "\u001b[1;4mValidati 24     - 100    -          1.9890 -           0.5759    | 0.5882   - 2.3485\u001b[0m\n",
      "Training 25     - 100    -          0.2792 -           0.9175    | 0.9144   - 31.3531\n",
      "\u001b[1;4mValidati 25     - 100    -          1.3144 -           0.6674    | 0.6832   - 2.3546\u001b[0m\n",
      "Training 26     - 100    -          0.2840 -           0.9200    | 0.9090   - 31.3243\n",
      "\u001b[1;4mValidati 26     - 100    -          3.3058 -           0.4955    | 0.4960   - 2.3569\u001b[0m\n",
      "Training 27     - 100    -          0.3320 -           0.9000    | 0.8933   - 31.3748\n",
      "\u001b[1;4mValidati 27     - 100    -          1.4024 -           0.6629    | 0.6723   - 2.3546\u001b[0m\n",
      "Training 28     - 100    -          0.2453 -           0.9306    | 0.9228   - 31.4824\n",
      "\u001b[1;4mValidati 28     - 100    -          1.7181 -           0.6138    | 0.6015   - 2.3496\u001b[0m\n",
      "Training 29     - 100    -          0.2821 -           0.9206    | 0.9125   - 31.4772\n",
      "\u001b[1;4mValidati 29     - 100    -          1.7203 -           0.6071    | 0.6211   - 2.3570\u001b[0m\n",
      "Training 30     - 100    -          0.2537 -           0.9237    | 0.9193   - 31.3610\n",
      "\u001b[1;4mValidati 30     - 100    -          1.7384 -           0.6518    | 0.6635   - 2.3523\u001b[0m\n",
      "Training 31     - 100    -          0.2582 -           0.9250    | 0.9262   - 31.3303\n",
      "\u001b[1;4mValidati 31     - 100    -          1.2007 -           0.6763    | 0.6818   - 2.3448\u001b[0m\n",
      "Training 32     - 100    -          0.1646 -           0.9556    | 0.9494   - 31.3059\n",
      "\u001b[1;4mValidati 32     - 100    -          1.3830 -           0.6741    | 0.6861   - 2.3453\u001b[0m\n",
      "Training 33     - 100    -          0.1249 -           0.9762    | 0.9702   - 31.3589\n",
      "\u001b[1;4mValidati 33     - 100    -          1.1934 -           0.6696    | 0.6893   - 2.3687\u001b[0m\n",
      "Training 34     - 100    -          0.0979 -           0.9819    | 0.9782   - 31.4795\n",
      "\u001b[1;4mValidati 34     - 100    -          1.5265 -           0.6830    | 0.6799   - 2.3602\u001b[0m\n",
      "Training 35     - 100    -          0.0949 -           0.9794    | 0.9760   - 31.5220\n",
      "\u001b[1;4mValidati 35     - 100    -          1.2261 -           0.7098    | 0.7232   - 2.3470\u001b[0m\n",
      "Training 36     - 100    -          0.0956 -           0.9787    | 0.9747   - 31.3473\n",
      "\u001b[1;4mValidati 36     - 100    -          1.3351 -           0.6987    | 0.6861   - 2.3563\u001b[0m\n",
      "Training 37     - 100    -          0.0964 -           0.9806    | 0.9778   - 31.2851\n",
      "\u001b[1;4mValidati 37     - 100    -          1.1213 -           0.7031    | 0.7146   - 2.3472\u001b[0m\n",
      "Training 38     - 100    -          0.0661 -           0.9869    | 0.9862   - 31.2758\n",
      "\u001b[1;4mValidati 38     - 100    -          1.3111 -           0.6853    | 0.6934   - 2.3604\u001b[0m\n",
      "Training 39     - 100    -          0.0670 -           0.9856    | 0.9874   - 31.3529\n",
      "\u001b[1;4mValidati 39     - 100    -          1.4242 -           0.6652    | 0.6800   - 2.3496\u001b[0m\n",
      "Training 40     - 100    -          0.0481 -           0.9931    | 0.9918   - 31.3783\n",
      "\u001b[1;4mValidati 40     - 100    -          1.1658 -           0.7009    | 0.7191   - 2.3558\u001b[0m\n",
      "Training 41     - 100    -          0.0406 -           0.9937    | 0.9937   - 31.3641\n",
      "\u001b[1;4mValidati 41     - 100    -          1.2500 -           0.7210    | 0.7045   - 2.3536\u001b[0m\n",
      "Training 42     - 100    -          0.0497 -           0.9906    | 0.9906   - 31.4324\n",
      "\u001b[1;4mValidati 42     - 100    -          1.3336 -           0.6897    | 0.6955   - 2.3528\u001b[0m\n",
      "Training 43     - 100    -          0.0561 -           0.9906    | 0.9893   - 31.3595\n",
      "\u001b[1;4mValidati 43     - 100    -          1.5923 -           0.6384    | 0.6541   - 2.3540\u001b[0m\n",
      "Training 44     - 100    -          0.0569 -           0.9900    | 0.9905   - 31.3651\n",
      "\u001b[1;4mValidati 44     - 100    -          1.0924 -           0.7188    | 0.7304   - 2.3528\u001b[0m\n",
      "Training 45     - 100    -          0.0375 -           0.9937    | 0.9944   - 31.3352\n",
      "\u001b[1;4mValidati 45     - 100    -          1.3465 -           0.7232    | 0.7131   - 2.3487\u001b[0m\n",
      "Training 46     - 100    -          0.0243 -           0.9987    | 0.9981   - 31.5805\n",
      "\u001b[1;4mValidati 46     - 100    -          1.2107 -           0.7121    | 0.7149   - 2.3483\u001b[0m\n",
      "Training 47     - 100    -          0.0173 -           1.0000    | 0.9997   - 31.3625\n",
      "\u001b[1;4mValidati 47     - 100    -          1.2235 -           0.7299    | 0.7467   - 2.3517\u001b[0m\n",
      "Training 48     - 100    -          0.0215 -           0.9969    | 0.9966   - 31.3969\n",
      "\u001b[1;4mValidati 48     - 100    -          1.1898 -           0.7388    | 0.7587   - 2.3603\u001b[0m\n",
      "Training 49     - 100    -          0.0288 -           0.9950    | 0.9937   - 31.3417\n",
      "\u001b[1;4mValidati 49     - 100    -          1.2844 -           0.6987    | 0.7219   - 2.3474\u001b[0m\n",
      "Training 50     - 100    -          0.0881 -           0.9744    | 0.9745   - 31.3652\n",
      "\u001b[1;4mValidati 50     - 100    -          3.0622 -           0.4933    | 0.4998   - 2.3532\u001b[0m\n",
      "Training 51     - 100    -          0.2172 -           0.9394    | 0.9353   - 31.3564\n",
      "\u001b[1;4mValidati 51     - 100    -          2.6364 -           0.5335    | 0.5363   - 2.3543\u001b[0m\n",
      "Training 52     - 100    -          0.3137 -           0.9038    | 0.9038   - 31.3216\n",
      "\u001b[1;4mValidati 52     - 100    -          2.8255 -           0.5402    | 0.5485   - 2.3477\u001b[0m\n",
      "Training 53     - 100    -          0.2648 -           0.9131    | 0.9162   - 31.4574\n",
      "\u001b[1;4mValidati 53     - 100    -          3.4306 -           0.5379    | 0.5323   - 2.3623\u001b[0m\n",
      "Training 54     - 100    -          0.2393 -           0.9344    | 0.9338   - 31.4673\n",
      "\u001b[1;4mValidati 54     - 100    -          1.5857 -           0.6317    | 0.6291   - 2.3571\u001b[0m\n",
      "Training 55     - 100    -          0.1681 -           0.9550    | 0.9537   - 31.4749\n",
      "\u001b[1;4mValidati 55     - 100    -          1.9405 -           0.5938    | 0.6028   - 2.3739\u001b[0m\n",
      "Training 56     - 100    -          0.1127 -           0.9675    | 0.9664   - 31.4639\n",
      "\u001b[1;4mValidati 56     - 100    -          1.8896 -           0.6540    | 0.6637   - 2.3538\u001b[0m\n",
      "Training 57     - 100    -          0.0727 -           0.9844    | 0.9843   - 31.5240\n",
      "\u001b[1;4mValidati 57     - 100    -          1.3373 -           0.6964    | 0.7092   - 2.3528\u001b[0m\n",
      "Training 58     - 100    -          0.0342 -           0.9962    | 0.9950   - 31.3796\n",
      "\u001b[1;4mValidati 58     - 100    -          1.2252 -           0.7210    | 0.7059   - 2.3584\u001b[0m\n",
      "Training 59     - 100    -          0.0210 -           0.9987    | 0.9981   - 31.3625\n",
      "\u001b[1;4mValidati 59     - 100    -          0.9922 -           0.7790    | 0.7632   - 2.3541\u001b[0m\n",
      "Training 60     - 100    -          0.0147 -           1.0000    | 0.9994   - 31.4051\n",
      "\u001b[1;4mValidati 60     - 100    -          0.9159 -           0.7790    | 0.7792   - 2.3571\u001b[0m\n",
      "Training 61     - 100    -          0.0099 -           1.0000    | 1.0000   - 31.3723\n",
      "\u001b[1;4mValidati 61     - 100    -          1.1050 -           0.7254    | 0.7323   - 2.3574\u001b[0m\n",
      "Training 62     - 100    -          0.0089 -           1.0000    | 1.0000   - 31.3766\n",
      "\u001b[1;4mValidati 62     - 100    -          1.0099 -           0.7768    | 0.7762   - 2.3563\u001b[0m\n",
      "Training 63     - 100    -          0.0105 -           0.9987    | 0.9991   - 31.5789\n",
      "\u001b[1;4mValidati 63     - 100    -          1.0350 -           0.7478    | 0.7532   - 2.3516\u001b[0m\n",
      "Training 64     - 100    -          0.0092 -           1.0000    | 1.0000   - 31.3570\n",
      "\u001b[1;4mValidati 64     - 100    -          0.9755 -           0.7522    | 0.7682   - 2.3529\u001b[0m\n",
      "Training 65     - 100    -          0.0117 -           0.9975    | 0.9975   - 31.3582\n",
      "\u001b[1;4mValidati 65     - 100    -          1.0119 -           0.7366    | 0.7533   - 2.3583\u001b[0m\n",
      "Training 66     - 100    -          0.0089 -           1.0000    | 1.0000   - 31.5291\n",
      "\u001b[1;4mValidati 66     - 100    -          0.9370 -           0.7612    | 0.7716   - 2.3474\u001b[0m\n",
      "Training 67     - 100    -          0.0140 -           0.9981    | 0.9981   - 31.5142\n",
      "\u001b[1;4mValidati 67     - 100    -          1.1225 -           0.7076    | 0.7249   - 2.3601\u001b[0m\n",
      "Training 68     - 100    -          0.0086 -           1.0000    | 1.0000   - 31.3633\n",
      "\u001b[1;4mValidati 68     - 100    -          1.0751 -           0.7411    | 0.7457   - 2.3592\u001b[0m\n",
      "Training 69     - 100    -          0.0061 -           1.0000    | 1.0000   - 31.3667\n",
      "\u001b[1;4mValidati 69     - 100    -          1.0288 -           0.7455    | 0.7612   - 2.3565\u001b[0m\n",
      "Training 70     - 100    -          0.0049 -           1.0000    | 1.0000   - 31.3736\n",
      "\u001b[1;4mValidati 70     - 100    -          0.9920 -           0.7634    | 0.7831   - 2.3578\u001b[0m\n",
      "Training 71     - 100    -          0.0047 -           1.0000    | 1.0000   - 31.3951\n",
      "\u001b[1;4mValidati 71     - 100    -          1.0101 -           0.7612    | 0.7681   - 2.3531\u001b[0m\n",
      "Training 72     - 100    -          0.0053 -           1.0000    | 1.0000   - 31.5156\n",
      "\u001b[1;4mValidati 72     - 100    -          1.1148 -           0.7254    | 0.7343   - 2.3527\u001b[0m\n",
      "Training 73     - 100    -          0.0049 -           1.0000    | 1.0000   - 31.3511\n",
      "\u001b[1;4mValidati 73     - 100    -          0.9797 -           0.7656    | 0.7646   - 2.3637\u001b[0m\n",
      "Training 74     - 100    -          0.0034 -           1.0000    | 1.0000   - 31.3455\n",
      "\u001b[1;4mValidati 74     - 100    -          1.0307 -           0.7545    | 0.7622   - 2.3610\u001b[0m\n",
      "Training 75     - 100    -          0.0033 -           1.0000    | 1.0000   - 31.4157\n",
      "\u001b[1;4mValidati 75     - 100    -          1.0219 -           0.7545    | 0.7613   - 2.3517\u001b[0m\n",
      "Training 76     - 100    -          0.0035 -           1.0000    | 1.0000   - 31.3738\n",
      "\u001b[1;4mValidati 76     - 100    -          1.0964 -           0.7321    | 0.7325   - 2.3507\u001b[0m\n",
      "Training 77     - 100    -          0.0029 -           1.0000    | 1.0000   - 31.3699\n",
      "\u001b[1;4mValidati 77     - 100    -          1.0933 -           0.7232    | 0.7388   - 2.3543\u001b[0m\n",
      "Training 78     - 100    -          0.0029 -           1.0000    | 1.0000   - 31.3544\n",
      "\u001b[1;4mValidati 78     - 100    -          1.0014 -           0.7612    | 0.7679   - 2.3580\u001b[0m\n",
      "Training 79     - 100    -          0.0031 -           1.0000    | 1.0000   - 31.4929\n",
      "\u001b[1;4mValidati 79     - 100    -          0.9576 -           0.7813    | 0.7834   - 2.3491\u001b[0m\n",
      "Training 80     - 100    -          0.0027 -           1.0000    | 1.0000   - 31.3707\n",
      "\u001b[1;4mValidati 80     - 100    -          1.0192 -           0.7478    | 0.7521   - 2.3556\u001b[0m\n",
      "Training 81     - 100    -          0.0028 -           1.0000    | 1.0000   - 31.5772\n",
      "\u001b[1;4mValidati 81     - 100    -          0.9938 -           0.7545    | 0.7589   - 2.3521\u001b[0m\n",
      "Training 82     - 100    -          0.0027 -           1.0000    | 1.0000   - 31.3846\n",
      "\u001b[1;4mValidati 82     - 100    -          0.9599 -           0.7701    | 0.7833   - 2.3528\u001b[0m\n",
      "Training 83     - 100    -          0.0024 -           1.0000    | 1.0000   - 31.3713\n",
      "\u001b[1;4mValidati 83     - 100    -          1.0309 -           0.7589    | 0.7556   - 2.3589\u001b[0m\n",
      "Training 84     - 100    -          0.0026 -           1.0000    | 1.0000   - 31.3554\n",
      "\u001b[1;4mValidati 84     - 100    -          0.9842 -           0.7634    | 0.7667   - 2.3621\u001b[0m\n",
      "Training 85     - 100    -          0.0023 -           1.0000    | 1.0000   - 31.3323\n",
      "\u001b[1;4mValidati 85     - 100    -          0.9476 -           0.7813    | 0.7804   - 2.3527\u001b[0m\n",
      "Training 86     - 100    -          0.0025 -           1.0000    | 1.0000   - 31.3775\n",
      "\u001b[1;4mValidati 86     - 100    -          1.1379 -           0.7545    | 0.7605   - 2.3473\u001b[0m\n",
      "Training 87     - 100    -          0.0023 -           1.0000    | 1.0000   - 31.3665\n",
      "\u001b[1;4mValidati 87     - 100    -          1.0179 -           0.7612    | 0.7712   - 2.3553\u001b[0m\n",
      "Training 88     - 100    -          0.0024 -           1.0000    | 1.0000   - 31.3379\n",
      "\u001b[1;4mValidati 88     - 100    -          1.0001 -           0.7589    | 0.7684   - 2.3562\u001b[0m\n",
      "Training 89     - 100    -          0.0022 -           1.0000    | 1.0000   - 31.3359\n",
      "\u001b[1;4mValidati 89     - 100    -          1.0736 -           0.7344    | 0.7349   - 2.3499\u001b[0m\n",
      "Training 90     - 100    -          0.0020 -           1.0000    | 1.0000   - 31.3642\n",
      "\u001b[1;4mValidati 90     - 100    -          1.0171 -           0.7545    | 0.7638   - 2.3571\u001b[0m\n",
      "Training 91     - 100    -          0.0016 -           1.0000    | 1.0000   - 31.3432\n",
      "\u001b[1;4mValidati 91     - 100    -          1.0118 -           0.7612    | 0.7638   - 2.3624\u001b[0m\n",
      "Training 92     - 100    -          0.0018 -           1.0000    | 1.0000   - 31.3589\n",
      "\u001b[1;4mValidati 92     - 100    -          1.0117 -           0.7567    | 0.7653   - 2.3563\u001b[0m\n",
      "Training 93     - 100    -          0.0027 -           1.0000    | 1.0000   - 31.3526\n",
      "\u001b[1;4mValidati 93     - 100    -          1.0368 -           0.7545    | 0.7554   - 2.3510\u001b[0m\n",
      "Training 94     - 100    -          0.0023 -           1.0000    | 1.0000   - 31.4960\n",
      "\u001b[1;4mValidati 94     - 100    -          1.0797 -           0.7500    | 0.7483   - 2.3538\u001b[0m\n",
      "Training 95     - 100    -          0.0019 -           1.0000    | 1.0000   - 31.3549\n",
      "\u001b[1;4mValidati 95     - 100    -          1.0672 -           0.7634    | 0.7732   - 2.3533\u001b[0m\n",
      "Training 96     - 100    -          0.0019 -           1.0000    | 1.0000   - 31.3687\n",
      "\u001b[1;4mValidati 96     - 100    -          1.0593 -           0.7500    | 0.7564   - 2.3722\u001b[0m\n",
      "Training 97     - 100    -          0.0015 -           1.0000    | 1.0000   - 31.5239\n",
      "\u001b[1;4mValidati 97     - 100    -          1.0932 -           0.7656    | 0.7630   - 2.3467\u001b[0m\n",
      "Training 98     - 100    -          0.0032 -           0.9994    | 0.9994   - 31.4049\n",
      "\u001b[1;4mValidati 98     - 100    -          1.1406 -           0.7433    | 0.7490   - 2.3562\u001b[0m\n",
      "Training 99     - 100    -          0.0141 -           0.9962    | 0.9959   - 31.3861\n",
      "\u001b[1;4mValidati 99     - 100    -          1.2190 -           0.7455    | 0.7505   - 2.3544\u001b[0m\n",
      "Training 100    - 100    -          0.0254 -           0.9937    | 0.9941   - 31.3381\n",
      "\u001b[1;4mValidati 100    - 100    -          1.5279 -           0.6763    | 0.6791   - 2.3614\u001b[0m\n",
      "Training 101    - 100    -          0.0669 -           0.9800    | 0.9802   - 31.3443\n",
      "\u001b[1;4mValidati 101    - 100    -          1.7822 -           0.6339    | 0.6478   - 2.3544\u001b[0m\n",
      "Training 102    - 100    -          0.0923 -           0.9719    | 0.9717   - 31.4956\n",
      "\u001b[1;4mValidati 102    - 100    -          2.4890 -           0.6116    | 0.6147   - 2.3471\u001b[0m\n",
      "Training 103    - 100    -          0.0892 -           0.9762    | 0.9746   - 31.3100\n",
      "\u001b[1;4mValidati 103    - 100    -          1.6822 -           0.6496    | 0.6578   - 2.3576\u001b[0m\n",
      "Training 104    - 100    -          0.1446 -           0.9594    | 0.9597   - 31.3371\n",
      "\u001b[1;4mValidati 104    - 100    -          1.9040 -           0.6272    | 0.6330   - 2.3412\u001b[0m\n",
      "Training 105    - 100    -          0.0755 -           0.9806    | 0.9771   - 31.3412\n",
      "\u001b[1;4mValidati 105    - 100    -          1.4558 -           0.6786    | 0.6827   - 2.3533\u001b[0m\n",
      "Training 106    - 100    -          0.0595 -           0.9869    | 0.9859   - 31.3819\n",
      "\u001b[1;4mValidati 106    - 100    -          1.4950 -           0.7031    | 0.6964   - 2.3592\u001b[0m\n",
      "Training 107    - 100    -          0.0338 -           0.9925    | 0.9918   - 31.5274\n",
      "\u001b[1;4mValidati 107    - 100    -          1.2995 -           0.7076    | 0.7203   - 2.3574\u001b[0m\n",
      "Training 108    - 100    -          0.0211 -           0.9944    | 0.9950   - 31.4487\n",
      "\u001b[1;4mValidati 108    - 100    -          1.2319 -           0.7277    | 0.7316   - 2.3557\u001b[0m\n",
      "Training 109    - 100    -          0.0217 -           0.9944    | 0.9944   - 31.3763\n",
      "\u001b[1;4mValidati 109    - 100    -          1.0432 -           0.7522    | 0.7657   - 2.3599\u001b[0m\n",
      "Training 110    - 100    -          0.0127 -           0.9981    | 0.9981   - 31.3384\n",
      "\u001b[1;4mValidati 110    - 100    -          1.1256 -           0.7254    | 0.7351   - 2.3576\u001b[0m\n",
      "Training 111    - 100    -          0.0071 -           1.0000    | 0.9994   - 31.3656\n",
      "\u001b[1;4mValidati 111    - 100    -          1.0431 -           0.7612    | 0.7684   - 2.3483\u001b[0m\n",
      "Training 112    - 100    -          0.0066 -           0.9994    | 0.9994   - 31.4069\n",
      "\u001b[1;4mValidati 112    - 100    -          1.1298 -           0.7411    | 0.7292   - 2.3595\u001b[0m\n",
      "Training 113    - 100    -          0.0035 -           1.0000    | 1.0000   - 31.3721\n",
      "\u001b[1;4mValidati 113    - 100    -          0.9115 -           0.7813    | 0.7759   - 2.3580\u001b[0m\n",
      "Training 114    - 100    -          0.0030 -           1.0000    | 1.0000   - 31.3715\n",
      "\u001b[1;4mValidati 114    - 100    -          0.8811 -           0.7790    | 0.7845   - 2.3598\u001b[0m\n",
      "Training 115    - 100    -          0.0027 -           1.0000    | 1.0000   - 31.3721\n",
      "\u001b[1;4mValidati 115    - 100    -          0.9562 -           0.7634    | 0.7651   - 2.3536\u001b[0m\n",
      "Training 116    - 100    -          0.0027 -           1.0000    | 1.0000   - 31.3734\n",
      "\u001b[1;4mValidati 116    - 100    -          0.9621 -           0.7634    | 0.7645   - 2.3586\u001b[0m\n",
      "Training 117    - 100    -          0.0022 -           1.0000    | 1.0000   - 31.3657\n",
      "\u001b[1;4mValidati 117    - 100    -          1.0189 -           0.7522    | 0.7484   - 2.3549\u001b[0m\n",
      "Training 118    - 100    -          0.0023 -           1.0000    | 1.0000   - 31.5043\n",
      "\u001b[1;4mValidati 118    - 100    -          0.9342 -           0.7835    | 0.7818   - 2.3699\u001b[0m\n",
      "Training 119    - 100    -          0.0024 -           1.0000    | 1.0000   - 31.4576\n",
      "\u001b[1;4mValidati 119    - 100    -          0.9948 -           0.7701    | 0.7684   - 2.3444\u001b[0m\n",
      "Training 120    - 100    -          0.0022 -           1.0000    | 1.0000   - 31.3487\n",
      "\u001b[1;4mValidati 120    - 100    -          1.0309 -           0.7656    | 0.7634   - 2.3592\u001b[0m\n",
      "Training 121    - 100    -          0.0021 -           1.0000    | 1.0000   - 31.3705\n",
      "\u001b[1;4mValidati 121    - 100    -          1.0112 -           0.7500    | 0.7532   - 2.3576\u001b[0m\n",
      "Training 122    - 100    -          0.0020 -           1.0000    | 1.0000   - 31.3816\n",
      "\u001b[1;4mValidati 122    - 100    -          0.8824 -           0.7857    | 0.7819   - 2.3549\u001b[0m\n",
      "Training 123    - 100    -          0.0021 -           1.0000    | 1.0000   - 31.3648\n",
      "\u001b[1;4mValidati 123    - 100    -          0.9040 -           0.7746    | 0.7759   - 2.3597\u001b[0m\n",
      "Training 124    - 100    -          0.0019 -           1.0000    | 1.0000   - 31.3687\n",
      "\u001b[1;4mValidati 124    - 100    -          0.8880 -           0.7723    | 0.7863   - 2.3562\u001b[0m\n",
      "Training 125    - 100    -          0.0015 -           1.0000    | 1.0000   - 31.3885\n",
      "\u001b[1;4mValidati 125    - 100    -          0.9282 -           0.7612    | 0.7750   - 2.3620\u001b[0m\n",
      "Training 126    - 100    -          0.0018 -           1.0000    | 1.0000   - 31.3544\n",
      "\u001b[1;4mValidati 126    - 100    -          0.9663 -           0.7679    | 0.7672   - 2.3516\u001b[0m\n",
      "Training 127    - 100    -          0.0014 -           1.0000    | 1.0000   - 31.3871\n",
      "\u001b[1;4mValidati 127    - 100    -          1.0531 -           0.7522    | 0.7473   - 2.3506\u001b[0m\n",
      "Training 128    - 100    -          0.0016 -           1.0000    | 1.0000   - 31.3684\n",
      "\u001b[1;4mValidati 128    - 100    -          0.9420 -           0.7746    | 0.7681   - 2.3544\u001b[0m\n",
      "Training 129    - 100    -          0.0018 -           1.0000    | 1.0000   - 31.4678\n",
      "\u001b[1;4mValidati 129    - 100    -          0.8821 -           0.7723    | 0.7798   - 2.3594\u001b[0m\n",
      "Training 130    - 100    -          0.0019 -           1.0000    | 1.0000   - 31.3757\n",
      "\u001b[1;4mValidati 130    - 100    -          0.9104 -           0.7790    | 0.7761   - 2.3704\u001b[0m\n",
      "Training 131    - 100    -          0.0016 -           1.0000    | 1.0000   - 31.3728\n",
      "\u001b[1;4mValidati 131    - 100    -          0.9364 -           0.7656    | 0.7722   - 2.3631\u001b[0m\n",
      "Training 132    - 100    -          0.0019 -           1.0000    | 1.0000   - 31.3447\n",
      "\u001b[1;4mValidati 132    - 100    -          1.0064 -           0.7366    | 0.7398   - 2.3639\u001b[0m\n",
      "Training 133    - 100    -          0.0015 -           1.0000    | 1.0000   - 31.5194\n",
      "\u001b[1;4mValidati 133    - 100    -          0.9385 -           0.7857    | 0.7793   - 2.3484\u001b[0m\n",
      "Training 134    - 100    -          0.0016 -           1.0000    | 1.0000   - 31.5128\n",
      "\u001b[1;4mValidati 134    - 100    -          0.9593 -           0.7656    | 0.7771   - 2.3589\u001b[0m\n",
      "Training 135    - 100    -          0.0016 -           1.0000    | 1.0000   - 31.2701\n",
      "\u001b[1;4mValidati 135    - 100    -          1.0167 -           0.7656    | 0.7650   - 2.3531\u001b[0m\n",
      "Training 136    - 100    -          0.0013 -           1.0000    | 1.0000   - 31.3328\n",
      "\u001b[1;4mValidati 136    - 100    -          0.9435 -           0.7701    | 0.7828   - 2.3526\u001b[0m\n",
      "Training 137    - 100    -          0.0012 -           1.0000    | 1.0000   - 31.3977\n",
      "\u001b[1;4mValidati 137    - 100    -          0.9750 -           0.7679    | 0.7707   - 2.3524\u001b[0m\n",
      "Training 138    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.4903\n",
      "\u001b[1;4mValidati 138    - 100    -          0.8866 -           0.7790    | 0.7858   - 2.3559\u001b[0m\n",
      "Training 139    - 100    -          0.0012 -           1.0000    | 1.0000   - 31.3600\n",
      "\u001b[1;4mValidati 139    - 100    -          1.0188 -           0.7612    | 0.7550   - 2.3556\u001b[0m\n",
      "Training 140    - 100    -          0.0012 -           1.0000    | 1.0000   - 31.4562\n",
      "\u001b[1;4mValidati 140    - 100    -          1.0623 -           0.7455    | 0.7519   - 2.3488\u001b[0m\n",
      "Training 141    - 100    -          0.0013 -           1.0000    | 1.0000   - 31.3677\n",
      "\u001b[1;4mValidati 141    - 100    -          0.8750 -           0.7879    | 0.7917   - 2.3978\u001b[0m\n",
      "Training 142    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.4517\n",
      "\u001b[1;4mValidati 142    - 100    -          0.9653 -           0.7679    | 0.7725   - 2.3562\u001b[0m\n",
      "Training 143    - 100    -          0.0010 -           1.0000    | 1.0000   - 31.3552\n",
      "\u001b[1;4mValidati 143    - 100    -          0.9210 -           0.7723    | 0.7780   - 2.3594\u001b[0m\n",
      "Training 144    - 100    -          0.0012 -           1.0000    | 1.0000   - 31.3668\n",
      "\u001b[1;4mValidati 144    - 100    -          0.9203 -           0.7634    | 0.7687   - 2.3410\u001b[0m\n",
      "Training 145    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.3713\n",
      "\u001b[1;4mValidati 145    - 100    -          0.9566 -           0.7746    | 0.7784   - 2.3547\u001b[0m\n",
      "Training 146    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.3658\n",
      "\u001b[1;4mValidati 146    - 100    -          0.9585 -           0.7723    | 0.7805   - 2.3559\u001b[0m\n",
      "Training 147    - 100    -          0.0010 -           1.0000    | 1.0000   - 31.4074\n",
      "\u001b[1;4mValidati 147    - 100    -          0.8904 -           0.7723    | 0.7788   - 2.3536\u001b[0m\n",
      "Training 148    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.3782\n",
      "\u001b[1;4mValidati 148    - 100    -          0.9303 -           0.7656    | 0.7657   - 2.3489\u001b[0m\n",
      "Training 149    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.3307\n",
      "\u001b[1;4mValidati 149    - 100    -          1.1097 -           0.7589    | 0.7681   - 2.3565\u001b[0m\n",
      "Training 150    - 100    -          0.0010 -           1.0000    | 1.0000   - 31.3413\n",
      "\u001b[1;4mValidati 150    - 100    -          1.0261 -           0.7723    | 0.7734   - 2.3513\u001b[0m\n",
      "Training 151    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.4140\n",
      "\u001b[1;4mValidati 151    - 100    -          1.0343 -           0.7545    | 0.7574   - 2.3517\u001b[0m\n",
      "Training 152    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.4854\n",
      "\u001b[1;4mValidati 152    - 100    -          0.9737 -           0.7612    | 0.7679   - 2.3460\u001b[0m\n",
      "Training 153    - 100    -          0.0010 -           1.0000    | 1.0000   - 31.4483\n",
      "\u001b[1;4mValidati 153    - 100    -          0.9702 -           0.7701    | 0.7727   - 2.3556\u001b[0m\n",
      "Training 154    - 100    -          0.0009 -           1.0000    | 1.0000   - 31.4655\n",
      "\u001b[1;4mValidati 154    - 100    -          0.9410 -           0.7723    | 0.7755   - 2.3561\u001b[0m\n",
      "Training 155    - 100    -          0.0010 -           1.0000    | 1.0000   - 31.3484\n",
      "\u001b[1;4mValidati 155    - 100    -          0.9152 -           0.7835    | 0.7896   - 2.3508\u001b[0m\n",
      "Training 156    - 100    -          0.0011 -           1.0000    | 1.0000   - 31.4892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9f5b0b822428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-0e92588afe60>\u001b[0m in \u001b[0;36mval\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpred_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    \n",
    "    tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â™«â™ª.Ä±lÄ±lÄ±ll|Ì…Ì²Ì…â—Ì…Ì²Ì…|Ì…Ì²Ì…=Ì…Ì²Ì…|Ì…Ì²Ì…â—Ì…Ì²Ì…|llÄ±lÄ±lÄ±.â™«â™ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
