{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "from DCT.util.utils import reset_seed, get_datetime,\n",
    "from DCT.util.model_loader import get_model_from_name\n",
    "from DCT.util.dataset_loader import load_dataset\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"cifar10\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"-m\", \"--model\", default=\"P_model\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=5000, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.0005, type=float)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_u = parser.add_argument_group(\"UrbanSound8k parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"supervised\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=128, checkpoint_path='../model_save/cifar10/full_supervised', dataset='cifar10', dataset_root='../datasets', learning_rate=0.003, model='wideresnet28_2', nb_epoch=300, resume=False, supervised_ratio=1.0, tensorboard_path='../tensorboard/cifar10/full_supervised', tensorboard_sufix='', train_folds=[1, 2, 3, 4, 5, 6, 7, 8, 9], val_folds=[10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "we pre-processed the images using ZCA and augmented the dataset using horizontal flips and random translations. The translations\n",
    "were drawn from [âˆ’2, 2] pixels,\n",
    "\"\"\"\n",
    "extra_train_transforms = [\n",
    "    transforms.Pad(4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "]\n",
    "\n",
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"supervised\",\n",
    "    \n",
    "    extra_train_transform = extra_train_transforms,\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "model_func = get_model_from_name(args.model)\n",
    "# model = ResNet(tm.resnet.Bottleneck, [2, 2, 2, 2], num_classes=10)\n",
    "model = model_func()\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "                                          Kernel Shape      Output Shape  \\\n",
      "Layer                                                                      \n",
      "0_conv1                                  [3, 32, 3, 3]  [64, 32, 32, 32]   \n",
      "1_bn1                                             [32]  [64, 32, 32, 32]   \n",
      "2_relu                                               -  [64, 32, 32, 32]   \n",
      "3_maxpool                                            -  [64, 32, 16, 16]   \n",
      "4_layer1.0.Conv2d_conv1                 [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "5_layer1.0.BatchNorm2d_bn1                        [32]  [64, 32, 16, 16]   \n",
      "6_layer1.0.ReLU_relu                                 -  [64, 32, 16, 16]   \n",
      "7_layer1.0.Conv2d_conv2                 [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "8_layer1.0.BatchNorm2d_bn2                        [32]  [64, 32, 16, 16]   \n",
      "9_layer1.0.ReLU_relu                                 -  [64, 32, 16, 16]   \n",
      "10_layer1.1.Conv2d_conv1                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "11_layer1.1.BatchNorm2d_bn1                       [32]  [64, 32, 16, 16]   \n",
      "12_layer1.1.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "13_layer1.1.Conv2d_conv2                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "14_layer1.1.BatchNorm2d_bn2                       [32]  [64, 32, 16, 16]   \n",
      "15_layer1.1.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "16_layer1.2.Conv2d_conv1                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "17_layer1.2.BatchNorm2d_bn1                       [32]  [64, 32, 16, 16]   \n",
      "18_layer1.2.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "19_layer1.2.Conv2d_conv2                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "20_layer1.2.BatchNorm2d_bn2                       [32]  [64, 32, 16, 16]   \n",
      "21_layer1.2.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "22_layer1.3.Conv2d_conv1                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "23_layer1.3.BatchNorm2d_bn1                       [32]  [64, 32, 16, 16]   \n",
      "24_layer1.3.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "25_layer1.3.Conv2d_conv2                [32, 32, 3, 3]  [64, 32, 16, 16]   \n",
      "26_layer1.3.BatchNorm2d_bn2                       [32]  [64, 32, 16, 16]   \n",
      "27_layer1.3.ReLU_relu                                -  [64, 32, 16, 16]   \n",
      "28_layer2.0.Conv2d_conv1                [32, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "29_layer2.0.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "30_layer2.0.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "31_layer2.0.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "32_layer2.0.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "33_layer2.0.downsample.Conv2d_0         [32, 64, 1, 1]    [64, 64, 8, 8]   \n",
      "34_layer2.0.downsample.BatchNorm2d_1              [64]    [64, 64, 8, 8]   \n",
      "35_layer2.0.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "36_layer2.1.Conv2d_conv1                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "37_layer2.1.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "38_layer2.1.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "39_layer2.1.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "40_layer2.1.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "41_layer2.1.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "42_layer2.2.Conv2d_conv1                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "43_layer2.2.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "44_layer2.2.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "45_layer2.2.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "46_layer2.2.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "47_layer2.2.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "48_layer2.3.Conv2d_conv1                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "49_layer2.3.BatchNorm2d_bn1                       [64]    [64, 64, 8, 8]   \n",
      "50_layer2.3.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "51_layer2.3.Conv2d_conv2                [64, 64, 3, 3]    [64, 64, 8, 8]   \n",
      "52_layer2.3.BatchNorm2d_bn2                       [64]    [64, 64, 8, 8]   \n",
      "53_layer2.3.ReLU_relu                                -    [64, 64, 8, 8]   \n",
      "54_layer3.0.Conv2d_conv1               [64, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "55_layer3.0.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "56_layer3.0.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "57_layer3.0.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "58_layer3.0.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "59_layer3.0.downsample.Conv2d_0        [64, 128, 1, 1]   [64, 128, 4, 4]   \n",
      "60_layer3.0.downsample.BatchNorm2d_1             [128]   [64, 128, 4, 4]   \n",
      "61_layer3.0.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "62_layer3.1.Conv2d_conv1              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "63_layer3.1.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "64_layer3.1.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "65_layer3.1.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "66_layer3.1.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "67_layer3.1.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "68_layer3.2.Conv2d_conv1              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "69_layer3.2.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "70_layer3.2.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "71_layer3.2.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "72_layer3.2.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "73_layer3.2.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "74_layer3.3.Conv2d_conv1              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "75_layer3.3.BatchNorm2d_bn1                      [128]   [64, 128, 4, 4]   \n",
      "76_layer3.3.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "77_layer3.3.Conv2d_conv2              [128, 128, 3, 3]   [64, 128, 4, 4]   \n",
      "78_layer3.3.BatchNorm2d_bn2                      [128]   [64, 128, 4, 4]   \n",
      "79_layer3.3.ReLU_relu                                -   [64, 128, 4, 4]   \n",
      "80_avgpool                                           -   [64, 128, 1, 1]   \n",
      "81_fc                                        [128, 10]          [64, 10]   \n",
      "\n",
      "                                        Params  Mult-Adds  \n",
      "Layer                                                      \n",
      "0_conv1                                  864.0   884.736k  \n",
      "1_bn1                                     64.0       32.0  \n",
      "2_relu                                       -          -  \n",
      "3_maxpool                                    -          -  \n",
      "4_layer1.0.Conv2d_conv1                 9.216k  2.359296M  \n",
      "5_layer1.0.BatchNorm2d_bn1                64.0       32.0  \n",
      "6_layer1.0.ReLU_relu                         -          -  \n",
      "7_layer1.0.Conv2d_conv2                 9.216k  2.359296M  \n",
      "8_layer1.0.BatchNorm2d_bn2                64.0       32.0  \n",
      "9_layer1.0.ReLU_relu                         -          -  \n",
      "10_layer1.1.Conv2d_conv1                9.216k  2.359296M  \n",
      "11_layer1.1.BatchNorm2d_bn1               64.0       32.0  \n",
      "12_layer1.1.ReLU_relu                        -          -  \n",
      "13_layer1.1.Conv2d_conv2                9.216k  2.359296M  \n",
      "14_layer1.1.BatchNorm2d_bn2               64.0       32.0  \n",
      "15_layer1.1.ReLU_relu                        -          -  \n",
      "16_layer1.2.Conv2d_conv1                9.216k  2.359296M  \n",
      "17_layer1.2.BatchNorm2d_bn1               64.0       32.0  \n",
      "18_layer1.2.ReLU_relu                        -          -  \n",
      "19_layer1.2.Conv2d_conv2                9.216k  2.359296M  \n",
      "20_layer1.2.BatchNorm2d_bn2               64.0       32.0  \n",
      "21_layer1.2.ReLU_relu                        -          -  \n",
      "22_layer1.3.Conv2d_conv1                9.216k  2.359296M  \n",
      "23_layer1.3.BatchNorm2d_bn1               64.0       32.0  \n",
      "24_layer1.3.ReLU_relu                        -          -  \n",
      "25_layer1.3.Conv2d_conv2                9.216k  2.359296M  \n",
      "26_layer1.3.BatchNorm2d_bn2               64.0       32.0  \n",
      "27_layer1.3.ReLU_relu                        -          -  \n",
      "28_layer2.0.Conv2d_conv1               18.432k  1.179648M  \n",
      "29_layer2.0.BatchNorm2d_bn1              128.0       64.0  \n",
      "30_layer2.0.ReLU_relu                        -          -  \n",
      "31_layer2.0.Conv2d_conv2               36.864k  2.359296M  \n",
      "32_layer2.0.BatchNorm2d_bn2              128.0       64.0  \n",
      "33_layer2.0.downsample.Conv2d_0         2.048k   131.072k  \n",
      "34_layer2.0.downsample.BatchNorm2d_1     128.0       64.0  \n",
      "35_layer2.0.ReLU_relu                        -          -  \n",
      "36_layer2.1.Conv2d_conv1               36.864k  2.359296M  \n",
      "37_layer2.1.BatchNorm2d_bn1              128.0       64.0  \n",
      "38_layer2.1.ReLU_relu                        -          -  \n",
      "39_layer2.1.Conv2d_conv2               36.864k  2.359296M  \n",
      "40_layer2.1.BatchNorm2d_bn2              128.0       64.0  \n",
      "41_layer2.1.ReLU_relu                        -          -  \n",
      "42_layer2.2.Conv2d_conv1               36.864k  2.359296M  \n",
      "43_layer2.2.BatchNorm2d_bn1              128.0       64.0  \n",
      "44_layer2.2.ReLU_relu                        -          -  \n",
      "45_layer2.2.Conv2d_conv2               36.864k  2.359296M  \n",
      "46_layer2.2.BatchNorm2d_bn2              128.0       64.0  \n",
      "47_layer2.2.ReLU_relu                        -          -  \n",
      "48_layer2.3.Conv2d_conv1               36.864k  2.359296M  \n",
      "49_layer2.3.BatchNorm2d_bn1              128.0       64.0  \n",
      "50_layer2.3.ReLU_relu                        -          -  \n",
      "51_layer2.3.Conv2d_conv2               36.864k  2.359296M  \n",
      "52_layer2.3.BatchNorm2d_bn2              128.0       64.0  \n",
      "53_layer2.3.ReLU_relu                        -          -  \n",
      "54_layer3.0.Conv2d_conv1               73.728k  1.179648M  \n",
      "55_layer3.0.BatchNorm2d_bn1              256.0      128.0  \n",
      "56_layer3.0.ReLU_relu                        -          -  \n",
      "57_layer3.0.Conv2d_conv2              147.456k  2.359296M  \n",
      "58_layer3.0.BatchNorm2d_bn2              256.0      128.0  \n",
      "59_layer3.0.downsample.Conv2d_0         8.192k   131.072k  \n",
      "60_layer3.0.downsample.BatchNorm2d_1     256.0      128.0  \n",
      "61_layer3.0.ReLU_relu                        -          -  \n",
      "62_layer3.1.Conv2d_conv1              147.456k  2.359296M  \n",
      "63_layer3.1.BatchNorm2d_bn1              256.0      128.0  \n",
      "64_layer3.1.ReLU_relu                        -          -  \n",
      "65_layer3.1.Conv2d_conv2              147.456k  2.359296M  \n",
      "66_layer3.1.BatchNorm2d_bn2              256.0      128.0  \n",
      "67_layer3.1.ReLU_relu                        -          -  \n",
      "68_layer3.2.Conv2d_conv1              147.456k  2.359296M  \n",
      "69_layer3.2.BatchNorm2d_bn1              256.0      128.0  \n",
      "70_layer3.2.ReLU_relu                        -          -  \n",
      "71_layer3.2.Conv2d_conv2              147.456k  2.359296M  \n",
      "72_layer3.2.BatchNorm2d_bn2              256.0      128.0  \n",
      "73_layer3.2.ReLU_relu                        -          -  \n",
      "74_layer3.3.Conv2d_conv1              147.456k  2.359296M  \n",
      "75_layer3.3.BatchNorm2d_bn1              256.0      128.0  \n",
      "76_layer3.3.ReLU_relu                        -          -  \n",
      "77_layer3.3.Conv2d_conv2              147.456k  2.359296M  \n",
      "78_layer3.3.BatchNorm2d_bn2              256.0      128.0  \n",
      "79_layer3.3.ReLU_relu                        -          -  \n",
      "80_avgpool                                   -          -  \n",
      "81_fc                                    1.29k      1.28k  \n",
      "---------------------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params           1.472554M\n",
      "Trainable params       1.472554M\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             55.413984M\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((64, 3, 32, 32), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "nb_conv = 0\n",
    "\n",
    "for layer in s.index.values:\n",
    "    if \"Conv\" in layer:\n",
    "        nb_conv += 1\n",
    "print(nb_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = model_func()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/cifar10/full_supervised/2020-08-26_16:47:23_wideresnet28_2_1.0S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1fS\" % (get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % (model_func.__name__, args.supervised_ratio)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (args.tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(args.tensorboard_path, tensorboard_title))\n",
    "\n",
    "# losses\n",
    "loss_ce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# callbacks\n",
    "# https://arxiv.org/pdf/1610.02242.pdf page 11\n",
    "# TODO FIND  A WAY TO DIFFERENTIATE LR SCHEDULER FOR CIFAR10 AND UBS8K\n",
    "def lr_lambda(e):\n",
    "    if e < 60:\n",
    "        return 1\n",
    "    \n",
    "    elif 60 <= e < 120:\n",
    "        return 0.2\n",
    "    \n",
    "    elif 120 <= e < 160:\n",
    "        return 0.04\n",
    "    \n",
    "    else:\n",
    "        return 0.008\n",
    "    \n",
    "# lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = CheckPoint(model, optimizer, mode=\"max\", name=\"%s/%s.torch\" % (args.checkpoint_path, checkpoint_title))\n",
    "\n",
    "# Metrics\n",
    "fscore_fn = FScore()\n",
    "acc_fn = CategoricalAccuracy()\n",
    "avg = ContinueAverage()\n",
    "\n",
    "reset_metrics = lambda : [m.reset() for m in [fscore_fn, acc_fn, avg]]\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2cf07c5550>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, 300, 300)\n",
    "y = [lr_lambda(x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def lr_lambda(e):\n",
    "#     if e < 80:\n",
    "#         phase = 1 - e / 80\n",
    "#         return np.exp(-5 * phase**2)\n",
    "    \n",
    "#     elif 80 <= e < args.nb_epoch - 50:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         phase = (args.nb_epoch -e) / 50\n",
    "#         return np.exp(-5 * (1 - phase)**2)\n",
    "\n",
    "# x = np.linspace(0, 300, 300)\n",
    "# y = [args.learning_rate * lr_lambda(x_) for x_ in x]\n",
    "\n",
    "# plt.plot(x, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"ce\", \"metrics: \", \"acc\", \"F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):        \n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        logits = model(X)        \n",
    "        loss = loss_ce(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=10)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(train_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"train/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    reset_metrics()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_ce(logits, y)\n",
    "\n",
    "            # metrics\n",
    "            pred = torch.softmax(logits, dim=1)\n",
    "            pred_arg = torch.argmax(logits, dim=1)\n",
    "            y_one_hot = F.one_hot(y, num_classes=10)\n",
    "\n",
    "            acc = acc_fn(pred_arg, y).mean\n",
    "            fscore = fscore_fn(pred, y_one_hot).mean\n",
    "            avg_ce = avg(loss.item()).mean\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", avg_ce,\n",
    "                \"\", acc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/Lce\", avg_ce, epoch)\n",
    "    tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc\", acc, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"max/acc\", maximum_fn(\"acc\", acc), epoch )\n",
    "    tensorboard.add_scalar(\"max/f1\", maximum_fn(\"f1\", fscore), epoch )\n",
    "\n",
    "    checkpoint.step(acc)\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  ce     - metrics:  acc         | F1       - Time  \n",
      "\n",
      "Training 1      - 100    -          1.6001 -           0.4152    | 0.2885   - 12.7816\n",
      "\u001b[1;4mValidati 1      - 100    -          1.3065 -           0.5440    | 0.4739   - 0.9119\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 2      - 100    -          1.1630 -           0.5857    | 0.5431   - 12.7385\n",
      "\u001b[1;4mValidati 2      - 100    -          1.0354 -           0.6278    | 0.5873   - 0.9508\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 3      - 100    -          0.9973 -           0.6466    | 0.6240   - 12.7729\n",
      "\u001b[1;4mValidati 3      - 100    -          1.0904 -           0.6190    | 0.6023   - 0.9466\u001b[0m\n",
      "Training 4      - 100    -          0.9007 -           0.6831    | 0.6686   - 10.3968\n",
      "\u001b[1;4mValidati 4      - 100    -          0.9197 -           0.6822    | 0.6628   - 0.8359\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 5      - 100    -          0.8087 -           0.7200    | 0.7097   - 10.5740\n",
      "\u001b[1;4mValidati 5      - 100    -          1.0364 -           0.6421    | 0.6343   - 0.9642\u001b[0m\n",
      "Training 6      - 100    -          0.7464 -           0.7399    | 0.7347   - 12.1802\n",
      "\u001b[1;4mValidati 6      - 100    -          0.8954 -           0.6935    | 0.6940   - 0.9241\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 7      - 100    -          0.7054 -           0.7543    | 0.7507   - 12.8233\n",
      "\u001b[1;4mValidati 7      - 100    -          0.7570 -           0.7334    | 0.7287   - 0.9479\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 8      - 100    -          0.6738 -           0.7658    | 0.7627   - 12.8688\n",
      "\u001b[1;4mValidati 8      - 100    -          0.8419 -           0.7081    | 0.7041   - 0.9546\u001b[0m\n",
      "Training 9      - 100    -          0.6404 -           0.7793    | 0.7760   - 12.8925\n",
      "\u001b[1;4mValidati 9      - 100    -          0.6258 -           0.7877    | 0.7900   - 0.9334\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 10     - 100    -          0.6257 -           0.7838    | 0.7815   - 13.2328\n",
      "\u001b[1;4mValidati 10     - 100    -          0.7630 -           0.7421    | 0.7431   - 0.9654\u001b[0m\n",
      "Training 11     - 100    -          0.6126 -           0.7891    | 0.7864   - 12.8732\n",
      "\u001b[1;4mValidati 11     - 100    -          0.8592 -           0.7019    | 0.7056   - 0.9715\u001b[0m\n",
      "Training 12     - 100    -          0.6045 -           0.7929    | 0.7903   - 12.3586\n",
      "\u001b[1;4mValidati 12     - 100    -          0.7751 -           0.7373    | 0.7358   - 0.9613\u001b[0m\n",
      "Training 13     - 100    -          0.5855 -           0.7974    | 0.7957   - 12.9450\n",
      "\u001b[1;4mValidati 13     - 100    -          0.6970 -           0.7665    | 0.7718   - 0.9674\u001b[0m\n",
      "Training 14     - 100    -          0.5723 -           0.8033    | 0.8014   - 13.0547\n",
      "\u001b[1;4mValidati 14     - 100    -          0.6422 -           0.7827    | 0.7887   - 0.9126\u001b[0m\n",
      "Training 15     - 100    -          0.5707 -           0.8035    | 0.8034   - 12.9671\n",
      "\u001b[1;4mValidati 15     - 100    -          0.6185 -           0.7892    | 0.7874   - 0.9659\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 16     - 100    -          0.5632 -           0.8076    | 0.8060   - 12.9115\n",
      "\u001b[1;4mValidati 16     - 100    -          0.6119 -           0.7871    | 0.7885   - 0.9291\u001b[0m\n",
      "Training 17     - 100    -          0.5513 -           0.8115    | 0.8115   - 12.9529\n",
      "\u001b[1;4mValidati 17     - 100    -          0.5880 -           0.8012    | 0.7997   - 0.9506\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 18     - 100    -          0.5464 -           0.8137    | 0.8128   - 12.9643\n",
      "\u001b[1;4mValidati 18     - 100    -          0.7093 -           0.7620    | 0.7599   - 0.9489\u001b[0m\n",
      "Training 19     - 100    -          0.5458 -           0.8111    | 0.8108   - 12.8289\n",
      "\u001b[1;4mValidati 19     - 100    -          0.6962 -           0.7617    | 0.7654   - 0.9691\u001b[0m\n",
      "Training 20     - 100    -          0.5378 -           0.8148    | 0.8130   - 12.8703\n",
      "\u001b[1;4mValidati 20     - 100    -          0.6268 -           0.7868    | 0.7893   - 0.9310\u001b[0m\n",
      "Training 21     - 100    -          0.5349 -           0.8175    | 0.8156   - 12.6907\n",
      "\u001b[1;4mValidati 21     - 100    -          0.6426 -           0.7783    | 0.7775   - 0.9467\u001b[0m\n",
      "Training 22     - 100    -          0.5296 -           0.8175    | 0.8185   - 12.9129\n",
      "\u001b[1;4mValidati 22     - 100    -          0.6206 -           0.7851    | 0.7876   - 0.9577\u001b[0m\n",
      "Training 23     - 100    -          0.5293 -           0.8196    | 0.8187   - 12.9451\n",
      "\u001b[1;4mValidati 23     - 100    -          0.7597 -           0.7481    | 0.7475   - 0.9374\u001b[0m\n",
      "Training 24     - 100    -          0.5242 -           0.8192    | 0.8190   - 12.8776\n",
      "\u001b[1;4mValidati 24     - 100    -          0.7373 -           0.7625    | 0.7659   - 0.9531\u001b[0m\n",
      "Training 25     - 100    -          0.5199 -           0.8217    | 0.8209   - 12.5328\n",
      "\u001b[1;4mValidati 25     - 100    -          0.6271 -           0.7902    | 0.7905   - 0.8949\u001b[0m\n",
      "Training 26     - 100    -          0.5182 -           0.8222    | 0.8220   - 12.8274\n",
      "\u001b[1;4mValidati 26     - 100    -          0.5992 -           0.7987    | 0.8014   - 0.9886\u001b[0m\n",
      "Training 27     - 100    -          0.5209 -           0.8217    | 0.8219   - 12.7020\n",
      "\u001b[1;4mValidati 27     - 100    -          1.0146 -           0.6929    | 0.6968   - 0.9404\u001b[0m\n",
      "Training 28     - 100    -          0.5125 -           0.8232    | 0.8231   - 12.7721\n",
      "\u001b[1;4mValidati 28     - 100    -          0.6339 -           0.7867    | 0.7852   - 0.9395\u001b[0m\n",
      "Training 29     - 100    -          0.5122 -           0.8241    | 0.8244   - 12.7545\n",
      "\u001b[1;4mValidati 29     - 100    -          0.5791 -           0.8088    | 0.8089   - 0.9346\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 30     - 100    -          0.5043 -           0.8276    | 0.8267   - 12.8281\n",
      "\u001b[1;4mValidati 30     - 100    -          0.6001 -           0.7929    | 0.7940   - 0.9870\u001b[0m\n",
      "Training 31     - 100    -          0.5052 -           0.8278    | 0.8269   - 10.5725\n",
      "\u001b[1;4mValidati 31     - 100    -          0.6486 -           0.7833    | 0.7853   - 0.9717\u001b[0m\n",
      "Training 32     - 100    -          0.5053 -           0.8272    | 0.8265   - 10.9840\n",
      "\u001b[1;4mValidati 32     - 100    -          0.5455 -           0.8127    | 0.8132   - 0.9686\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 33     - 100    -          0.4968 -           0.8291    | 0.8289   - 11.1730\n",
      "\u001b[1;4mValidati 33     - 100    -          0.7031 -           0.7676    | 0.7675   - 1.0313\u001b[0m\n",
      "Training 34     - 100    -          0.4996 -           0.8296    | 0.8292   - 10.6271\n",
      "\u001b[1;4mValidati 34     - 100    -          0.6460 -           0.7852    | 0.7842   - 0.9420\u001b[0m\n",
      "Training 35     - 100    -          0.4974 -           0.8290    | 0.8286   - 10.5495\n",
      "\u001b[1;4mValidati 35     - 100    -          0.7490 -           0.7438    | 0.7441   - 0.8908\u001b[0m\n",
      "Training 36     - 100    -          0.4995 -           0.8304    | 0.8308   - 12.9302\n",
      "\u001b[1;4mValidati 36     - 100    -          0.9253 -           0.7340    | 0.7275   - 0.9219\u001b[0m\n",
      "Training 37     - 100    -          0.5010 -           0.8292    | 0.8299   - 12.9787\n",
      "\u001b[1;4mValidati 37     - 100    -          0.6697 -           0.7738    | 0.7812   - 0.9677\u001b[0m\n",
      "Training 38     - 100    -          0.4989 -           0.8278    | 0.8278   - 12.9889\n",
      "\u001b[1;4mValidati 38     - 100    -          0.6709 -           0.7798    | 0.7794   - 0.9732\u001b[0m\n",
      "Training 39     - 100    -          0.4909 -           0.8345    | 0.8339   - 12.7342\n",
      "\u001b[1;4mValidati 39     - 100    -          0.6302 -           0.7881    | 0.7918   - 0.9270\u001b[0m\n",
      "Training 40     - 100    -          0.4951 -           0.8290    | 0.8298   - 13.0237\n",
      "\u001b[1;4mValidati 40     - 100    -          0.6654 -           0.7831    | 0.7833   - 0.9711\u001b[0m\n",
      "Training 41     - 100    -          0.4882 -           0.8327    | 0.8325   - 12.8574\n",
      "\u001b[1;4mValidati 41     - 100    -          0.6998 -           0.7802    | 0.7808   - 0.9687\u001b[0m\n",
      "Training 42     - 100    -          0.4968 -           0.8280    | 0.8284   - 12.8543\n",
      "\u001b[1;4mValidati 42     - 100    -          0.6503 -           0.7838    | 0.7839   - 0.9934\u001b[0m\n",
      "Training 43     - 100    -          0.4939 -           0.8302    | 0.8302   - 13.0986\n",
      "\u001b[1;4mValidati 43     - 100    -          0.5906 -           0.7998    | 0.7961   - 0.9895\u001b[0m\n",
      "Training 44     - 100    -          0.4845 -           0.8346    | 0.8344   - 12.8997\n",
      "\u001b[1;4mValidati 44     - 100    -          0.6856 -           0.7794    | 0.7825   - 0.9672\u001b[0m\n",
      "Training 45     - 100    -          0.4944 -           0.8313    | 0.8306   - 13.0014\n",
      "\u001b[1;4mValidati 45     - 100    -          0.6013 -           0.7945    | 0.7985   - 0.9571\u001b[0m\n",
      "Training 46     - 100    -          0.4912 -           0.8310    | 0.8311   - 12.8817\n",
      "\u001b[1;4mValidati 46     - 100    -          0.5952 -           0.7987    | 0.8011   - 0.9303\u001b[0m\n",
      "Training 47     - 100    -          0.4808 -           0.8352    | 0.8371   - 12.5842\n",
      "\u001b[1;4mValidati 47     - 100    -          0.5708 -           0.8026    | 0.8036   - 0.9277\u001b[0m\n",
      "Training 48     - 100    -          0.4872 -           0.8335    | 0.8330   - 12.8818\n",
      "\u001b[1;4mValidati 48     - 100    -          0.7072 -           0.7565    | 0.7561   - 0.9017\u001b[0m\n",
      "Training 49     - 100    -          0.4875 -           0.8334    | 0.8328   - 13.1726\n",
      "\u001b[1;4mValidati 49     - 100    -          0.5716 -           0.8113    | 0.8132   - 0.9502\u001b[0m\n",
      "Training 50     - 100    -          0.4869 -           0.8334    | 0.8337   - 13.0084\n",
      "\u001b[1;4mValidati 50     - 100    -          0.6110 -           0.7989    | 0.7992   - 0.9693\u001b[0m\n",
      "Training 51     - 100    -          0.4799 -           0.8352    | 0.8356   - 12.8864\n",
      "\u001b[1;4mValidati 51     - 100    -          0.6011 -           0.7920    | 0.7926   - 0.9531\u001b[0m\n",
      "Training 52     - 100    -          0.4849 -           0.8344    | 0.8338   - 13.0274\n",
      "\u001b[1;4mValidati 52     - 100    -          0.5798 -           0.7994    | 0.7978   - 0.9533\u001b[0m\n",
      "Training 53     - 100    -          0.4807 -           0.8347    | 0.8357   - 13.1553\n",
      "\u001b[1;4mValidati 53     - 100    -          0.5858 -           0.8013    | 0.8042   - 0.9739\u001b[0m\n",
      "Training 54     - 100    -          0.4835 -           0.8337    | 0.8341   - 13.0719\n",
      "\u001b[1;4mValidati 54     - 100    -          0.6590 -           0.7759    | 0.7757   - 0.9476\u001b[0m\n",
      "Training 55     - 100    -          0.4783 -           0.8366    | 0.8358   - 12.8896\n",
      "\u001b[1;4mValidati 55     - 100    -          0.6114 -           0.7967    | 0.7997   - 0.9893\u001b[0m\n",
      "Training 56     - 100    -          0.4875 -           0.8321    | 0.8330   - 12.6128\n",
      "\u001b[1;4mValidati 56     - 100    -          0.5548 -           0.8164    | 0.8130   - 0.9460\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 57     - 100    -          0.4753 -           0.8382    | 0.8376   - 12.9770\n",
      "\u001b[1;4mValidati 57     - 100    -          0.5337 -           0.8208    | 0.8216   - 0.9353\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 58     - 100    -          0.4854 -           0.8332    | 0.8332   - 13.2784\n",
      "\u001b[1;4mValidati 58     - 100    -          0.7282 -           0.7573    | 0.7572   - 0.9333\u001b[0m\n",
      "Training 59     - 100    -          0.4769 -           0.8366    | 0.8369   - 13.1083\n",
      "\u001b[1;4mValidati 59     - 100    -          0.5342 -           0.8171    | 0.8200   - 0.9297\u001b[0m\n",
      "Training 60     - 100    -          0.4830 -           0.8342    | 0.8345   - 13.0006\n",
      "\u001b[1;4mValidati 60     - 100    -          0.5143 -           0.8239    | 0.8265   - 0.9856\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 61     - 100    -          0.3032 -           0.8970    | 0.8970   - 12.9171\n",
      "\u001b[1;4mValidati 61     - 100    -          0.3243 -           0.8884    | 0.8898   - 0.9656\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 62     - 100    -          0.2605 -           0.9095    | 0.9107   - 13.0651\n",
      "\u001b[1;4mValidati 62     - 100    -          0.3214 -           0.8893    | 0.8907   - 0.9656\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 63     - 100    -          0.2473 -           0.9154    | 0.9160   - 12.6427\n",
      "\u001b[1;4mValidati 63     - 100    -          0.3118 -           0.8966    | 0.8978   - 0.9462\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 64     - 100    -          0.2371 -           0.9195    | 0.9205   - 13.0316\n",
      "\u001b[1;4mValidati 64     - 100    -          0.3249 -           0.8948    | 0.8950   - 0.9064\u001b[0m\n",
      "Training 65     - 100    -          0.2330 -           0.9202    | 0.9205   - 12.9074\n",
      "\u001b[1;4mValidati 65     - 100    -          0.3360 -           0.8910    | 0.8925   - 0.8938\u001b[0m\n",
      "Training 66     - 100    -          0.2269 -           0.9217    | 0.9224   - 13.1984\n",
      "\u001b[1;4mValidati 66     - 100    -          0.3427 -           0.8835    | 0.8859   - 0.9461\u001b[0m\n",
      "Training 67     - 100    -          0.2305 -           0.9187    | 0.9196   - 13.1230\n",
      "\u001b[1;4mValidati 67     - 100    -          0.3501 -           0.8855    | 0.8887   - 0.9221\u001b[0m\n",
      "Training 68     - 100    -          0.2286 -           0.9211    | 0.9217   - 13.1379\n",
      "\u001b[1;4mValidati 68     - 100    -          0.3201 -           0.8953    | 0.8969   - 0.9442\u001b[0m\n",
      "Training 69     - 100    -          0.2228 -           0.9221    | 0.9225   - 12.9583\n",
      "\u001b[1;4mValidati 69     - 100    -          0.3327 -           0.8920    | 0.8949   - 0.9912\u001b[0m\n",
      "Training 70     - 100    -          0.2223 -           0.9236    | 0.9243   - 13.2831\n",
      "\u001b[1;4mValidati 70     - 100    -          0.3375 -           0.8880    | 0.8893   - 0.9186\u001b[0m\n",
      "Training 71     - 100    -          0.2234 -           0.9218    | 0.9226   - 13.0386\n",
      "\u001b[1;4mValidati 71     - 100    -          0.3725 -           0.8812    | 0.8824   - 1.0193\u001b[0m\n",
      "Training 72     - 100    -          0.2263 -           0.9206    | 0.9223   - 12.9406\n",
      "\u001b[1;4mValidati 72     - 100    -          0.3651 -           0.8800    | 0.8810   - 0.9911\u001b[0m\n",
      "Training 73     - 100    -          0.2317 -           0.9188    | 0.9205   - 12.8789\n",
      "\u001b[1;4mValidati 73     - 100    -          0.3579 -           0.8809    | 0.8828   - 0.9367\u001b[0m\n",
      "Training 74     - 100    -          0.2236 -           0.9228    | 0.9233   - 13.0256\n",
      "\u001b[1;4mValidati 74     - 100    -          0.3540 -           0.8826    | 0.8846   - 1.0088\u001b[0m\n",
      "Training 75     - 100    -          0.2293 -           0.9209    | 0.9212   - 10.5057\n",
      "\u001b[1;4mValidati 75     - 100    -          0.3656 -           0.8777    | 0.8786   - 0.9608\u001b[0m\n",
      "Training 76     - 100    -          0.2282 -           0.9204    | 0.9215   - 10.8580\n",
      "\u001b[1;4mValidati 76     - 100    -          0.3668 -           0.8758    | 0.8794   - 0.9592\u001b[0m\n",
      "Training 77     - 100    -          0.2296 -           0.9196    | 0.9203   - 13.0830\n",
      "\u001b[1;4mValidati 77     - 100    -          0.3775 -           0.8747    | 0.8787   - 0.9632\u001b[0m\n",
      "Training 78     - 100    -          0.2344 -           0.9188    | 0.9199   - 10.6063\n",
      "\u001b[1;4mValidati 78     - 100    -          0.3729 -           0.8787    | 0.8799   - 0.8987\u001b[0m\n",
      "Training 79     - 100    -          0.2320 -           0.9203    | 0.9209   - 11.7357\n",
      "\u001b[1;4mValidati 79     - 100    -          0.4103 -           0.8671    | 0.8700   - 0.9663\u001b[0m\n",
      "Training 80     - 100    -          0.2292 -           0.9199    | 0.9214   - 10.5770\n",
      "\u001b[1;4mValidati 80     - 100    -          0.3743 -           0.8768    | 0.8801   - 0.8824\u001b[0m\n",
      "Training 81     - 100    -          0.2316 -           0.9200    | 0.9206   - 12.7116\n",
      "\u001b[1;4mValidati 81     - 100    -          0.3839 -           0.8781    | 0.8795   - 0.9323\u001b[0m\n",
      "Training 82     - 100    -          0.2261 -           0.9222    | 0.9226   - 13.0022\n",
      "\u001b[1;4mValidati 82     - 100    -          0.3740 -           0.8780    | 0.8809   - 0.9267\u001b[0m\n",
      "Training 83     - 100    -          0.2282 -           0.9203    | 0.9211   - 13.1868\n",
      "\u001b[1;4mValidati 83     - 100    -          0.4049 -           0.8679    | 0.8684   - 1.0205\u001b[0m\n",
      "Training 84     - 100    -          0.2324 -           0.9189    | 0.9199   - 13.0725\n",
      "\u001b[1;4mValidati 84     - 100    -          0.3795 -           0.8747    | 0.8772   - 0.9769\u001b[0m\n",
      "Training 85     - 100    -          0.2230 -           0.9235    | 0.9237   - 13.0521\n",
      "\u001b[1;4mValidati 85     - 100    -          0.3698 -           0.8807    | 0.8824   - 0.9534\u001b[0m\n",
      "Training 86     - 100    -          0.2244 -           0.9222    | 0.9236   - 13.2193\n",
      "\u001b[1;4mValidati 86     - 100    -          0.4065 -           0.8711    | 0.8715   - 0.9852\u001b[0m\n",
      "Training 87     - 100    -          0.2288 -           0.9215    | 0.9220   - 13.1051\n",
      "\u001b[1;4mValidati 87     - 100    -          0.3829 -           0.8752    | 0.8775   - 0.9529\u001b[0m\n",
      "Training 88     - 100    -          0.2223 -           0.9231    | 0.9233   - 13.1113\n",
      "\u001b[1;4mValidati 88     - 100    -          0.3855 -           0.8749    | 0.8755   - 0.9279\u001b[0m\n",
      "Training 89     - 100    -          0.2242 -           0.9224    | 0.9231   - 13.0668\n",
      "\u001b[1;4mValidati 89     - 100    -          0.3721 -           0.8804    | 0.8816   - 0.9814\u001b[0m\n",
      "Training 90     - 100    -          0.2260 -           0.9223    | 0.9231   - 13.0302\n",
      "\u001b[1;4mValidati 90     - 100    -          0.3797 -           0.8791    | 0.8809   - 0.9861\u001b[0m\n",
      "Training 91     - 100    -          0.2214 -           0.9247    | 0.9250   - 12.9723\n",
      "\u001b[1;4mValidati 91     - 100    -          0.3928 -           0.8762    | 0.8769   - 0.9336\u001b[0m\n",
      "Training 92     - 100    -          0.2231 -           0.9210    | 0.9216   - 13.2774\n",
      "\u001b[1;4mValidati 92     - 100    -          0.4710 -           0.8539    | 0.8554   - 0.9436\u001b[0m\n",
      "Training 93     - 100    -          0.2183 -           0.9229    | 0.9240   - 12.8994\n",
      "\u001b[1;4mValidati 93     - 100    -          0.3768 -           0.8770    | 0.8795   - 0.9212\u001b[0m\n",
      "Training 94     - 100    -          0.2222 -           0.9235    | 0.9238   - 12.9877\n",
      "\u001b[1;4mValidati 94     - 100    -          0.3925 -           0.8730    | 0.8756   - 0.9459\u001b[0m\n",
      "Training 95     - 100    -          0.2160 -           0.9247    | 0.9260   - 12.7582\n",
      "\u001b[1;4mValidati 95     - 100    -          0.3484 -           0.8870    | 0.8872   - 1.0042\u001b[0m\n",
      "Training 96     - 100    -          0.2193 -           0.9240    | 0.9243   - 12.9121\n",
      "\u001b[1;4mValidati 96     - 100    -          0.3919 -           0.8763    | 0.8794   - 0.9345\u001b[0m\n",
      "Training 97     - 100    -          0.2151 -           0.9246    | 0.9260   - 12.1110\n",
      "\u001b[1;4mValidati 97     - 100    -          0.3521 -           0.8825    | 0.8844   - 0.9811\u001b[0m\n",
      "Training 98     - 100    -          0.2179 -           0.9240    | 0.9247   - 12.9439\n",
      "\u001b[1;4mValidati 98     - 100    -          0.3440 -           0.8844    | 0.8872   - 0.9543\u001b[0m\n",
      "Training 99     - 100    -          0.2173 -           0.9253    | 0.9256   - 12.9541\n",
      "\u001b[1;4mValidati 99     - 100    -          0.3604 -           0.8812    | 0.8844   - 0.9278\u001b[0m\n",
      "Training 100    - 100    -          0.2184 -           0.9240    | 0.9251   - 12.9979\n",
      "\u001b[1;4mValidati 100    - 100    -          0.3806 -           0.8750    | 0.8796   - 0.9741\u001b[0m\n",
      "Training 101    - 100    -          0.2142 -           0.9255    | 0.9262   - 13.0094\n",
      "\u001b[1;4mValidati 101    - 100    -          0.3713 -           0.8786    | 0.8796   - 1.0051\u001b[0m\n",
      "Training 102    - 100    -          0.2128 -           0.9260    | 0.9269   - 13.1228\n",
      "\u001b[1;4mValidati 102    - 100    -          0.4246 -           0.8633    | 0.8668   - 0.9490\u001b[0m\n",
      "Training 103    - 100    -          0.2151 -           0.9260    | 0.9262   - 13.0836\n",
      "\u001b[1;4mValidati 103    - 100    -          0.3956 -           0.8761    | 0.8770   - 0.9313\u001b[0m\n",
      "Training 104    - 100    -          0.2100 -           0.9280    | 0.9286   - 12.7541\n",
      "\u001b[1;4mValidati 104    - 100    -          0.3574 -           0.8860    | 0.8872   - 0.9741\u001b[0m\n",
      "Training 105    - 100    -          0.2075 -           0.9287    | 0.9286   - 12.8435\n",
      "\u001b[1;4mValidati 105    - 100    -          0.3945 -           0.8722    | 0.8743   - 0.9354\u001b[0m\n",
      "Training 106    - 100    -          0.2098 -           0.9270    | 0.9274   - 13.3278\n",
      "\u001b[1;4mValidati 106    - 100    -          0.3668 -           0.8805    | 0.8828   - 1.0194\u001b[0m\n",
      "Training 107    - 100    -          0.2090 -           0.9263    | 0.9270   - 12.8758\n",
      "\u001b[1;4mValidati 107    - 100    -          0.5406 -           0.8389    | 0.8415   - 0.8840\u001b[0m\n",
      "Training 108    - 100    -          0.2167 -           0.9252    | 0.9258   - 13.0614\n",
      "\u001b[1;4mValidati 108    - 100    -          0.3636 -           0.8845    | 0.8869   - 1.0242\u001b[0m\n",
      "Training 109    - 100    -          0.2149 -           0.9254    | 0.9257   - 12.7786\n",
      "\u001b[1;4mValidati 109    - 100    -          0.3525 -           0.8868    | 0.8891   - 0.9303\u001b[0m\n",
      "Training 110    - 100    -          0.2057 -           0.9288    | 0.9293   - 12.9871\n",
      "\u001b[1;4mValidati 110    - 100    -          0.4623 -           0.8526    | 0.8548   - 1.0159\u001b[0m\n",
      "Training 111    - 100    -          0.2075 -           0.9285    | 0.9294   - 13.0842\n",
      "\u001b[1;4mValidati 111    - 100    -          0.3666 -           0.8795    | 0.8815   - 0.9349\u001b[0m\n",
      "Training 112    - 100    -          0.2079 -           0.9278    | 0.9281   - 13.2044\n",
      "\u001b[1;4mValidati 112    - 100    -          0.3932 -           0.8723    | 0.8745   - 0.9432\u001b[0m\n",
      "Training 113    - 100    -          0.2091 -           0.9269    | 0.9271   - 13.0421\n",
      "\u001b[1;4mValidati 113    - 100    -          0.3947 -           0.8755    | 0.8778   - 0.9469\u001b[0m\n",
      "Training 114    - 100    -          0.2029 -           0.9300    | 0.9302   - 10.5853\n",
      "\u001b[1;4mValidati 114    - 100    -          0.3781 -           0.8777    | 0.8811   - 0.9118\u001b[0m\n",
      "Training 115    - 100    -          0.2080 -           0.9279    | 0.9291   - 12.2673\n",
      "\u001b[1;4mValidati 115    - 100    -          0.3416 -           0.8865    | 0.8894   - 0.9293\u001b[0m\n",
      "Training 116    - 100    -          0.2077 -           0.9289    | 0.9302   - 11.0980\n",
      "\u001b[1;4mValidati 116    - 100    -          0.3633 -           0.8820    | 0.8855   - 0.9775\u001b[0m\n",
      "Training 117    - 100    -          0.2040 -           0.9288    | 0.9296   - 13.1426\n",
      "\u001b[1;4mValidati 117    - 100    -          0.3726 -           0.8762    | 0.8773   - 0.9655\u001b[0m\n",
      "Training 118    - 100    -          0.2070 -           0.9282    | 0.9287   - 12.9204\n",
      "\u001b[1;4mValidati 118    - 100    -          0.3817 -           0.8786    | 0.8815   - 0.9522\u001b[0m\n",
      "Training 119    - 100    -          0.2047 -           0.9284    | 0.9291   - 12.9451\n",
      "\u001b[1;4mValidati 119    - 100    -          0.3894 -           0.8779    | 0.8785   - 0.9978\u001b[0m\n",
      "Training 120    - 100    -          0.2030 -           0.9302    | 0.9309   - 12.4817\n",
      "\u001b[1;4mValidati 120    - 100    -          0.3835 -           0.8779    | 0.8790   - 0.9667\u001b[0m\n",
      "Training 121    - 100    -          0.1181 -           0.9596    | 0.9600   - 13.1770\n",
      "\u001b[1;4mValidati 121    - 100    -          0.2639 -           0.9177    | 0.9193   - 0.9399\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 122    - 100    -          0.0865 -           0.9707    | 0.9712   - 12.8016\n",
      "\u001b[1;4mValidati 122    - 100    -          0.2646 -           0.9178    | 0.9187   - 0.9349\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 123    - 100    -          0.0809 -           0.9719    | 0.9722   - 12.9976\n",
      "\u001b[1;4mValidati 123    - 100    -          0.2613 -           0.9199    | 0.9215   - 0.9802\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 124    - 100    -          0.0732 -           0.9750    | 0.9750   - 13.2873\n",
      "\u001b[1;4mValidati 124    - 100    -          0.2741 -           0.9190    | 0.9200   - 0.9980\u001b[0m\n",
      "Training 125    - 100    -          0.0657 -           0.9781    | 0.9781   - 13.0772\n",
      "\u001b[1;4mValidati 125    - 100    -          0.2683 -           0.9220    | 0.9231   - 0.9584\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 126    - 100    -          0.0617 -           0.9794    | 0.9795   - 13.1221\n",
      "\u001b[1;4mValidati 126    - 100    -          0.2754 -           0.9215    | 0.9209   - 0.9481\u001b[0m\n",
      "Training 127    - 100    -          0.0601 -           0.9802    | 0.9801   - 13.1158\n",
      "\u001b[1;4mValidati 127    - 100    -          0.2942 -           0.9155    | 0.9165   - 0.9650\u001b[0m\n",
      "Training 128    - 100    -          0.0580 -           0.9804    | 0.9804   - 13.0263\n",
      "\u001b[1;4mValidati 128    - 100    -          0.2855 -           0.9201    | 0.9206   - 0.9001\u001b[0m\n",
      "Training 129    - 100    -          0.0541 -           0.9819    | 0.9821   - 12.9940\n",
      "\u001b[1;4mValidati 129    - 100    -          0.2956 -           0.9159    | 0.9173   - 0.9081\u001b[0m\n",
      "Training 130    - 100    -          0.0541 -           0.9814    | 0.9815   - 13.1207\n",
      "\u001b[1;4mValidati 130    - 100    -          0.2899 -           0.9201    | 0.9207   - 0.9791\u001b[0m\n",
      "Training 131    - 100    -          0.0506 -           0.9827    | 0.9826   - 13.0474\n",
      "\u001b[1;4mValidati 131    - 100    -          0.3012 -           0.9165    | 0.9175   - 0.9822\u001b[0m\n",
      "Training 132    - 100    -          0.0507 -           0.9834    | 0.9836   - 13.0782\n",
      "\u001b[1;4mValidati 132    - 100    -          0.2970 -           0.9173    | 0.9186   - 0.9609\u001b[0m\n",
      "Training 133    - 100    -          0.0458 -           0.9843    | 0.9845   - 13.4357\n",
      "\u001b[1;4mValidati 133    - 100    -          0.3074 -           0.9162    | 0.9172   - 0.9444\u001b[0m\n",
      "Training 134    - 100    -          0.0480 -           0.9840    | 0.9841   - 13.4212\n",
      "\u001b[1;4mValidati 134    - 100    -          0.3109 -           0.9152    | 0.9167   - 0.9610\u001b[0m\n",
      "Training 135    - 100    -          0.0424 -           0.9866    | 0.9865   - 13.1579\n",
      "\u001b[1;4mValidati 135    - 100    -          0.2995 -           0.9208    | 0.9217   - 0.9564\u001b[0m\n",
      "Training 136    - 100    -          0.0473 -           0.9840    | 0.9841   - 13.0001\n",
      "\u001b[1;4mValidati 136    - 100    -          0.3072 -           0.9194    | 0.9203   - 0.9463\u001b[0m\n",
      "Training 137    - 100    -          0.0471 -           0.9847    | 0.9849   - 13.5577\n",
      "\u001b[1;4mValidati 137    - 100    -          0.3087 -           0.9177    | 0.9185   - 0.9965\u001b[0m\n",
      "Training 138    - 100    -          0.0458 -           0.9845    | 0.9846   - 12.3485\n",
      "\u001b[1;4mValidati 138    - 100    -          0.3247 -           0.9152    | 0.9151   - 0.9420\u001b[0m\n",
      "Training 139    - 100    -          0.0441 -           0.9858    | 0.9859   - 13.1357\n",
      "\u001b[1;4mValidati 139    - 100    -          0.3074 -           0.9169    | 0.9182   - 1.0016\u001b[0m\n",
      "Training 140    - 100    -          0.0425 -           0.9857    | 0.9855   - 13.1065\n",
      "\u001b[1;4mValidati 140    - 100    -          0.3194 -           0.9180    | 0.9190   - 1.0025\u001b[0m\n",
      "Training 141    - 100    -          0.0463 -           0.9850    | 0.9850   - 13.1760\n",
      "\u001b[1;4mValidati 141    - 100    -          0.3177 -           0.9154    | 0.9164   - 0.9364\u001b[0m\n",
      "Training 142    - 100    -          0.0460 -           0.9839    | 0.9839   - 13.1049\n",
      "\u001b[1;4mValidati 142    - 100    -          0.3095 -           0.9154    | 0.9172   - 1.0008\u001b[0m\n",
      "Training 143    - 100    -          0.0428 -           0.9858    | 0.9860   - 12.9161\n",
      "\u001b[1;4mValidati 143    - 100    -          0.3268 -           0.9140    | 0.9149   - 0.9234\u001b[0m\n",
      "Training 144    - 100    -          0.0431 -           0.9855    | 0.9853   - 13.4941\n",
      "\u001b[1;4mValidati 144    - 100    -          0.3346 -           0.9114    | 0.9119   - 0.9947\u001b[0m\n",
      "Training 145    - 100    -          0.0429 -           0.9862    | 0.9861   - 13.1159\n",
      "\u001b[1;4mValidati 145    - 100    -          0.3275 -           0.9139    | 0.9152   - 1.0399\u001b[0m\n",
      "Training 146    - 100    -          0.0444 -           0.9853    | 0.9853   - 13.4143\n",
      "\u001b[1;4mValidati 146    - 100    -          0.3130 -           0.9168    | 0.9181   - 0.9633\u001b[0m\n",
      "Training 147    - 100    -          0.0428 -           0.9854    | 0.9855   - 13.6644\n",
      "\u001b[1;4mValidati 147    - 100    -          0.3095 -           0.9197    | 0.9200   - 0.9099\u001b[0m\n",
      "Training 148    - 100    -          0.0450 -           0.9850    | 0.9850   - 13.2488\n",
      "\u001b[1;4mValidati 148    - 100    -          0.3159 -           0.9176    | 0.9179   - 0.9348\u001b[0m\n",
      "Training 149    - 100    -          0.0416 -           0.9862    | 0.9862   - 13.0240\n",
      "\u001b[1;4mValidati 149    - 100    -          0.3412 -           0.9145    | 0.9154   - 0.9695\u001b[0m\n",
      "Training 150    - 100    -          0.0420 -           0.9857    | 0.9857   - 13.0569\n",
      "\u001b[1;4mValidati 150    - 100    -          0.3321 -           0.9149    | 0.9152   - 0.9260\u001b[0m\n",
      "Training 151    - 100    -          0.0420 -           0.9862    | 0.9862   - 13.2885\n",
      "\u001b[1;4mValidati 151    - 100    -          0.3282 -           0.9132    | 0.9137   - 0.9676\u001b[0m\n",
      "Training 152    - 100    -          0.0435 -           0.9854    | 0.9854   - 13.0219\n",
      "\u001b[1;4mValidati 152    - 100    -          0.3375 -           0.9102    | 0.9117   - 0.9564\u001b[0m\n",
      "Training 153    - 100    -          0.0447 -           0.9852    | 0.9850   - 13.0817\n",
      "\u001b[1;4mValidati 153    - 100    -          0.3379 -           0.9110    | 0.9122   - 0.9696\u001b[0m\n",
      "Training 154    - 100    -          0.0425 -           0.9857    | 0.9857   - 12.8901\n",
      "\u001b[1;4mValidati 154    - 100    -          0.3149 -           0.9164    | 0.9170   - 1.0077\u001b[0m\n",
      "Training 155    - 100    -          0.0412 -           0.9865    | 0.9865   - 13.2326\n",
      "\u001b[1;4mValidati 155    - 100    -          0.3439 -           0.9094    | 0.9096   - 0.9200\u001b[0m\n",
      "Training 156    - 100    -          0.0461 -           0.9850    | 0.9850   - 12.7082\n",
      "\u001b[1;4mValidati 156    - 100    -          0.3250 -           0.9158    | 0.9173   - 0.9789\u001b[0m\n",
      "Training 157    - 100    -          0.0440 -           0.9852    | 0.9852   - 13.1553\n",
      "\u001b[1;4mValidati 157    - 100    -          0.3226 -           0.9167    | 0.9180   - 1.0096\u001b[0m\n",
      "Training 158    - 100    -          0.0441 -           0.9855    | 0.9856   - 13.1321\n",
      "\u001b[1;4mValidati 158    - 100    -          0.3375 -           0.9133    | 0.9147   - 0.9310\u001b[0m\n",
      "Training 159    - 100    -          0.0452 -           0.9846    | 0.9848   - 13.2631\n",
      "\u001b[1;4mValidati 159    - 100    -          0.3545 -           0.9064    | 0.9075   - 0.9765\u001b[0m\n",
      "Training 160    - 100    -          0.0423 -           0.9856    | 0.9855   - 10.7201\n",
      "\u001b[1;4mValidati 160    - 100    -          0.3362 -           0.9152    | 0.9166   - 0.9691\u001b[0m\n",
      "Training 161    - 100    -          0.0271 -           0.9911    | 0.9913   - 12.5811\n",
      "\u001b[1;4mValidati 161    - 100    -          0.3166 -           0.9207    | 0.9211   - 0.9455\u001b[0m\n",
      "Training 162    - 100    -          0.0204 -           0.9941    | 0.9940   - 13.0551\n",
      "\u001b[1;4mValidati 162    - 100    -          0.3193 -           0.9209    | 0.9227   - 1.0008\u001b[0m\n",
      "Training 163    - 100    -          0.0184 -           0.9946    | 0.9947   - 13.0053\n",
      "\u001b[1;4mValidati 163    - 100    -          0.3206 -           0.9222    | 0.9228   - 0.9412\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 164    - 100    -          0.0173 -           0.9948    | 0.9949   - 12.8508\n",
      "\u001b[1;4mValidati 164    - 100    -          0.3250 -           0.9225    | 0.9233   - 0.9451\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 165    - 100    -          0.0166 -           0.9952    | 0.9951   - 13.2642\n",
      "\u001b[1;4mValidati 165    - 100    -          0.3140 -           0.9242    | 0.9245   - 0.9745\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 166    - 100    -          0.0164 -           0.9952    | 0.9952   - 12.9789\n",
      "\u001b[1;4mValidati 166    - 100    -          0.3150 -           0.9230    | 0.9235   - 0.9578\u001b[0m\n",
      "Training 167    - 100    -          0.0152 -           0.9954    | 0.9954   - 12.6504\n",
      "\u001b[1;4mValidati 167    - 100    -          0.3216 -           0.9241    | 0.9251   - 0.9359\u001b[0m\n",
      "Training 168    - 100    -          0.0141 -           0.9959    | 0.9958   - 13.1124\n",
      "\u001b[1;4mValidati 168    - 100    -          0.3228 -           0.9236    | 0.9245   - 0.9412\u001b[0m\n",
      "Training 169    - 100    -          0.0141 -           0.9959    | 0.9959   - 13.4594\n",
      "\u001b[1;4mValidati 169    - 100    -          0.3310 -           0.9224    | 0.9232   - 0.9541\u001b[0m\n",
      "Training 170    - 100    -          0.0140 -           0.9960    | 0.9960   - 12.8580\n",
      "\u001b[1;4mValidati 170    - 100    -          0.3274 -           0.9233    | 0.9241   - 0.9339\u001b[0m\n",
      "Training 171    - 100    -          0.0132 -           0.9964    | 0.9963   - 13.0749\n",
      "\u001b[1;4mValidati 171    - 100    -          0.3372 -           0.9229    | 0.9232   - 0.9422\u001b[0m\n",
      "Training 172    - 100    -          0.0124 -           0.9969    | 0.9969   - 13.0001\n",
      "\u001b[1;4mValidati 172    - 100    -          0.3258 -           0.9238    | 0.9250   - 0.9421\u001b[0m\n",
      "Training 173    - 100    -          0.0121 -           0.9966    | 0.9967   - 13.3046\n",
      "\u001b[1;4mValidati 173    - 100    -          0.3278 -           0.9240    | 0.9245   - 0.9347\u001b[0m\n",
      "Training 174    - 100    -          0.0125 -           0.9967    | 0.9967   - 12.5450\n",
      "\u001b[1;4mValidati 174    - 100    -          0.3213 -           0.9243    | 0.9251   - 0.9821\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 175    - 100    -          0.0119 -           0.9967    | 0.9967   - 13.2517\n",
      "\u001b[1;4mValidati 175    - 100    -          0.3307 -           0.9239    | 0.9245   - 0.9837\u001b[0m\n",
      "Training 176    - 100    -          0.0116 -           0.9966    | 0.9966   - 12.8250\n",
      "\u001b[1;4mValidati 176    - 100    -          0.3291 -           0.9224    | 0.9228   - 0.9793\u001b[0m\n",
      "Training 177    - 100    -          0.0114 -           0.9969    | 0.9969   - 13.1894\n",
      "\u001b[1;4mValidati 177    - 100    -          0.3277 -           0.9236    | 0.9244   - 0.9278\u001b[0m\n",
      "Training 178    - 100    -          0.0108 -           0.9970    | 0.9971   - 12.8107\n",
      "\u001b[1;4mValidati 178    - 100    -          0.3279 -           0.9242    | 0.9249   - 0.9399\u001b[0m\n",
      "Training 179    - 100    -          0.0104 -           0.9973    | 0.9973   - 13.3530\n",
      "\u001b[1;4mValidati 179    - 100    -          0.3246 -           0.9237    | 0.9245   - 1.0289\u001b[0m\n",
      "Training 180    - 100    -          0.0100 -           0.9976    | 0.9975   - 13.2865\n",
      "\u001b[1;4mValidati 180    - 100    -          0.3348 -           0.9252    | 0.9253   - 0.9747\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 181    - 100    -          0.0109 -           0.9969    | 0.9968   - 13.2995\n",
      "\u001b[1;4mValidati 181    - 100    -          0.3303 -           0.9234    | 0.9247   - 0.9723\u001b[0m\n",
      "Training 182    - 100    -          0.0105 -           0.9972    | 0.9972   - 13.0265\n",
      "\u001b[1;4mValidati 182    - 100    -          0.3305 -           0.9235    | 0.9244   - 0.9762\u001b[0m\n",
      "Training 183    - 100    -          0.0096 -           0.9975    | 0.9974   - 12.5103\n",
      "\u001b[1;4mValidati 183    - 100    -          0.3278 -           0.9250    | 0.9258   - 0.9546\u001b[0m\n",
      "Training 184    - 100    -          0.0099 -           0.9974    | 0.9974   - 13.3544\n",
      "\u001b[1;4mValidati 184    - 100    -          0.3439 -           0.9241    | 0.9242   - 0.9914\u001b[0m\n",
      "Training 185    - 100    -          0.0099 -           0.9972    | 0.9972   - 12.8796\n",
      "\u001b[1;4mValidati 185    - 100    -          0.3392 -           0.9248    | 0.9257   - 0.9592\u001b[0m\n",
      "Training 186    - 100    -          0.0096 -           0.9976    | 0.9976   - 13.2358\n",
      "\u001b[1;4mValidati 186    - 100    -          0.3358 -           0.9246    | 0.9252   - 0.9264\u001b[0m\n",
      "Training 187    - 100    -          0.0099 -           0.9972    | 0.9971   - 13.0207\n",
      "\u001b[1;4mValidati 187    - 100    -          0.3329 -           0.9249    | 0.9255   - 1.0388\u001b[0m\n",
      "Training 188    - 100    -          0.0103 -           0.9970    | 0.9969   - 13.0665\n",
      "\u001b[1;4mValidati 188    - 100    -          0.3350 -           0.9246    | 0.9254   - 0.9862\u001b[0m\n",
      "Training 189    - 100    -          0.0093 -           0.9976    | 0.9976   - 13.1069\n",
      "\u001b[1;4mValidati 189    - 100    -          0.3411 -           0.9229    | 0.9235   - 0.9611\u001b[0m\n",
      "Training 190    - 100    -          0.0088 -           0.9975    | 0.9976   - 11.4919\n",
      "\u001b[1;4mValidati 190    - 100    -          0.3308 -           0.9257    | 0.9266   - 0.9392\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 191    - 100    -          0.0092 -           0.9975    | 0.9975   - 13.1749\n",
      "\u001b[1;4mValidati 191    - 100    -          0.3359 -           0.9235    | 0.9244   - 0.9049\u001b[0m\n",
      "Training 192    - 100    -          0.0086 -           0.9978    | 0.9978   - 13.1439\n",
      "\u001b[1;4mValidati 192    - 100    -          0.3407 -           0.9247    | 0.9255   - 0.9920\u001b[0m\n",
      "Training 193    - 100    -          0.0088 -           0.9976    | 0.9976   - 13.0951\n",
      "\u001b[1;4mValidati 193    - 100    -          0.3425 -           0.9242    | 0.9245   - 0.9708\u001b[0m\n",
      "Training 194    - 100    -          0.0095 -           0.9971    | 0.9971   - 13.2394\n",
      "\u001b[1;4mValidati 194    - 100    -          0.3356 -           0.9245    | 0.9250   - 0.9757\u001b[0m\n",
      "Training 195    - 100    -          0.0087 -           0.9978    | 0.9978   - 13.2635\n",
      "\u001b[1;4mValidati 195    - 100    -          0.3337 -           0.9245    | 0.9252   - 0.9640\u001b[0m\n",
      "Training 196    - 100    -          0.0082 -           0.9979    | 0.9980   - 11.4044\n",
      "\u001b[1;4mValidati 196    - 100    -          0.3419 -           0.9241    | 0.9246   - 0.9358\u001b[0m\n",
      "Training 197    - 100    -          0.0082 -           0.9979    | 0.9979   - 13.1033\n",
      "\u001b[1;4mValidati 197    - 100    -          0.3473 -           0.9245    | 0.9248   - 0.9396\u001b[0m\n",
      "Training 198    - 100    -          0.0081 -           0.9980    | 0.9980   - 13.4296\n",
      "\u001b[1;4mValidati 198    - 100    -          0.3420 -           0.9253    | 0.9260   - 0.9592\u001b[0m\n",
      "Training 199    - 100    -          0.0079 -           0.9981    | 0.9982   - 13.0829\n",
      "\u001b[1;4mValidati 199    - 100    -          0.3408 -           0.9247    | 0.9252   - 0.9927\u001b[0m\n",
      "Training 200    - 100    -          0.0076 -           0.9981    | 0.9981   - 13.6024\n",
      "\u001b[1;4mValidati 200    - 100    -          0.3372 -           0.9256    | 0.9258   - 0.9612\u001b[0m\n",
      "Training 201    - 100    -          0.0078 -           0.9981    | 0.9981   - 13.3097\n",
      "\u001b[1;4mValidati 201    - 100    -          0.3447 -           0.9231    | 0.9232   - 0.9851\u001b[0m\n",
      "Training 202    - 100    -          0.0080 -           0.9979    | 0.9979   - 12.9841\n",
      "\u001b[1;4mValidati 202    - 100    -          0.3418 -           0.9259    | 0.9268   - 0.9828\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 203    - 100    -          0.0087 -           0.9978    | 0.9978   - 13.1141\n",
      "\u001b[1;4mValidati 203    - 100    -          0.3317 -           0.9245    | 0.9253   - 1.0007\u001b[0m\n",
      "Training 204    - 100    -          0.0084 -           0.9977    | 0.9977   - 13.1007\n",
      "\u001b[1;4mValidati 204    - 100    -          0.3370 -           0.9248    | 0.9254   - 0.9390\u001b[0m\n",
      "Training 205    - 100    -          0.0080 -           0.9979    | 0.9979   - 12.1034\n",
      "\u001b[1;4mValidati 205    - 100    -          0.3351 -           0.9264    | 0.9274   - 1.0008\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 206    - 100    -          0.0082 -           0.9979    | 0.9979   - 13.1537\n",
      "\u001b[1;4mValidati 206    - 100    -          0.3334 -           0.9263    | 0.9269   - 0.9638\u001b[0m\n",
      "Training 207    - 100    -          0.0081 -           0.9977    | 0.9977   - 13.3044\n",
      "\u001b[1;4mValidati 207    - 100    -          0.3344 -           0.9246    | 0.9251   - 0.9422\u001b[0m\n",
      "Training 208    - 100    -          0.0081 -           0.9978    | 0.9978   - 13.1667\n",
      "\u001b[1;4mValidati 208    - 100    -          0.3525 -           0.9233    | 0.9239   - 0.9395\u001b[0m\n",
      "Training 209    - 100    -          0.0072 -           0.9983    | 0.9983   - 13.3319\n",
      "\u001b[1;4mValidati 209    - 100    -          0.3368 -           0.9272    | 0.9279   - 1.0410\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 210    - 100    -          0.0075 -           0.9979    | 0.9979   - 13.0329\n",
      "\u001b[1;4mValidati 210    - 100    -          0.3457 -           0.9238    | 0.9247   - 0.9901\u001b[0m\n",
      "Training 211    - 100    -          0.0076 -           0.9979    | 0.9979   - 13.2791\n",
      "\u001b[1;4mValidati 211    - 100    -          0.3361 -           0.9261    | 0.9271   - 1.0420\u001b[0m\n",
      "Training 212    - 100    -          0.0070 -           0.9981    | 0.9980   - 13.4134\n",
      "\u001b[1;4mValidati 212    - 100    -          0.3450 -           0.9241    | 0.9250   - 0.9615\u001b[0m\n",
      "Training 213    - 100    -          0.0086 -           0.9975    | 0.9976   - 13.2096\n",
      "\u001b[1;4mValidati 213    - 100    -          0.3432 -           0.9245    | 0.9255   - 0.9465\u001b[0m\n",
      "Training 214    - 100    -          0.0073 -           0.9981    | 0.9982   - 12.9453\n",
      "\u001b[1;4mValidati 214    - 100    -          0.3477 -           0.9238    | 0.9239   - 1.0144\u001b[0m\n",
      "Training 215    - 100    -          0.0065 -           0.9984    | 0.9983   - 13.3845\n",
      "\u001b[1;4mValidati 215    - 100    -          0.3464 -           0.9268    | 0.9271   - 0.9725\u001b[0m\n",
      "Training 216    - 100    -          0.0071 -           0.9983    | 0.9983   - 13.4007\n",
      "\u001b[1;4mValidati 216    - 100    -          0.3501 -           0.9236    | 0.9244   - 0.8890\u001b[0m\n",
      "Training 217    - 100    -          0.0076 -           0.9979    | 0.9979   - 11.6534\n",
      "\u001b[1;4mValidati 217    - 100    -          0.3495 -           0.9240    | 0.9251   - 0.9455\u001b[0m\n",
      "Training 218    - 100    -          0.0075 -           0.9978    | 0.9978   - 13.2823\n",
      "\u001b[1;4mValidati 218    - 100    -          0.3376 -           0.9259    | 0.9264   - 0.9477\u001b[0m\n",
      "Training 219    - 100    -          0.0074 -           0.9981    | 0.9981   - 12.9253\n",
      "\u001b[1;4mValidati 219    - 100    -          0.3500 -           0.9239    | 0.9244   - 0.9759\u001b[0m\n",
      "Training 220    - 100    -          0.0067 -           0.9984    | 0.9983   - 13.3898\n",
      "\u001b[1;4mValidati 220    - 100    -          0.3449 -           0.9262    | 0.9269   - 0.9385\u001b[0m\n",
      "Training 221    - 100    -          0.0066 -           0.9983    | 0.9984   - 13.0114\n",
      "\u001b[1;4mValidati 221    - 100    -          0.3561 -           0.9214    | 0.9224   - 0.9944\u001b[0m\n",
      "Training 222    - 100    -          0.0069 -           0.9982    | 0.9981   - 10.8094\n",
      "\u001b[1;4mValidati 222    - 100    -          0.3460 -           0.9247    | 0.9249   - 0.9140\u001b[0m\n",
      "Training 223    - 100    -          0.0070 -           0.9982    | 0.9982   - 12.4243\n",
      "\u001b[1;4mValidati 223    - 100    -          0.3374 -           0.9264    | 0.9263   - 1.0262\u001b[0m\n",
      "Training 224    - 100    -          0.0065 -           0.9985    | 0.9984   - 13.3135\n",
      "\u001b[1;4mValidati 224    - 100    -          0.3368 -           0.9262    | 0.9267   - 0.9501\u001b[0m\n",
      "Training 225    - 100    -          0.0068 -           0.9984    | 0.9984   - 13.3647\n",
      "\u001b[1;4mValidati 225    - 100    -          0.3470 -           0.9258    | 0.9263   - 0.9433\u001b[0m\n",
      "Training 226    - 100    -          0.0064 -           0.9984    | 0.9984   - 13.2214\n",
      "\u001b[1;4mValidati 226    - 100    -          0.3514 -           0.9241    | 0.9243   - 0.9633\u001b[0m\n",
      "Training 227    - 100    -          0.0063 -           0.9984    | 0.9984   - 12.9557\n",
      "\u001b[1;4mValidati 227    - 100    -          0.3473 -           0.9246    | 0.9250   - 0.9608\u001b[0m\n",
      "Training 228    - 100    -          0.0068 -           0.9983    | 0.9983   - 13.4256\n",
      "\u001b[1;4mValidati 228    - 100    -          0.3503 -           0.9238    | 0.9250   - 0.9563\u001b[0m\n",
      "Training 229    - 100    -          0.0068 -           0.9983    | 0.9983   - 13.0663\n",
      "\u001b[1;4mValidati 229    - 100    -          0.3497 -           0.9234    | 0.9238   - 0.9094\u001b[0m\n",
      "Training 230    - 100    -          0.0071 -           0.9983    | 0.9983   - 13.1690\n",
      "\u001b[1;4mValidati 230    - 100    -          0.3417 -           0.9241    | 0.9247   - 0.9605\u001b[0m\n",
      "Training 231    - 100    -          0.0075 -           0.9980    | 0.9980   - 13.3329\n",
      "\u001b[1;4mValidati 231    - 100    -          0.3450 -           0.9235    | 0.9243   - 0.9293\u001b[0m\n",
      "Training 232    - 100    -          0.0064 -           0.9985    | 0.9985   - 13.1560\n",
      "\u001b[1;4mValidati 232    - 100    -          0.3514 -           0.9233    | 0.9238   - 0.9799\u001b[0m\n",
      "Training 233    - 100    -          0.0075 -           0.9980    | 0.9980   - 13.0611\n",
      "\u001b[1;4mValidati 233    - 100    -          0.3448 -           0.9243    | 0.9250   - 1.0278\u001b[0m\n",
      "Training 234    - 100    -          0.0066 -           0.9983    | 0.9983   - 13.0784\n",
      "\u001b[1;4mValidati 234    - 100    -          0.3471 -           0.9225    | 0.9233   - 1.0083\u001b[0m\n",
      "Training 235    - 100    -          0.0073 -           0.9980    | 0.9980   - 11.9183\n",
      "\u001b[1;4mValidati 235    - 100    -          0.3499 -           0.9238    | 0.9244   - 0.9660\u001b[0m\n",
      "Training 236    - 100    -          0.0069 -           0.9982    | 0.9982   - 13.2623\n",
      "\u001b[1;4mValidati 236    - 100    -          0.3394 -           0.9240    | 0.9249   - 0.9893\u001b[0m\n",
      "Training 237    - 100    -          0.0067 -           0.9984    | 0.9983   - 13.2271\n",
      "\u001b[1;4mValidati 237    - 100    -          0.3373 -           0.9251    | 0.9254   - 0.9653\u001b[0m\n",
      "Training 238    - 100    -          0.0064 -           0.9983    | 0.9984   - 13.2640\n",
      "\u001b[1;4mValidati 238    - 100    -          0.3434 -           0.9265    | 0.9267   - 0.9480\u001b[0m\n",
      "Training 239    - 100    -          0.0066 -           0.9983    | 0.9982   - 13.4455\n",
      "\u001b[1;4mValidati 239    - 100    -          0.3437 -           0.9253    | 0.9258   - 0.9416\u001b[0m\n",
      "Training 240    - 100    -          0.0061 -           0.9985    | 0.9985   - 13.3585\n",
      "\u001b[1;4mValidati 240    - 100    -          0.3436 -           0.9242    | 0.9248   - 0.9746\u001b[0m\n",
      "Training 241    - 100    -          0.0060 -           0.9985    | 0.9984   - 13.1481\n",
      "\u001b[1;4mValidati 241    - 100    -          0.3408 -           0.9252    | 0.9258   - 0.9469\u001b[0m\n",
      "Training 242    - 100    -          0.0064 -           0.9984    | 0.9984   - 13.8996\n",
      "\u001b[1;4mValidati 242    - 100    -          0.3524 -           0.9244    | 0.9249   - 0.9900\u001b[0m\n",
      "Training 243    - 100    -          0.0059 -           0.9984    | 0.9985   - 14.1633\n",
      "\u001b[1;4mValidati 243    - 100    -          0.3468 -           0.9250    | 0.9254   - 0.9293\u001b[0m\n",
      "Training 244    - 100    -          0.0065 -           0.9983    | 0.9983   - 11.7597\n",
      "\u001b[1;4mValidati 244    - 100    -          0.3423 -           0.9254    | 0.9264   - 0.9454\u001b[0m\n",
      "Training 245    - 100    -          0.0062 -           0.9984    | 0.9984   - 13.0416\n",
      "\u001b[1;4mValidati 245    - 100    -          0.3503 -           0.9220    | 0.9226   - 0.9409\u001b[0m\n",
      "Training 246    - 100    -          0.0065 -           0.9984    | 0.9983   - 13.2303\n",
      "\u001b[1;4mValidati 246    - 100    -          0.3461 -           0.9259    | 0.9268   - 0.9774\u001b[0m\n",
      "Training 247    - 100    -          0.0065 -           0.9983    | 0.9983   - 12.9935\n",
      "\u001b[1;4mValidati 247    - 100    -          0.3481 -           0.9224    | 0.9230   - 0.9661\u001b[0m\n",
      "Training 248    - 100    -          0.0066 -           0.9982    | 0.9982   - 13.3523\n",
      "\u001b[1;4mValidati 248    - 100    -          0.3407 -           0.9247    | 0.9255   - 0.9475\u001b[0m\n",
      "Training 249    - 100    -          0.0070 -           0.9982    | 0.9982   - 13.1727\n",
      "\u001b[1;4mValidati 249    - 100    -          0.3420 -           0.9266    | 0.9272   - 0.9568\u001b[0m\n",
      "Training 250    - 100    -          0.0057 -           0.9987    | 0.9987   - 13.0638\n",
      "\u001b[1;4mValidati 250    - 100    -          0.3521 -           0.9225    | 0.9229   - 0.9565\u001b[0m\n",
      "Training 251    - 100    -          0.0066 -           0.9982    | 0.9982   - 13.1800\n",
      "\u001b[1;4mValidati 251    - 100    -          0.3531 -           0.9225    | 0.9230   - 0.8956\u001b[0m\n",
      "Training 252    - 100    -          0.0063 -           0.9986    | 0.9986   - 13.3099\n",
      "\u001b[1;4mValidati 252    - 100    -          0.3476 -           0.9250    | 0.9260   - 1.0036\u001b[0m\n",
      "Training 253    - 100    -          0.0063 -           0.9984    | 0.9984   - 13.2531\n",
      "\u001b[1;4mValidati 253    - 100    -          0.3485 -           0.9237    | 0.9241   - 0.9333\u001b[0m\n",
      "Training 254    - 100    -          0.0068 -           0.9981    | 0.9981   - 12.7947\n",
      "\u001b[1;4mValidati 254    - 100    -          0.3446 -           0.9236    | 0.9242   - 0.9827\u001b[0m\n",
      "Training 255    - 100    -          0.0061 -           0.9984    | 0.9984   - 12.9615\n",
      "\u001b[1;4mValidati 255    - 100    -          0.3469 -           0.9251    | 0.9253   - 0.9396\u001b[0m\n",
      "Training 256    - 100    -          0.0064 -           0.9981    | 0.9982   - 12.9869\n",
      "\u001b[1;4mValidati 256    - 100    -          0.3494 -           0.9243    | 0.9247   - 1.0209\u001b[0m\n",
      "Training 257    - 100    -          0.0064 -           0.9983    | 0.9983   - 12.8462\n",
      "\u001b[1;4mValidati 257    - 100    -          0.3546 -           0.9241    | 0.9241   - 0.9586\u001b[0m\n",
      "Training 258    - 100    -          0.0055 -           0.9987    | 0.9987   - 12.8610\n",
      "\u001b[1;4mValidati 258    - 100    -          0.3495 -           0.9257    | 0.9266   - 1.0046\u001b[0m\n",
      "Training 259    - 100    -          0.0061 -           0.9985    | 0.9985   - 13.0124\n",
      "\u001b[1;4mValidati 259    - 100    -          0.3592 -           0.9241    | 0.9245   - 0.9986\u001b[0m\n",
      "Training 260    - 100    -          0.0063 -           0.9985    | 0.9985   - 12.8784\n",
      "\u001b[1;4mValidati 260    - 100    -          0.3516 -           0.9250    | 0.9253   - 0.9670\u001b[0m\n",
      "Training 261    - 100    -          0.0068 -           0.9981    | 0.9982   - 13.4978\n",
      "\u001b[1;4mValidati 261    - 100    -          0.3540 -           0.9230    | 0.9237   - 0.9341\u001b[0m\n",
      "Training 262    - 100    -          0.0063 -           0.9984    | 0.9985   - 13.1970\n",
      "\u001b[1;4mValidati 262    - 100    -          0.3460 -           0.9248    | 0.9254   - 0.9633\u001b[0m\n",
      "Training 263    - 100    -          0.0059 -           0.9986    | 0.9985   - 13.0307\n",
      "\u001b[1;4mValidati 263    - 100    -          0.3513 -           0.9241    | 0.9241   - 1.0150\u001b[0m\n",
      "Training 264    - 100    -          0.0069 -           0.9981    | 0.9980   - 13.1002\n",
      "\u001b[1;4mValidati 264    - 100    -          0.3513 -           0.9238    | 0.9235   - 0.9475\u001b[0m\n",
      "Training 265    - 100    -          0.0068 -           0.9983    | 0.9982   - 13.0046\n",
      "\u001b[1;4mValidati 265    - 100    -          0.3447 -           0.9259    | 0.9266   - 1.0349\u001b[0m\n",
      "Training 266    - 100    -          0.0067 -           0.9982    | 0.9982   - 13.0145\n",
      "\u001b[1;4mValidati 266    - 100    -          0.3503 -           0.9234    | 0.9237   - 1.0158\u001b[0m\n",
      "Training 267    - 100    -          0.0066 -           0.9984    | 0.9984   - 13.1747\n",
      "\u001b[1;4mValidati 267    - 100    -          0.3532 -           0.9234    | 0.9234   - 0.9587\u001b[0m\n",
      "Training 268    - 100    -          0.0063 -           0.9983    | 0.9984   - 10.6773\n",
      "\u001b[1;4mValidati 268    - 100    -          0.3574 -           0.9220    | 0.9224   - 0.9225\u001b[0m\n",
      "Training 269    - 100    -          0.0065 -           0.9983    | 0.9983   - 13.3323\n",
      "\u001b[1;4mValidati 269    - 100    -          0.3633 -           0.9237    | 0.9235   - 0.9713\u001b[0m\n",
      "Training 270    - 100    -          0.0065 -           0.9985    | 0.9985   - 12.9627\n",
      "\u001b[1;4mValidati 270    - 100    -          0.3636 -           0.9241    | 0.9249   - 0.9193\u001b[0m\n",
      "Training 271    - 100    -          0.0058 -           0.9986    | 0.9986   - 13.1397\n",
      "\u001b[1;4mValidati 271    - 100    -          0.3503 -           0.9269    | 0.9273   - 0.9890\u001b[0m\n",
      "Training 272    - 100    -          0.0062 -           0.9984    | 0.9984   - 13.0793\n",
      "\u001b[1;4mValidati 272    - 100    -          0.3560 -           0.9241    | 0.9242   - 1.0015\u001b[0m\n",
      "Training 273    - 100    -          0.0066 -           0.9983    | 0.9983   - 12.9869\n",
      "\u001b[1;4mValidati 273    - 100    -          0.3557 -           0.9268    | 0.9266   - 0.9849\u001b[0m\n",
      "Training 274    - 100    -          0.0060 -           0.9985    | 0.9986   - 13.3783\n",
      "\u001b[1;4mValidati 274    - 100    -          0.3505 -           0.9257    | 0.9264   - 1.0077\u001b[0m\n",
      "Training 275    - 100    -          0.0054 -           0.9988    | 0.9988   - 12.8244\n",
      "\u001b[1;4mValidati 275    - 100    -          0.3527 -           0.9257    | 0.9257   - 0.9160\u001b[0m\n",
      "Training 276    - 100    -          0.0055 -           0.9987    | 0.9987   - 13.3265\n",
      "\u001b[1;4mValidati 276    - 100    -          0.3556 -           0.9240    | 0.9245   - 1.0462\u001b[0m\n",
      "Training 277    - 100    -          0.0059 -           0.9985    | 0.9986   - 13.2805\n",
      "\u001b[1;4mValidati 277    - 100    -          0.3623 -           0.9239    | 0.9245   - 0.9807\u001b[0m\n",
      "Training 278    - 100    -          0.0055 -           0.9986    | 0.9986   - 13.2778\n",
      "\u001b[1;4mValidati 278    - 100    -          0.3524 -           0.9240    | 0.9247   - 0.9623\u001b[0m\n",
      "Training 279    - 100    -          0.0061 -           0.9984    | 0.9984   - 13.3853\n",
      "\u001b[1;4mValidati 279    - 100    -          0.3519 -           0.9249    | 0.9260   - 1.0522\u001b[0m\n",
      "Training 280    - 100    -          0.0060 -           0.9986    | 0.9986   - 12.5069\n",
      "\u001b[1;4mValidati 280    - 100    -          0.3579 -           0.9228    | 0.9236   - 1.0493\u001b[0m\n",
      "Training 281    - 100    -          0.0065 -           0.9982    | 0.9983   - 13.2965\n",
      "\u001b[1;4mValidati 281    - 100    -          0.3535 -           0.9243    | 0.9249   - 0.9793\u001b[0m\n",
      "Training 282    - 100    -          0.0065 -           0.9982    | 0.9982   - 13.1335\n",
      "\u001b[1;4mValidati 282    - 100    -          0.3570 -           0.9241    | 0.9239   - 0.9532\u001b[0m\n",
      "Training 283    - 100    -          0.0068 -           0.9982    | 0.9981   - 13.3309\n",
      "\u001b[1;4mValidati 283    - 100    -          0.3551 -           0.9258    | 0.9260   - 0.9472\u001b[0m\n",
      "Training 284    - 100    -          0.0052 -           0.9986    | 0.9986   - 12.7037\n",
      "\u001b[1;4mValidati 284    - 100    -          0.3590 -           0.9235    | 0.9238   - 0.9663\u001b[0m\n",
      "Training 285    - 100    -          0.0059 -           0.9984    | 0.9984   - 13.2742\n",
      "\u001b[1;4mValidati 285    - 100    -          0.3510 -           0.9249    | 0.9256   - 0.9574\u001b[0m\n",
      "Training 286    - 100    -          0.0062 -           0.9983    | 0.9983   - 13.1262\n",
      "\u001b[1;4mValidati 286    - 100    -          0.3542 -           0.9226    | 0.9224   - 0.9813\u001b[0m\n",
      "Training 287    - 100    -          0.0070 -           0.9983    | 0.9982   - 13.2959\n",
      "\u001b[1;4mValidati 287    - 100    -          0.3484 -           0.9251    | 0.9255   - 0.9409\u001b[0m\n",
      "Training 288    - 100    -          0.0062 -           0.9983    | 0.9983   - 12.4751\n",
      "\u001b[1;4mValidati 288    - 100    -          0.3488 -           0.9220    | 0.9229   - 0.9889\u001b[0m\n",
      "Training 289    - 100    -          0.0059 -           0.9986    | 0.9986   - 10.7318\n",
      "\u001b[1;4mValidati 289    - 100    -          0.3554 -           0.9231    | 0.9237   - 0.9502\u001b[0m\n",
      "Training 290    - 100    -          0.0066 -           0.9982    | 0.9983   - 13.4447\n",
      "\u001b[1;4mValidati 290    - 100    -          0.3514 -           0.9237    | 0.9239   - 1.0125\u001b[0m\n",
      "Training 291    - 100    -          0.0066 -           0.9984    | 0.9984   - 13.2795\n",
      "\u001b[1;4mValidati 291    - 100    -          0.3521 -           0.9221    | 0.9223   - 0.9748\u001b[0m\n",
      "Training 292    - 100    -          0.0066 -           0.9983    | 0.9983   - 13.4682\n",
      "\u001b[1;4mValidati 292    - 100    -          0.3524 -           0.9228    | 0.9234   - 0.9296\u001b[0m\n",
      "Training 293    - 100    -          0.0066 -           0.9983    | 0.9983   - 13.2051\n",
      "\u001b[1;4mValidati 293    - 100    -          0.3578 -           0.9228    | 0.9234   - 0.9820\u001b[0m\n",
      "Training 294    - 100    -          0.0052 -           0.9988    | 0.9988   - 12.8684\n",
      "\u001b[1;4mValidati 294    - 100    -          0.3490 -           0.9221    | 0.9224   - 1.0068\u001b[0m\n",
      "Training 295    - 100    -          0.0064 -           0.9985    | 0.9985   - 13.1539\n",
      "\u001b[1;4mValidati 295    - 100    -          0.3495 -           0.9221    | 0.9230   - 0.9469\u001b[0m\n",
      "Training 296    - 100    -          0.0060 -           0.9985    | 0.9986   - 13.2947\n",
      "\u001b[1;4mValidati 296    - 100    -          0.3596 -           0.9219    | 0.9227   - 1.0103\u001b[0m\n",
      "Training 297    - 100    -          0.0066 -           0.9981    | 0.9981   - 13.1751\n",
      "\u001b[1;4mValidati 297    - 100    -          0.3592 -           0.9226    | 0.9232   - 0.9472\u001b[0m\n",
      "Training 298    - 100    -          0.0066 -           0.9980    | 0.9981   - 13.5227\n",
      "\u001b[1;4mValidati 298    - 100    -          0.3514 -           0.9241    | 0.9248   - 0.9193\u001b[0m\n",
      "Training 299    - 100    -          0.0069 -           0.9982    | 0.9982   - 13.3231\n",
      "\u001b[1;4mValidati 299    - 100    -          0.3728 -           0.9200    | 0.9206   - 0.9540\u001b[0m\n",
      "Training 300    - 100    -          0.0067 -           0.9982    | 0.9982   - 13.3538\n",
      "\u001b[1;4mValidati 300    - 100    -          0.3614 -           0.9241    | 0.9247   - 0.9229\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "start_epoch = checkpoint.epoch_counter\n",
    "end_epoch = args.nb_epoch\n",
    "\n",
    "for e in range(start_epoch, args.nb_epoch):\n",
    "    train(e)\n",
    "    val(e)\n",
    "    tensorboard.flush()\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â™«â™ª.Ä±lÄ±lÄ±ll|Ì…Ì²Ì…â—Ì…Ì²Ì…|Ì…Ì²Ì…=Ì…Ì²Ì…|Ì…Ì²Ì…â—Ì…Ì²Ì…|llÄ±lÄ±lÄ±.â™«â™ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}