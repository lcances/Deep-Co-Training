{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#Utils\" data-toc-modified-id=\"Utils-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Utils</a></span></li><li><span><a href=\"#Load-and-prepare-the-data\" data-toc-modified-id=\"Load-and-prepare-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load and prepare the data</a></span></li><li><span><a href=\"#Create-model\" data-toc-modified-id=\"Create-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create model</a></span></li><li><span><a href=\"#Loss-function-&amp;-warmup\" data-toc-modified-id=\"Loss-function-&amp;-warmup-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Loss function &amp; warmup</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normal-train\" data-toc-modified-id=\"Normal-train-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Normal train</a></span></li></ul></li><li><span><a href=\"#End\" data-toc-modified-id=\"End-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>End</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:19.642471Z",
     "start_time": "2019-11-03T21:58:19.001919Z"
    },
    "run_control": {
     "marked": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:19.661276Z",
     "start_time": "2019-11-03T21:58:19.645588Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.value = 0\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.count += 1\n",
    "\n",
    "        \n",
    "class BinaryAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = (y_pred>0.5).float()\n",
    "            correct = (y_pred == y_true).float().sum()\n",
    "            self.value = correct/ (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "        \n",
    "class CategoricalAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true, maxi: bool = True):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.value = torch.mean((y_true == y_pred).float())\n",
    "            self.accumulate_value += self.value\n",
    "\n",
    "            return self.accumulate_value / self.count\n",
    "\n",
    "        \n",
    "class Ratio(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_adv_pred):\n",
    "        super().__call__()\n",
    "        \n",
    "        results = zip(y_pred, y_adv_pred)\n",
    "        results_bool = [int(r[0] != r[1]) for r in results]\n",
    "        self.value = sum(results_bool) / len(results_bool) * 100\n",
    "        self.accumulate_value += self.value\n",
    "        \n",
    "        return self.accumulate_value / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:19.674106Z",
     "start_time": "2019-11-03T21:58:19.663583Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:19.681993Z",
     "start_time": "2019-11-03T21:58:19.676414Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:19.691448Z",
     "start_time": "2019-11-03T21:58:19.683832Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:21.447513Z",
     "start_time": "2019-11-03T21:58:19.693283Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c45f938db469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdataset_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files already downloaded and verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmakedir_exist_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# downloads file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mmakedir_exist_ok\u001b[0;34m(dirpath)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEXIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '../dataset'"
     ],
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../dataset'",
     "output_type": "error"
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomAffine(0, translate=(1/16, 1/16)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_root = os.path.join(\"..\", \"dataset\")\n",
    "train_set = torchvision.datasets.CIFAR10(dataset_root, train=True, download=True, transform=transform_train)\n",
    "val_set = torchvision.datasets.CIFAR10(dataset_root, train=False, download=True, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:30.152347Z",
     "start_time": "2019-11-03T21:58:21.449982Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "S_idx, U_idx = [], []\n",
    "classes = [[] for _ in range(10)]\n",
    "\n",
    "for i in tqdm.tqdm(range(len(train_set))):\n",
    "    data, label = train_set[i]\n",
    "    classes[label].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:30.162019Z",
     "start_time": "2019-11-03T21:58:30.155018Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for indexes in classes:\n",
    "    np.random.shuffle(indexes)\n",
    "    S_idx += indexes[:400]\n",
    "    U_idx += indexes[400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:30.196887Z",
     "start_time": "2019-11-03T21:58:30.163613Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class repro(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        \n",
    "#         self.gaussian = GaussianNoise(sigma=0.15)\n",
    "        self.features = torch.nn.Sequential(\n",
    "            weight_norm(nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(128, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            weight_norm(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            weight_norm(nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=0)),\n",
    "            nn.BatchNorm2d(512, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0)),\n",
    "            nn.BatchNorm2d(128, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.AvgPool2d(6, stride=2, padding=0),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            weight_norm(nn.Linear(128, 10))\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "#         x = self.gaussian(x)\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 128)\n",
    "#         x = nn.functional.avg_pool2d(x, kernel_size=(6,6))\n",
    "#         x = x.view(-1, 128)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function & warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:30.208971Z",
     "start_time": "2019-11-03T21:58:30.198421Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def Lsup(logit_S1, logit_S2, labels_S1, labels_S2):\n",
    "    ce = nn.CrossEntropyLoss() \n",
    "    loss1 = ce(logit_S1, labels_S1)\n",
    "    loss2 = ce(logit_S2, labels_S2) \n",
    "    return (loss1+loss2)\n",
    "\n",
    "def Lcot(U_p1, U_p2):\n",
    "# the Jensen-Shannon divergence between p1(x) and p2(x)\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    a1 = 0.5 * (S(U_p1) + S(U_p2))\n",
    "    loss1 = a1 * torch.log(a1)\n",
    "    loss1 = -torch.sum(loss1)\n",
    "    loss2 = S(U_p1) * LS(U_p1)\n",
    "    loss2 = -torch.sum(loss2)\n",
    "    loss3 = S(U_p2) * LS(U_p2)\n",
    "    loss3 = -torch.sum(loss3)\n",
    "\n",
    "    return (loss1 - 0.5 * (loss2 + loss3))/U_batch_size\n",
    "\n",
    "\n",
    "def Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2):\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    a = S(logit_S2) * LS(perturbed_logit_S1)\n",
    "    a = torch.sum(a)\n",
    "\n",
    "    b = S(logit_S1) * LS(perturbed_logit_S2)\n",
    "    b = torch.sum(b)\n",
    "\n",
    "    c = S(logit_U2) * LS(perturbed_logit_U1)\n",
    "    c = torch.sum(c)\n",
    "\n",
    "    d = S(logit_U1) * LS(perturbed_logit_U2)\n",
    "    d = torch.sum(d)\n",
    "\n",
    "    return -(a+b+c+d)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:30.220371Z",
     "start_time": "2019-11-03T21:58:30.210200Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"cosine scheduling\"\"\"\n",
    "    epoch = epoch + 1\n",
    "    lr = 0.05*(1.0 + np.cos((epoch-1)*np.pi/600))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def adjust_lamda(epoch):\n",
    "    epoch = epoch + 1\n",
    "    global lambda_cot\n",
    "    global lambda_diff\n",
    "    if epoch <= 80:\n",
    "        lambda_cot = lambda_cot_max*np.exp(-5*(1-epoch/80)**2)\n",
    "        lambda_diff = lambda_diff_max*np.exp(-5*(1-epoch/80)**2)\n",
    "    else: \n",
    "        lambda_cot = lambda_cot_max\n",
    "        lambda_diff = lambda_diff_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:32.586871Z",
     "start_time": "2019-11-03T21:58:30.221662Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "nb_view = 4\n",
    "\n",
    "if nb_view % 2 != 0:\n",
    "    raise AssertionError(\"Nb view must be a multiple of 2\")\n",
    "\n",
    "model_func = repro\n",
    "\n",
    "models = [model_func() for _ in range(nb_view)]\n",
    "\n",
    "for m in models:\n",
    "    m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:32.598142Z",
     "start_time": "2019-11-03T21:58:32.588558Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "U_batch_size = int(batch_size * 46./50.)\n",
    "S_batch_size = batch_size - U_batch_size\n",
    "nb_batch = len(train_set) // batch_size\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"repro_cotraining/%d_views_%s_%s\" % (nb_view, get_datetime(), model_func.__name__), comment=model_func.__name__)\n",
    "    \n",
    "S_sampler = torch.utils.data.SubsetRandomSampler(S_idx)\n",
    "U_sampler = torch.utils.data.SubsetRandomSampler(U_idx)\n",
    "\n",
    "S_loaders = []\n",
    "for _ in range(nb_view):\n",
    "    S_loaders.append(torch.utils.data.DataLoader(train_set, batch_size=S_batch_size, num_workers=2, sampler=S_sampler))\n",
    "U_loader = torch.utils.data.DataLoader(train_set, batch_size=U_batch_size, num_workers=2, sampler=U_sampler)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "\n",
    "adv_generators = []\n",
    "for i in range(nb_view):\n",
    "    adv_generators.append(\n",
    "        GradientSignAttack( \n",
    "            models[i],\n",
    "            loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "            eps=0.02, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:32.631710Z",
     "start_time": "2019-11-03T21:58:32.606837Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lambda_cot_max = 10\n",
    "lambda_diff_max = 0.5\n",
    "lambda_cot = 0.0\n",
    "lambda_diff = 0.0\n",
    "best_acc = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T21:58:32.642628Z",
     "start_time": "2019-11-03T21:58:32.633395Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "parameters = list(models[0].parameters())\n",
    "for i in range(1, nb_view):\n",
    "    parameters += list(models[i].parameters())\n",
    "    \n",
    "optimizer = torch.optim.SGD(\n",
    "    parameters,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    "    lr=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T21:56:31.106Z"
    },
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "accuracies_func = [CategoricalAccuracy() for _ in range(nb_view)]\n",
    "ratios_func = [Ratio() for _ in range(nb_view)]\n",
    "\n",
    "def train(epoch):\n",
    "    # Real Multi-view implementation\n",
    "    # Pair are randomly chose at each iteration\n",
    "    if nb_view >= 4:\n",
    "        nb_tuple = nb_view // 2\n",
    "        m_index = np.asarray(range(nb_view))\n",
    "        np.random.shuffle(m_index)\n",
    "        model_couples = np.split(m_index, nb_tuple)\n",
    "    else:\n",
    "        model_couples = ((0, 1),)\n",
    "            \n",
    "    for m in models:\n",
    "        m.train()\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    adjust_lamda(epoch)\n",
    "    \n",
    "    # reset metrics\n",
    "    for i in range(nb_view):\n",
    "        accuracies_func[i].reset()\n",
    "        ratios_func[i].reset()\n",
    "        \n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    # create iterator for b1, b2, bu\n",
    "    S_iterators = [iter(s_loader) for s_loader in S_loaders]\n",
    "    U_iter = iter(U_loader)\n",
    "    \n",
    "    print('')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for b in range(nb_batch):\n",
    "        X_S, y_S = [], []\n",
    "        for s_iterator in S_iterators:\n",
    "            X, y = s_iterator.next()\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            X_S.append(X)\n",
    "            y_S.append(y)\n",
    "\n",
    "        X_U, y_U = U_iter.next() # note that labels_U will not be used for training. \n",
    "        X_U = X_U.cuda()    \n",
    "        y_U = y_U.cuda()\n",
    "\n",
    "        logits_S = [models[i](X_S[i]) for i in range(nb_view)]\n",
    "        logits_U = [models[i](X_U) for i in range(nb_view)]\n",
    "    \n",
    "        predictions_S, predictions_U = [], []\n",
    "        for i in range(nb_view):\n",
    "            _, p_s = torch.max(logits_S[i], 1)\n",
    "            _, p_u = torch.max(logits_U[i], 1)\n",
    "            \n",
    "            predictions_S.append(p_s)\n",
    "            predictions_U.append(p_u)\n",
    "\n",
    "        # fix batchnorm\n",
    "        for m in models:\n",
    "            m.eval()\n",
    "    \n",
    "        #generate adversarial examples\n",
    "        adv_S, adv_U = [], []\n",
    "        for i in range(nb_view):\n",
    "            adv_S.append(adv_generators[i].perturb(X_S[i], y_S[i]))\n",
    "            adv_U.append(adv_generators[i].perturb(X_U, predictions_U[i]))\n",
    "        \n",
    "        for m in models:\n",
    "            m.train()\n",
    "\n",
    "        # Prediction on the adversarial exemple MUST use the couple\n",
    "        # randomly chosen at the beginning of the iteration\n",
    "        adv_logits_S, adv_logits_U = [None] * nb_view, [None] * nb_view\n",
    "        print(\"predict adv\")\n",
    "        for couple in model_couples:\n",
    "            print(\"couple: \", couple)\n",
    "            adv_logits_S[couple[0]] = models[couple[0]](adv_S[couple[1]])\n",
    "            adv_logits_S[couple[1]] = models[couple[1]](adv_S[couple[0]])\n",
    "            \n",
    "            adv_logits_U[couple[0]] = models[couple[0]](adv_U[couple[1]])\n",
    "            adv_logits_U[couple[1]] = models[couple[1]](adv_U[couple[0]])\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        for m in models:\n",
    "            m.zero_grad()\n",
    "        \n",
    "        Loss_sup = 0\n",
    "        Loss_cot = 0\n",
    "        Loss_diff = 0\n",
    "        for couple in model_couples:\n",
    "            Loss_sup += Lsup(\n",
    "                logits_S[couple[0]], logits_S[couple[1]],\n",
    "                y_S[couple[0]], y_S[couple[1]]\n",
    "            )\n",
    "            \n",
    "            Loss_cot += Lcot(logits_U[couple[0]], logits_U[couple[1]])\n",
    "            \n",
    "            Loss_diff += Ldiff(\n",
    "                logits_S[couple[0]], logits_S[couple[1]],\n",
    "                adv_logits_S[couple[0]], adv_logits_S[couple[1]],\n",
    "                logits_U[couple[0]], logits_U[couple[1]],\n",
    "                adv_logits_U[couple[0]], adv_logits_U[couple[1]]\n",
    "            )\n",
    "            \n",
    "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calc the metrics\n",
    "        accuracies, ratios = [], []\n",
    "        for i in range(nb_view):\n",
    "            # accuracy\n",
    "            X_SU = torch.cat((predictions_S[i], predictions_U[i]), 0)\n",
    "            y_SU = torch.cat((y_S[i], y_U), 0)\n",
    "            accuracies.append(accuracies_func[i](X_SU, y_SU))\n",
    "            \n",
    "            # ratio\n",
    "            _, prediction_adv_U = torch.max(adv_logits_U[i], 1)\n",
    "            ratios.append(ratios_func[i](predictions_U[i], prediction_adv_U) )\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        msg = \"Epoch {:4}, {:3d}% \\t acc: \" + \"{:3.4e} \" * nb_view + \" - loss {:3.4e} {:3.4e} {:3.4e} {:3.4e} took: {:.2f}s\"\n",
    "        msg = msg.format(\n",
    "            epoch+1,\n",
    "            int(100 * (b+1) / nb_batch),\n",
    "            \n",
    "            *accuracies,\n",
    "            running_loss/(b+1), ls/(b+1), lc/(b+1), ld/(b+1), \n",
    "            time.time() - start_time,\n",
    "        )\n",
    "        print(msg, end=\"\\r\")\n",
    "            \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "        tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "        for i in range(nb_view):\n",
    "            tensorboard.add_scalar(\"train/acc_%d\" % (i+1), 100. * accuracies[i], epoch )\n",
    "            tensorboard.add_scalar(\"train/ratio_%d\" % (i+1), ratios[i], epoch)\n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    for i in range(nb_view):\n",
    "        models[i].eval()\n",
    "        accuracies_func[i].reset()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            X_val = inputs.cuda()\n",
    "            y_val = targets.cuda()\n",
    "\n",
    "            predictions = []\n",
    "            for m in models:\n",
    "                predictions.append(m(X_val))\n",
    "                \n",
    "            accuracies = []\n",
    "            for i in range(nb_view):\n",
    "                _, pred_class = torch.max(predictions[i], 1)\n",
    "                accuracies.append(accuracies_func[i](pred_class, y_val))\n",
    "            \n",
    "        for i in range(nb_view):\n",
    "            tensorboard.add_scalar('val/acc_%d' % (i+1) , accuracies[i], epoch)\n",
    "\n",
    "        msg = \"Epoch {:4}, {:3d}% \\t val acc: \" + \"{:3.4e} \" * nb_view\n",
    "        msg = msg.format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            \n",
    "            *accuracies,\n",
    "        )\n",
    "        print(\"\")\n",
    "        print(msg)\n",
    "\n",
    "for epoch in range(0, 600):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-18T08:09:18.742Z"
    }
   },
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "443.391px",
    "left": "1741.39px",
    "top": "131.594px",
    "width": "150.531px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}