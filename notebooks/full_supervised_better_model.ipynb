{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from functools import reduce \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from datasetManager import DatasetManager\n",
    "from generators import Generator\n",
    "import signal_augmentations as sa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.973823Z",
     "start_time": "2019-11-12T15:36:12.893994Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.value = 0\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.count += 1\n",
    "\n",
    "        \n",
    "class BinaryAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = (y_pred>0.5).float()\n",
    "            correct = (y_pred == y_true).float().sum()\n",
    "            self.value = correct/ (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "        \n",
    "class CategoricalAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.value = torch.mean((y_true == y_pred).float())\n",
    "            self.accumulate_value += self.value\n",
    "\n",
    "            return self.accumulate_value / self.count\n",
    "\n",
    "        \n",
    "class Ratio(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_adv_pred):\n",
    "        super().__call__()\n",
    "        \n",
    "        results = zip(y_pred, y_adv_pred)\n",
    "        results_bool = [int(r[0] != r[1]) for r in results]\n",
    "        self.value = sum(results_bool) / len(results_bool) * 100\n",
    "        self.accumulate_value += self.value\n",
    "        \n",
    "        return self.accumulate_value / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.997511Z",
     "start_time": "2019-11-12T15:36:12.975482Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:13.020782Z",
     "start_time": "2019-11-12T15:36:13.000410Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_seed(seed=43):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:34.259332Z",
     "start_time": "2019-11-12T15:36:34.233822Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## CNN original\n",
    "https://arxiv.org/pdf/1608.04363.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding,\n",
    "                pool_kernel_size, pool_stride):\n",
    "        super(ConvPoolReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvPoolReLU(1, 24, 3, 1, 1, (4,2), (4,2)),\n",
    "            ConvPoolReLU(24, 48, 3, 1, 1, (4,2), (4,2)),\n",
    "            ConvPoolReLU(48, 48, 3, 1, 1, (4,2), (4,2)),\n",
    "            ConvReLU(48, 48, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1008, 10),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, conv_kernel_size, conv_stride, conv_padding):\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(crnn, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvBNReLU(1, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,2), stride=(4,2)),\n",
    "            ConvBNReLU(64, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,2), stride=(4,2)),\n",
    "            ConvBNReLU(64, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=(4,1), stride=(4,1)),\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.GRU(64, 64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.strong = nn.Sequential(\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        \n",
    "        x = x.squeeze(dim=-2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x, h = self.rnn(x)\n",
    "        \n",
    "        strong = self.strong(x)\n",
    "        \n",
    "        weak = strong.permute(0, 2, 1)\n",
    "        weak = F.avg_pool1d(weak, kernel_size=weak.size()[2:])\n",
    "#         max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        weak = weak.view(-1, weak.shape[1])\n",
    "        \n",
    "        \n",
    "        return weak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultisampleDropout2d(nn.Module):\n",
    "    \"\"\"https://arxiv.org/pdf/1905.09788.pdf\"\"\"\n",
    "    def __init__(self, ratio, nb_sample):\n",
    "        super(MultisampleDropout2d, self).__init__()\n",
    "        self.nb_sample = nb_sample\n",
    "        \n",
    "        self.dropouts = [nn.Dropout2d(ratio) for _ in range(nb_sample)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d = [dropout(x) for dropout in self.dropouts]\n",
    "        return torch.mean(torch.stack(d, dim=0), dim=0)\n",
    "    \n",
    "class MultisampleDropout1d(nn.Module):\n",
    "    \"\"\"https://arxiv.org/pdf/1905.09788.pdf\"\"\"\n",
    "    def __init__(self, ratio, nb_sample):\n",
    "        super(MultisampleDropout1d, self).__init__()\n",
    "        self.nb_sample = nb_sample\n",
    "        \n",
    "        self.dropouts = [nn.Dropout(ratio) for _ in range(nb_sample)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d = [dropout(x) for dropout in self.dropouts]\n",
    "        return torch.mean(torch.stack(d, dim=0), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_size, out_size, t, kernel_size, stride, padding):\n",
    "        super(MBConv, self).__init__()\n",
    "        expand_dim = in_size * t\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_size, expand_dim, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(expand_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(expand_dim, expand_dim, kernel_size=kernel_size, stride=stride, padding=padding, groups=expand_dim),\n",
    "            nn.BatchNorm2d(expand_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(expand_dim, out_size, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.stride == 1:\n",
    "            return x + self.conv(x)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 conv_input_dim: tuple = (64, 431),\n",
    "                 conv_in_size: list = [1, 64, 64],\n",
    "                 conv_out_size: list = [64, 64, 64],\n",
    "                 t = [1, 6, 6],\n",
    "                 s = [1, 2, 2],\n",
    "                 n = [1, 2, 2],\n",
    "                ):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.i =0\n",
    "        \n",
    "        self.conv_input_dim = conv_input_dim\n",
    "        self.conv_in_size = conv_in_size\n",
    "        self.conv_out_size = conv_out_size\n",
    "        self.t = t\n",
    "        \n",
    "        conv_layers = []\n",
    "        for i in range(len(conv_in_size)):\n",
    "            if i == 0:\n",
    "                conv_layers.append(nn.Conv2d(conv_in_size[i], conv_out_size[i], 3, 1, 1))\n",
    "                continue\n",
    "            \n",
    "            conv_layers.append( MBConv(conv_in_size[i], conv_out_size[i], t[i], 3, s[i], 1) )\n",
    "            for j in range(n[i]-1):\n",
    "                conv_layers.append( MBConv(conv_out_size[i], conv_out_size[i], t[i], 3, 1, 1) )\n",
    "    \n",
    "        self.features = nn.Sequential(*conv_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            MultisampleDropout2d(0.2, 8),\n",
    "            nn.Conv2d(self.conv_out_size[-1], 10, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.AdaptiveMaxPool2d((1, 1)),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "#         x = x.view(-1, 1, self.conv_input_dim[0], self.conv_input_dim[1])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        x= x.view(-1, x.shape[1])\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN With dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLUPool(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding,\n",
    "                pool_kernel_size, pool_stride, dropout: float = 0.0):\n",
    "        super(ConvBNReLUPool, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn_d, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvBNReLUPool(1, 32, 3, 1, 1, (4,2), (4,2), 0.0),\n",
    "            ConvBNReLUPool(32, 64, 3, 1, 1, (4,2), (4,2), 0.3),\n",
    "            ConvBNReLUPool(64, 64, 3, 1, 1, (4,2), (4,2), 0.3),\n",
    "            ConvReLU(64, 64, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1344, 10),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN compound scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic find valid scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.linspace(1, 2, 6)\n",
    "beta = np.linspace(1, 2, 6)\n",
    "gamma = np.linspace(1, 1, 1)\n",
    "\n",
    "import itertools\n",
    "\n",
    "valid_scaling = []\n",
    "for a, b, g in itertools.product(alpha, beta, gamma):\n",
    "    M = a * b**2 * g**2\n",
    "    \n",
    "    if M <= 4:\n",
    "        valid_scaling.append((a, b, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0, 1.0),\n",
       " (1.0, 1.2, 1.0),\n",
       " (1.0, 1.4, 1.0),\n",
       " (1.0, 1.6, 1.0),\n",
       " (1.0, 1.8, 1.0),\n",
       " (1.0, 2.0, 1.0),\n",
       " (1.2, 1.0, 1.0),\n",
       " (1.2, 1.2, 1.0),\n",
       " (1.2, 1.4, 1.0),\n",
       " (1.2, 1.6, 1.0),\n",
       " (1.2, 1.8, 1.0),\n",
       " (1.4, 1.0, 1.0),\n",
       " (1.4, 1.2, 1.0),\n",
       " (1.4, 1.4, 1.0),\n",
       " (1.4, 1.6, 1.0),\n",
       " (1.6, 1.0, 1.0),\n",
       " (1.6, 1.2, 1.0),\n",
       " (1.6, 1.4, 1.0),\n",
       " (1.8, 1.0, 1.0),\n",
       " (1.8, 1.2, 1.0),\n",
       " (1.8, 1.4, 1.0),\n",
       " (2.0, 1.0, 1.0),\n",
       " (2.0, 1.2, 1.0),\n",
       " (2.0, 1.4, 1.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLUPool(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding,\n",
    "                pool_kernel_size, pool_stride, dropout: float = 0.0):\n",
    "        super(ConvBNReLUPool, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalableCnn1(nn.Module):\n",
    "    def __init__(self, compound_scales: tuple = (1, 1, 1)):\n",
    "        super(ScalableCnn1, self).__init__()\n",
    "        alpha, beta, gamma = compound_scales[0], compound_scales[1], compound_scales[2]\n",
    "        \n",
    "        initial_conv_inputs = [1, 32, 64, 64]\n",
    "        initial_conv_outputs = [32, 64, 64, 64]\n",
    "        initial_nb_conv = 4\n",
    "        initial_dense_inputs = [1344]\n",
    "        initial_dense_outputs = [10]\n",
    "        initial_nb_dense = 1\n",
    "        initial_resolution = (64, 173)\n",
    "        \n",
    "        # Apply compound scaling\n",
    "        # depth ----\n",
    "        scaled_nb_conv = np.floor(initial_nb_conv * alpha)\n",
    "        scaled_nb_dense = np.floor(initial_nb_dense * alpha)\n",
    "        \n",
    "        if scaled_nb_conv != initial_nb_conv:  # Another conv layer must be created\n",
    "            print(\"More conv layer must be created\")\n",
    "            gaps = np.array(initial_conv_outputs) - np.array(initial_conv_inputs) # average filter gap\n",
    "            avg_gap = gaps.mean()\n",
    "            \n",
    "            while len(initial_conv_inputs) < scaled_nb_conv:\n",
    "                initial_conv_outputs.append(int(np.floor(initial_conv_outputs[-1] + avg_gap)))\n",
    "                initial_conv_inputs.append(initial_conv_outputs[-2])\n",
    "                \n",
    "            print(\"new conv layers:\")\n",
    "            print(\"inputs: \", initial_conv_inputs)\n",
    "            print(\"ouputs: \", initial_conv_outputs)\n",
    "            \n",
    "        if scaled_nb_dense != initial_nb_dense:  # Another dense layer must be created\n",
    "            print(\"More dense layer must be created\")\n",
    "            dense_list = np.linspace(initial_dense_inputs[0], initial_dense_outputs[-1], scaled_nb_dense+1)\n",
    "            initial_dense_inputs = dense_list[:-1]\n",
    "            initial_dense_outputs = dense_list[1:]\n",
    "            \n",
    "            print(\"new dense layers:\")\n",
    "            print(\"inputs: \", initial_dense_inputs)\n",
    "            print(\"ouputs: \", initial_dense_outputs)\n",
    "                \n",
    "        # width ----\n",
    "        scaled_conv_inputs = [int(np.floor(i * beta)) for i in initial_conv_inputs]\n",
    "        scaled_conv_outputs = [int(np.floor(i * beta)) for i in initial_conv_outputs]\n",
    "        scaled_dense_inputs = [int(np.floor(i * beta)) for i in initial_dense_inputs]\n",
    "        scaled_dense_outputs = [int(np.floor(i * beta)) for i in initial_dense_outputs]\n",
    "        \n",
    "        # Check how many conv with pooling layer can be used\n",
    "        nb_max_pooling = np.min([np.log2(initial_resolution[0]), int(np.log2(initial_resolution[1]))])\n",
    "        nb_model_pooling = len(scaled_conv_inputs)\n",
    "        \n",
    "        if nb_model_pooling > nb_max_pooling:\n",
    "            nb_model_pooling = nb_max_pooling\n",
    "        \n",
    "        # fixe initial and final conv & linear input\n",
    "        scaled_conv_inputs[0] = 1\n",
    "        scaled_dense_inputs[0] = self.calc_initial_dense_input(initial_resolution, nb_model_pooling, scaled_conv_outputs)\n",
    "        scaled_dense_outputs[-1] = 10\n",
    "        \n",
    "        # ======== Create the convolution part ========\n",
    "        features = []\n",
    "        \n",
    "        # Create the layers\n",
    "        for idx, (inp, out) in enumerate(zip(scaled_conv_inputs, scaled_conv_outputs)):\n",
    "            if idx < nb_model_pooling:\n",
    "                dropout = 0.3 if idx != 0 else 0.0\n",
    "                features.append(ConvBNReLUPool( inp, out, 3, 1, 1, (2, 2), (2, 2), dropout))\n",
    "            \n",
    "            else:\n",
    "                features.append(ConvReLU(inp, out, 3, 1, 1))\n",
    "            \n",
    "        self.features = nn.Sequential(\n",
    "            *features,\n",
    "        )\n",
    "\n",
    "        # ======== Craete the classifier part ========\n",
    "        linears = []\n",
    "        for inp, out in zip(scaled_dense_inputs[:-1], scaled_dense_outputs[:-1]):\n",
    "            print(inp, out)\n",
    "            linears.append(nn.Linear(inp, out))\n",
    "            linears.append(nn.ReLU6(inplace=True))\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            *linears,\n",
    "            nn.Linear(scaled_dense_inputs[-1], scaled_dense_outputs[-1])\n",
    "        )\n",
    "                      \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def calc_initial_dense_input(self, initial_resolution, nb_model_pooling, conv_outputs):\n",
    "        dim1 = initial_resolution[0]\n",
    "        dim2 = initial_resolution[1]\n",
    "        \n",
    "        for i in range(int(nb_model_pooling)):\n",
    "            dim1 = dim1 // 2\n",
    "            dim2 = dim2 // 2\n",
    "            \n",
    "        return dim1 * dim2 * conv_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== Training ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More conv layer must be created\n",
      "new conv layers:\n",
      "inputs:  [1, 32, 64, 64, 64]\n",
      "ouputs:  [32, 64, 64, 64, 79]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ---- Efficient net ----\n",
    "# model_func = EfficientNet\n",
    "# m1 = EfficientNet(\n",
    "#     conv_in_size= [1, 8, 16, 24, 40],\n",
    "#     conv_out_size= [8, 16, 24, 40, 40],\n",
    "#     t = [1, 6, 6, 4, 6, 6],\n",
    "#     s = [1, 2, 2, 2, 2, 1],\n",
    "#     n = [1, 3, 3, 1, 1, 1]\n",
    "# )\n",
    "\n",
    "# ---- Cnn with dropout ----\n",
    "# model_func = cnn_d\n",
    "# m1 = model_func()\n",
    "\n",
    "# ---- cnn ----\n",
    "# m1 = cnn()\n",
    "\n",
    "# ---- ScallableCNN ----\n",
    "model_func = ScalableCnn1\n",
    "m1 = model_func(valid_scaling[14])\n",
    "\n",
    "# Just trying the different model generated\n",
    "# model_func = ScalableCnn1\n",
    "# for compound_scaler in valid_scaling:\n",
    "#     m1 = model_func(compound_scaler)\n",
    "\n",
    "#     #m1 = m1.cuda()\n",
    "#     print(m1.features)\n",
    "#     print(m1.classifier)\n",
    "    \n",
    "#     from torchsummaryX import summary\n",
    "#     input_tensor = torch.zeros((100, 64, 173), dtype=torch.float)\n",
    "#     #input_tensor = input_tensor.cuda()\n",
    "\n",
    "#     s = summary(m1, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "                                 Kernel Shape        Output Shape    Params  \\\n",
      "Layer                                                                         \n",
      "0_features.0.Conv2d_0           [1, 51, 3, 3]  [100, 51, 64, 173]     510.0   \n",
      "1_features.0.BatchNorm2d_1               [51]  [100, 51, 64, 173]     102.0   \n",
      "2_features.0.Dropout2d_2                    -  [100, 51, 64, 173]         -   \n",
      "3_features.0.ReLU6_3                        -  [100, 51, 64, 173]         -   \n",
      "4_features.0.MaxPool2d_4                    -   [100, 51, 32, 86]         -   \n",
      "5_features.1.Conv2d_0         [51, 102, 3, 3]  [100, 102, 32, 86]    46.92k   \n",
      "6_features.1.BatchNorm2d_1              [102]  [100, 102, 32, 86]     204.0   \n",
      "7_features.1.Dropout2d_2                    -  [100, 102, 32, 86]         -   \n",
      "8_features.1.ReLU6_3                        -  [100, 102, 32, 86]         -   \n",
      "9_features.1.MaxPool2d_4                    -  [100, 102, 16, 43]         -   \n",
      "10_features.2.Conv2d_0       [102, 102, 3, 3]  [100, 102, 16, 43]   93.738k   \n",
      "11_features.2.BatchNorm2d_1             [102]  [100, 102, 16, 43]     204.0   \n",
      "12_features.2.Dropout2d_2                   -  [100, 102, 16, 43]         -   \n",
      "13_features.2.ReLU6_3                       -  [100, 102, 16, 43]         -   \n",
      "14_features.2.MaxPool2d_4                   -   [100, 102, 8, 21]         -   \n",
      "15_features.3.Conv2d_0       [102, 102, 3, 3]   [100, 102, 8, 21]   93.738k   \n",
      "16_features.3.BatchNorm2d_1             [102]   [100, 102, 8, 21]     204.0   \n",
      "17_features.3.Dropout2d_2                   -   [100, 102, 8, 21]         -   \n",
      "18_features.3.ReLU6_3                       -   [100, 102, 8, 21]         -   \n",
      "19_features.3.MaxPool2d_4                   -   [100, 102, 4, 10]         -   \n",
      "20_features.4.Conv2d_0       [102, 126, 3, 3]   [100, 126, 4, 10]  115.794k   \n",
      "21_features.4.BatchNorm2d_1             [126]   [100, 126, 4, 10]     252.0   \n",
      "22_features.4.Dropout2d_2                   -   [100, 126, 4, 10]         -   \n",
      "23_features.4.ReLU6_3                       -   [100, 126, 4, 10]         -   \n",
      "24_features.4.MaxPool2d_4                   -    [100, 126, 2, 5]         -   \n",
      "25_classifier.Flatten_0                     -         [100, 1260]         -   \n",
      "26_classifier.Dropout_1                     -         [100, 1260]         -   \n",
      "27_classifier.Linear_2             [1260, 10]           [100, 10]    12.61k   \n",
      "\n",
      "                               Mult-Adds  \n",
      "Layer                                     \n",
      "0_features.0.Conv2d_0          5.082048M  \n",
      "1_features.0.BatchNorm2d_1          51.0  \n",
      "2_features.0.Dropout2d_2               -  \n",
      "3_features.0.ReLU6_3                   -  \n",
      "4_features.0.MaxPool2d_4               -  \n",
      "5_features.1.Conv2d_0        128.843136M  \n",
      "6_features.1.BatchNorm2d_1         102.0  \n",
      "7_features.1.Dropout2d_2               -  \n",
      "8_features.1.ReLU6_3                   -  \n",
      "9_features.1.MaxPool2d_4               -  \n",
      "10_features.2.Conv2d_0        64.421568M  \n",
      "11_features.2.BatchNorm2d_1        102.0  \n",
      "12_features.2.Dropout2d_2              -  \n",
      "13_features.2.ReLU6_3                  -  \n",
      "14_features.2.MaxPool2d_4              -  \n",
      "15_features.3.Conv2d_0        15.730848M  \n",
      "16_features.3.BatchNorm2d_1        102.0  \n",
      "17_features.3.Dropout2d_2              -  \n",
      "18_features.3.ReLU6_3                  -  \n",
      "19_features.3.MaxPool2d_4              -  \n",
      "20_features.4.Conv2d_0          4.62672M  \n",
      "21_features.4.BatchNorm2d_1        126.0  \n",
      "22_features.4.Dropout2d_2              -  \n",
      "23_features.4.ReLU6_3                  -  \n",
      "24_features.4.MaxPool2d_4              -  \n",
      "25_classifier.Flatten_0                -  \n",
      "26_classifier.Dropout_1                -  \n",
      "27_classifier.Linear_2             12.6k  \n",
      "----------------------------------------------------------------------------------------\n",
      "                           Totals\n",
      "Total params             364.276k\n",
      "Trainable params         364.276k\n",
      "Non-trainable params          0.0\n",
      "Mult-Adds             218.717403M\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((100, 64, 173), dtype=torch.float)\n",
    "s = summary(m1, input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ed17feb472414796f4913ad3815666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06a109d6bb644569b9d0e97ae7c01f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "\n",
    "dataset = DatasetManager(metadata_root, audio_root, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using the combination dictionary, several models will be tested. Procedure to follow:**\n",
    "- create feature extract function using *extract feature helper*\n",
    "- change feature extract function from the dataset_manager\n",
    "- if extract parameters change, invalide the validation cache\n",
    "- create the model using the parameters\n",
    "- define criterion and optimizer\n",
    "- generate loader\n",
    "- create tensorboard log name\n",
    "- perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcdd0d98f1e43f19411ff7fda63b015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=837), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "m1 = model_func()\n",
    "m1.cuda()\n",
    "\n",
    "# loss and optimizer\n",
    "criterion_bce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     m1.parameters(),\n",
    "#     weight_decay=1e-3,\n",
    "#     lr=0.05\n",
    "# )\n",
    "optimizer = torch.optim.AdamW(m1.parameters(), weight_decay=1e-3)\n",
    "\n",
    "# Augmentation to use\n",
    "augments = []\n",
    "\n",
    "# train and val loaders\n",
    "train_dataset = Generator(dataset, augments=augments)\n",
    "\n",
    "x, y = train_dataset.validation\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)\n",
    "val_dataset = torch.utils.data.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epoch = 200\n",
    "batch_size = 64\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# scheduler\n",
    "lr_lambda = lambda epoch: 0.05 * (np.cos(np.pi * epoch / nb_epoch) + 1)\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "callbacks = [lr_scheduler]\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"%s_%s_Cosd-lr_sgd-0.01lr-wd0.001_%de_no_augment\" % ( get_datetime(), model_func.__name__, nb_epoch )\n",
    "tensorboard = SummaryWriter(log_dir=\"tensorboard/%s\" % title, comment=model_func.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed81ebbc118a4cc58c210ebcba543130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, 100% \t ce: 2.0356 - acc: 0.2333 - ce val: 1.7893 - acc val: 0.4147 - took: 11.83s\n",
      "Epoch 2, 100% \t ce: 2.0286 - acc: 0.3437 - ce val: 1.7250 - acc val: 0.4775 - took: 11.53s\n",
      "Epoch 3, 100% \t ce: 1.5805 - acc: 0.4139 - ce val: 2.2310 - acc val: 0.5310 - took: 11.79s\n",
      "Epoch 4, 100% \t ce: 1.2979 - acc: 0.4673 - ce val: 1.0758 - acc val: 0.5373 - took: 11.28s\n",
      "Epoch 5, 100% \t ce: 1.5698 - acc: 0.5062 - ce val: 1.0975 - acc val: 0.5554 - took: 11.76s\n",
      "Epoch 6, 100% \t ce: 1.5627 - acc: 0.5334 - ce val: 1.6052 - acc val: 0.5386 - took: 11.64s\n",
      "Epoch 7, 100% \t ce: 1.1646 - acc: 0.5644 - ce val: 1.0828 - acc val: 0.5875 - took: 11.76s\n",
      "Epoch 8, 100% \t ce: 0.8448 - acc: 0.5762 - ce val: 1.7150 - acc val: 0.5810 - took: 11.47s\n",
      "Epoch 9, 100% \t ce: 0.8625 - acc: 0.6010 - ce val: 0.4507 - acc val: 0.6453 - took: 11.84s\n",
      "Epoch 10, 100% \t ce: 1.1111 - acc: 0.6195 - ce val: 2.1911 - acc val: 0.5926 - took: 11.51s\n",
      "Epoch 11, 100% \t ce: 1.0652 - acc: 0.6317 - ce val: 0.4785 - acc val: 0.6699 - took: 11.89s\n",
      "Epoch 12, 100% \t ce: 1.3700 - acc: 0.6590 - ce val: 1.7883 - acc val: 0.6156 - took: 11.69s\n",
      "Epoch 13, 100% \t ce: 1.2814 - acc: 0.6683 - ce val: 1.5328 - acc val: 0.6980 - took: 11.88s\n",
      "Epoch 14, 100% \t ce: 0.8621 - acc: 0.6845 - ce val: 1.2371 - acc val: 0.6846 - took: 12.20s\n",
      "Epoch 15, 100% \t ce: 1.0448 - acc: 0.6877 - ce val: 0.5383 - acc val: 0.7076 - took: 11.68s\n",
      "Epoch 16, 100% \t ce: 0.6454 - acc: 0.7042 - ce val: 1.2262 - acc val: 0.6958 - took: 12.14s\n",
      "Epoch 17, 100% \t ce: 0.5220 - acc: 0.7136 - ce val: 1.8003 - acc val: 0.7071 - took: 11.98s\n",
      "Epoch 18, 100% \t ce: 0.7241 - acc: 0.7188 - ce val: 1.5296 - acc val: 0.7170 - took: 12.33s\n",
      "Epoch 19, 100% \t ce: 0.4996 - acc: 0.7285 - ce val: 2.2842 - acc val: 0.7402 - took: 11.97s\n",
      "Epoch 20, 100% \t ce: 0.6103 - acc: 0.7480 - ce val: 0.5643 - acc val: 0.7402 - took: 11.93s\n",
      "Epoch 21, 100% \t ce: 0.5445 - acc: 0.7481 - ce val: 0.9818 - acc val: 0.6935 - took: 11.98s\n",
      "Epoch 22, 100% \t ce: 0.8289 - acc: 0.7545 - ce val: 1.3630 - acc val: 0.7306 - took: 12.20s\n",
      "Epoch 23, 100% \t ce: 0.3554 - acc: 0.7577 - ce val: 0.7215 - acc val: 0.7379 - took: 12.14s\n",
      "Epoch 24, 100% \t ce: 0.4342 - acc: 0.7684 - ce val: 0.5517 - acc val: 0.7480 - took: 12.22s\n",
      "Epoch 25, 100% \t ce: 0.8252 - acc: 0.7693 - ce val: 1.2087 - acc val: 0.7359 - took: 12.07s\n",
      "Epoch 26, 100% \t ce: 1.2345 - acc: 0.7730 - ce val: 2.3657 - acc val: 0.7560 - took: 11.97s\n",
      "Epoch 27, 100% \t ce: 0.6651 - acc: 0.7825 - ce val: 0.7189 - acc val: 0.7424 - took: 12.03s\n",
      "Epoch 28, 100% \t ce: 0.6923 - acc: 0.7779 - ce val: 0.6776 - acc val: 0.7527 - took: 12.57s\n",
      "Epoch 29, 100% \t ce: 0.4902 - acc: 0.7953 - ce val: 0.1994 - acc val: 0.7879 - took: 11.75s\n",
      "Epoch 30, 100% \t ce: 0.6781 - acc: 0.7957 - ce val: 0.1085 - acc val: 0.7824 - took: 11.95s\n",
      "Epoch 31, 100% \t ce: 0.3173 - acc: 0.7996 - ce val: 1.0452 - acc val: 0.7705 - took: 12.29s\n",
      "Epoch 32, 100% \t ce: 0.5590 - acc: 0.8031 - ce val: 0.4668 - acc val: 0.7991 - took: 12.03s\n",
      "Epoch 33, 100% \t ce: 0.5763 - acc: 0.8082 - ce val: 0.2468 - acc val: 0.7935 - took: 12.12s\n",
      "Epoch 34, 100% \t ce: 0.4036 - acc: 0.8133 - ce val: 1.1766 - acc val: 0.7518 - took: 12.20s\n",
      "Epoch 35, 100% \t ce: 0.5305 - acc: 0.8132 - ce val: 0.2305 - acc val: 0.7991 - took: 12.01s\n",
      "Epoch 36, 100% \t ce: 0.6630 - acc: 0.8170 - ce val: 0.6972 - acc val: 0.7859 - took: 12.00s\n",
      "Epoch 37, 100% \t ce: 0.8145 - acc: 0.8226 - ce val: 0.7606 - acc val: 0.7804 - took: 12.20s\n",
      "Epoch 38, 100% \t ce: 0.5398 - acc: 0.8227 - ce val: 1.3165 - acc val: 0.7462 - took: 11.94s\n",
      "Epoch 39, 100% \t ce: 0.3066 - acc: 0.8275 - ce val: 0.8627 - acc val: 0.7859 - took: 11.76s\n",
      "Epoch 40, 100% \t ce: 0.6263 - acc: 0.8261 - ce val: 0.7123 - acc val: 0.7636 - took: 12.26s\n",
      "Epoch 41, 100% \t ce: 0.4521 - acc: 0.8222 - ce val: 0.1610 - acc val: 0.7723 - took: 12.05s\n",
      "Epoch 42, 100% \t ce: 0.5866 - acc: 0.8317 - ce val: 2.1994 - acc val: 0.7560 - took: 12.21s\n",
      "Epoch 43, 100% \t ce: 0.7262 - acc: 0.8304 - ce val: 0.1861 - acc val: 0.7871 - took: 12.13s\n",
      "Epoch 44, 100% \t ce: 0.5031 - acc: 0.8403 - ce val: 0.6444 - acc val: 0.7462 - took: 12.05s\n",
      "Epoch 45, 100% \t ce: 0.4188 - acc: 0.8388 - ce val: 0.1545 - acc val: 0.7969 - took: 12.26s\n",
      "Epoch 46, 100% \t ce: 0.1589 - acc: 0.8333 - ce val: 0.0392 - acc val: 0.7891 - took: 12.42s\n",
      "Epoch 47, 100% \t ce: 0.3563 - acc: 0.8366 - ce val: 0.4768 - acc val: 0.7770 - took: 12.36s\n",
      "Epoch 48, 100% \t ce: 0.3790 - acc: 0.8464 - ce val: 1.1649 - acc val: 0.7859 - took: 12.13s\n",
      "Epoch 49, 100% \t ce: 0.4041 - acc: 0.8494 - ce val: 0.4866 - acc val: 0.7815 - took: 12.11s\n",
      "Epoch 50, 100% \t ce: 0.1872 - acc: 0.8442 - ce val: 0.8956 - acc val: 0.7549 - took: 12.42s\n",
      "Epoch 51, 100% \t ce: 0.2408 - acc: 0.8465 - ce val: 0.1617 - acc val: 0.7991 - took: 12.23s\n",
      "Epoch 52, 100% \t ce: 0.4376 - acc: 0.8529 - ce val: 0.9581 - acc val: 0.7594 - took: 12.07s\n",
      "Epoch 53, 100% \t ce: 0.2926 - acc: 0.8586 - ce val: 0.9686 - acc val: 0.7306 - took: 11.93s\n",
      "Epoch 54, 100% \t ce: 0.8911 - acc: 0.8547 - ce val: 1.9028 - acc val: 0.7605 - took: 11.95s\n",
      "Epoch 55, 100% \t ce: 0.2628 - acc: 0.8595 - ce val: 1.1248 - acc val: 0.7516 - took: 12.36s\n",
      "Epoch 56, 100% \t ce: 0.3068 - acc: 0.8596 - ce val: 0.0071 - acc val: 0.7891 - took: 12.31s\n",
      "Epoch 57, 100% \t ce: 0.4545 - acc: 0.8573 - ce val: 0.8959 - acc val: 0.7882 - took: 12.33s\n",
      "Epoch 58, 100% \t ce: 0.2794 - acc: 0.8630 - ce val: 1.1267 - acc val: 0.7629 - took: 12.21s\n",
      "Epoch 59, 100% \t ce: 0.5278 - acc: 0.8614 - ce val: 1.6403 - acc val: 0.7893 - took: 12.45s\n",
      "Epoch 60, 100% \t ce: 0.5752 - acc: 0.8649 - ce val: 0.8545 - acc val: 0.7493 - took: 12.10s\n",
      "Epoch 61, 100% \t ce: 0.2489 - acc: 0.8663 - ce val: 0.0622 - acc val: 0.7846 - took: 11.88s\n",
      "Epoch 62, 100% \t ce: 0.5347 - acc: 0.8669 - ce val: 1.2300 - acc val: 0.7661 - took: 12.16s\n",
      "Epoch 63, 100% \t ce: 0.2708 - acc: 0.8689 - ce val: 0.4506 - acc val: 0.7504 - took: 12.06s\n",
      "Epoch 64, 100% \t ce: 0.3472 - acc: 0.8716 - ce val: 0.0757 - acc val: 0.7924 - took: 12.10s\n",
      "Epoch 65, 100% \t ce: 0.4694 - acc: 0.8707 - ce val: 0.3871 - acc val: 0.7857 - took: 12.06s\n",
      "Epoch 66, 100% \t ce: 0.3951 - acc: 0.8657 - ce val: 0.1700 - acc val: 0.7813 - took: 12.12s\n",
      "Epoch 67, 100% \t ce: 0.3149 - acc: 0.8747 - ce val: 0.0368 - acc val: 0.7891 - took: 12.19s\n",
      "Epoch 68, 100% \t ce: 0.3981 - acc: 0.8751 - ce val: 0.4101 - acc val: 0.7625 - took: 12.40s\n",
      "Epoch 69, 100% \t ce: 0.1166 - acc: 0.8776 - ce val: 0.3472 - acc val: 0.7592 - took: 12.01s\n",
      "Epoch 70, 100% \t ce: 0.2485 - acc: 0.8797 - ce val: 2.2279 - acc val: 0.7826 - took: 12.32s\n",
      "Epoch 71, 100% \t ce: 0.5475 - acc: 0.8732 - ce val: 0.7678 - acc val: 0.7795 - took: 12.39s\n",
      "Epoch 72, 100% \t ce: 0.6609 - acc: 0.8749 - ce val: 0.2981 - acc val: 0.7804 - took: 12.10s\n",
      "Epoch 73, 100% \t ce: 0.6154 - acc: 0.8772 - ce val: 0.8631 - acc val: 0.7748 - took: 12.24s\n",
      "Epoch 74, 100% \t ce: 0.2842 - acc: 0.8773 - ce val: 0.0951 - acc val: 0.7813 - took: 12.05s\n",
      "Epoch 75, 100% \t ce: 0.8584 - acc: 0.8773 - ce val: 0.9308 - acc val: 0.7792 - took: 11.83s\n",
      "Epoch 76, 100% \t ce: 0.2875 - acc: 0.8869 - ce val: 0.0355 - acc val: 0.8002 - took: 12.25s\n",
      "Epoch 77, 100% \t ce: 0.3183 - acc: 0.8834 - ce val: 0.0120 - acc val: 0.7913 - took: 12.38s\n",
      "Epoch 78, 100% \t ce: 0.3760 - acc: 0.8880 - ce val: 1.1353 - acc val: 0.7538 - took: 12.28s\n",
      "Epoch 79, 100% \t ce: 0.3660 - acc: 0.8822 - ce val: 0.1446 - acc val: 0.7958 - took: 12.36s\n",
      "Epoch 80, 100% \t ce: 0.3396 - acc: 0.8849 - ce val: 0.1174 - acc val: 0.7946 - took: 11.88s\n",
      "Epoch 81, 100% \t ce: 0.2282 - acc: 0.8872 - ce val: 0.2243 - acc val: 0.7902 - took: 12.05s\n",
      "Epoch 82, 100% \t ce: 0.6227 - acc: 0.8919 - ce val: 1.5667 - acc val: 0.7759 - took: 12.32s\n",
      "Epoch 83, 100% \t ce: 0.3955 - acc: 0.8868 - ce val: 0.0052 - acc val: 0.7835 - took: 12.37s\n",
      "Epoch 84, 100% \t ce: 0.0941 - acc: 0.8924 - ce val: 0.1675 - acc val: 0.7980 - took: 12.10s\n",
      "Epoch 85, 100% \t ce: 0.1777 - acc: 0.8886 - ce val: 1.6981 - acc val: 0.7714 - took: 12.11s\n",
      "Epoch 86, 100% \t ce: 0.3152 - acc: 0.8903 - ce val: 2.2082 - acc val: 0.7815 - took: 12.06s\n",
      "Epoch 87, 100% \t ce: 0.1390 - acc: 0.8902 - ce val: 2.5686 - acc val: 0.7516 - took: 12.13s\n",
      "Epoch 88, 100% \t ce: 0.2072 - acc: 0.8941 - ce val: 0.0408 - acc val: 0.7980 - took: 12.30s\n",
      "Epoch 89, 100% \t ce: 0.3164 - acc: 0.8932 - ce val: 3.5094 - acc val: 0.7663 - took: 12.41s\n",
      "Epoch 90, 100% \t ce: 0.4217 - acc: 0.8867 - ce val: 1.4126 - acc val: 0.7460 - took: 12.29s\n",
      "Epoch 91, 100% \t ce: 0.4856 - acc: 0.8924 - ce val: 0.1825 - acc val: 0.7723 - took: 12.21s\n",
      "Epoch 92, 100% \t ce: 0.4171 - acc: 0.8913 - ce val: 5.4248 - acc val: 0.7310 - took: 12.13s\n",
      "Epoch 93, 100% \t ce: 0.0999 - acc: 0.8991 - ce val: 2.3089 - acc val: 0.7683 - took: 12.58s\n",
      "Epoch 94, 100% \t ce: 0.3434 - acc: 0.8992 - ce val: 0.4306 - acc val: 0.7882 - took: 12.15s\n",
      "Epoch 95, 100% \t ce: 0.3488 - acc: 0.8999 - ce val: 0.3396 - acc val: 0.7636 - took: 12.48s\n",
      "Epoch 96, 100% \t ce: 0.5519 - acc: 0.9016 - ce val: 0.9972 - acc val: 0.7904 - took: 12.06s\n",
      "Epoch 97, 100% \t ce: 0.2238 - acc: 0.8977 - ce val: 1.3952 - acc val: 0.7583 - took: 12.27s\n",
      "Epoch 98, 100% \t ce: 0.1964 - acc: 0.9004 - ce val: 0.2288 - acc val: 0.7904 - took: 12.03s\n",
      "Epoch 99, 100% \t ce: 0.6220 - acc: 0.8971 - ce val: 0.2964 - acc val: 0.7759 - took: 12.38s\n",
      "Epoch 100, 100% \t ce: 0.3122 - acc: 0.9022 - ce val: 0.0676 - acc val: 0.7879 - took: 12.32s\n",
      "Epoch 101, 100% \t ce: 0.1986 - acc: 0.8934 - ce val: 3.7731 - acc val: 0.7739 - took: 12.46s\n",
      "Epoch 102, 100% \t ce: 0.3002 - acc: 0.9018 - ce val: 0.2463 - acc val: 0.7991 - took: 12.59s\n",
      "Epoch 103, 100% \t ce: 0.1571 - acc: 0.9127 - ce val: 0.2353 - acc val: 0.7801 - took: 12.17s\n",
      "Epoch 104, 100% \t ce: 0.2414 - acc: 0.9048 - ce val: 3.9875 - acc val: 0.7440 - took: 12.44s\n",
      "Epoch 105, 100% \t ce: 0.3006 - acc: 0.9042 - ce val: 0.2475 - acc val: 0.7846 - took: 12.35s\n",
      "Epoch 106, 100% \t ce: 0.2541 - acc: 0.9009 - ce val: 1.5241 - acc val: 0.7638 - took: 12.43s\n",
      "Epoch 107, 100% \t ce: 0.2502 - acc: 0.8999 - ce val: 0.4297 - acc val: 0.7893 - took: 12.30s\n",
      "Epoch 108, 100% \t ce: 0.1352 - acc: 0.9054 - ce val: 0.7622 - acc val: 0.7862 - took: 12.31s\n",
      "Epoch 109, 100% \t ce: 0.1261 - acc: 0.9086 - ce val: 0.0022 - acc val: 0.8069 - took: 12.26s\n",
      "Epoch 110, 100% \t ce: 0.2771 - acc: 0.9032 - ce val: 0.4166 - acc val: 0.7681 - took: 12.52s\n",
      "Epoch 111, 100% \t ce: 0.3173 - acc: 0.9087 - ce val: 0.2228 - acc val: 0.7837 - took: 12.42s\n",
      "Epoch 112, 100% \t ce: 0.1010 - acc: 0.9077 - ce val: 0.6793 - acc val: 0.7616 - took: 12.39s\n",
      "Epoch 113, 100% \t ce: 0.2246 - acc: 0.9063 - ce val: 1.3753 - acc val: 0.7384 - took: 12.46s\n",
      "Epoch 114, 100% \t ce: 0.2347 - acc: 0.9096 - ce val: 1.1133 - acc val: 0.7837 - took: 12.19s\n",
      "Epoch 115, 100% \t ce: 0.2745 - acc: 0.9058 - ce val: 0.9296 - acc val: 0.7792 - took: 12.40s\n",
      "Epoch 116, 100% \t ce: 0.1758 - acc: 0.9108 - ce val: 3.1124 - acc val: 0.7241 - took: 12.44s\n",
      "Epoch 117, 100% \t ce: 0.2549 - acc: 0.9070 - ce val: 0.5273 - acc val: 0.7594 - took: 12.21s\n",
      "Epoch 118, 100% \t ce: 0.3782 - acc: 0.9124 - ce val: 0.0528 - acc val: 0.8025 - took: 12.24s\n",
      "Epoch 119, 100% \t ce: 0.2808 - acc: 0.9043 - ce val: 1.3731 - acc val: 0.7638 - took: 12.29s\n",
      "Epoch 120, 100% \t ce: 0.1936 - acc: 0.9050 - ce val: 0.5332 - acc val: 0.7460 - took: 12.67s\n",
      "Epoch 121, 100% \t ce: 0.1957 - acc: 0.9156 - ce val: 0.0909 - acc val: 0.8002 - took: 12.46s\n",
      "Epoch 122, 100% \t ce: 0.2736 - acc: 0.9160 - ce val: 0.9098 - acc val: 0.7783 - took: 12.02s\n",
      "Epoch 123, 100% \t ce: 0.2113 - acc: 0.9176 - ce val: 0.2174 - acc val: 0.7980 - took: 12.30s\n",
      "Epoch 124, 100% \t ce: 0.5550 - acc: 0.9156 - ce val: 3.4001 - acc val: 0.7781 - took: 12.24s\n",
      "Epoch 125, 100% \t ce: 0.4330 - acc: 0.9142 - ce val: 0.3363 - acc val: 0.7592 - took: 12.46s\n",
      "Epoch 126, 100% \t ce: 0.2551 - acc: 0.9136 - ce val: 1.4119 - acc val: 0.7580 - took: 12.27s\n",
      "Epoch 127, 100% \t ce: 0.2558 - acc: 0.9180 - ce val: 0.0312 - acc val: 0.7924 - took: 12.84s\n",
      "Epoch 128, 100% \t ce: 0.0619 - acc: 0.9132 - ce val: 0.8127 - acc val: 0.7516 - took: 12.33s\n",
      "Epoch 129, 100% \t ce: 0.1977 - acc: 0.9151 - ce val: 1.4970 - acc val: 0.7804 - took: 12.45s\n",
      "Epoch 130, 100% \t ce: 0.1097 - acc: 0.9171 - ce val: 0.2985 - acc val: 0.7904 - took: 12.62s\n",
      "Epoch 131, 100% \t ce: 0.2045 - acc: 0.9166 - ce val: 0.1300 - acc val: 0.8058 - took: 12.41s\n",
      "Epoch 132, 100% \t ce: 0.3400 - acc: 0.9149 - ce val: 1.5201 - acc val: 0.7728 - took: 12.40s\n",
      "Epoch 133, 100% \t ce: 0.1325 - acc: 0.9223 - ce val: 1.0662 - acc val: 0.7904 - took: 12.46s\n",
      "Epoch 134, 100% \t ce: 0.2518 - acc: 0.9115 - ce val: 0.4980 - acc val: 0.7783 - took: 12.55s\n",
      "Epoch 135, 100% \t ce: 0.1438 - acc: 0.9176 - ce val: 0.9466 - acc val: 0.7949 - took: 12.46s\n",
      "Epoch 136, 100% \t ce: 0.3784 - acc: 0.9200 - ce val: 0.1123 - acc val: 0.8002 - took: 12.51s\n",
      "Epoch 137, 100% \t ce: 0.4179 - acc: 0.9180 - ce val: 0.8963 - acc val: 0.7804 - took: 12.36s\n",
      "Epoch 138, 100% \t ce: 0.1463 - acc: 0.9215 - ce val: 3.5327 - acc val: 0.8038 - took: 12.08s\n",
      "Epoch 139, 100% \t ce: 0.1780 - acc: 0.9205 - ce val: 0.6339 - acc val: 0.7728 - took: 12.52s\n",
      "Epoch 140, 100% \t ce: 0.2236 - acc: 0.9199 - ce val: 1.9809 - acc val: 0.7650 - took: 12.44s\n",
      "Epoch 141, 100% \t ce: 0.2830 - acc: 0.9199 - ce val: 0.1547 - acc val: 0.8170 - took: 12.23s\n",
      "Epoch 142, 100% \t ce: 0.3007 - acc: 0.9222 - ce val: 1.7078 - acc val: 0.7993 - took: 12.83s\n",
      "Epoch 143, 100% \t ce: 0.4953 - acc: 0.9206 - ce val: 2.3553 - acc val: 0.7741 - took: 12.44s\n",
      "Epoch 144, 100% \t ce: 0.4871 - acc: 0.9212 - ce val: 0.6283 - acc val: 0.7826 - took: 12.41s\n",
      "Epoch 145, 100% \t ce: 0.3742 - acc: 0.9213 - ce val: 0.8731 - acc val: 0.7960 - took: 12.21s\n",
      "Epoch 146, 100% \t ce: 0.1503 - acc: 0.9237 - ce val: 0.4264 - acc val: 0.8004 - took: 12.67s\n",
      "Epoch 147, 100% \t ce: 0.2495 - acc: 0.9205 - ce val: 0.2568 - acc val: 0.7949 - took: 12.60s\n",
      "Epoch 148, 100% \t ce: 0.0536 - acc: 0.9244 - ce val: 2.3019 - acc val: 0.7904 - took: 12.43s\n",
      "Epoch 149, 100% \t ce: 0.2184 - acc: 0.9232 - ce val: 0.0656 - acc val: 0.8225 - took: 12.31s\n",
      "Epoch 150, 100% \t ce: 0.2489 - acc: 0.9220 - ce val: 0.7675 - acc val: 0.7772 - took: 12.36s\n",
      "Epoch 151, 100% \t ce: 0.1062 - acc: 0.9199 - ce val: 0.5906 - acc val: 0.8049 - took: 12.37s\n",
      "Epoch 152, 100% \t ce: 0.4387 - acc: 0.9300 - ce val: 1.1502 - acc val: 0.7804 - took: 12.23s\n",
      "Epoch 153, 100% \t ce: 0.4259 - acc: 0.9216 - ce val: 0.3203 - acc val: 0.7848 - took: 12.22s\n",
      "Epoch 154, 100% \t ce: 0.0536 - acc: 0.9246 - ce val: 0.1664 - acc val: 0.8103 - took: 12.59s\n",
      "Epoch 155, 100% \t ce: 0.1607 - acc: 0.9255 - ce val: 0.1410 - acc val: 0.7902 - took: 12.64s\n",
      "Epoch 156, 100% \t ce: 0.2425 - acc: 0.9259 - ce val: 0.6066 - acc val: 0.7772 - took: 12.23s\n",
      "Epoch 157, 100% \t ce: 0.2295 - acc: 0.9207 - ce val: 0.7269 - acc val: 0.7859 - took: 12.21s\n",
      "Epoch 158, 100% \t ce: 0.1562 - acc: 0.9261 - ce val: 0.0475 - acc val: 0.8002 - took: 12.40s\n",
      "Epoch 159, 100% \t ce: 0.1896 - acc: 0.9295 - ce val: 0.6653 - acc val: 0.7627 - took: 12.45s\n",
      "Epoch 160, 100% \t ce: 0.2985 - acc: 0.9314 - ce val: 0.2702 - acc val: 0.7837 - took: 12.47s\n",
      "Epoch 161, 100% \t ce: 0.2443 - acc: 0.9235 - ce val: 0.0096 - acc val: 0.7991 - took: 12.24s\n",
      "Epoch 162, 100% \t ce: 0.0511 - acc: 0.9243 - ce val: 0.0606 - acc val: 0.8080 - took: 12.86s\n",
      "Epoch 163, 100% \t ce: 0.1589 - acc: 0.9260 - ce val: 4.1755 - acc val: 0.7817 - took: 12.45s\n",
      "Epoch 164, 100% \t ce: 0.0637 - acc: 0.9224 - ce val: 2.4072 - acc val: 0.7728 - took: 12.58s\n",
      "Epoch 165, 100% \t ce: 0.1491 - acc: 0.9251 - ce val: 0.7782 - acc val: 0.7683 - took: 12.81s\n",
      "Epoch 166, 100% \t ce: 0.4086 - acc: 0.9314 - ce val: 0.4677 - acc val: 0.7893 - took: 12.68s\n",
      "Epoch 167, 100% \t ce: 0.1903 - acc: 0.9284 - ce val: 0.3494 - acc val: 0.7828 - took: 12.54s\n",
      "Epoch 168, 100% \t ce: 0.2450 - acc: 0.9287 - ce val: 0.3927 - acc val: 0.7993 - took: 12.65s\n",
      "Epoch 169, 100% \t ce: 0.0524 - acc: 0.9303 - ce val: 3.7713 - acc val: 0.7862 - took: 12.31s\n",
      "Epoch 170, 100% \t ce: 0.2284 - acc: 0.9326 - ce val: 0.0468 - acc val: 0.8058 - took: 12.42s\n",
      "Epoch 171, 100% \t ce: 0.2275 - acc: 0.9310 - ce val: 0.4547 - acc val: 0.7982 - took: 12.21s\n",
      "Epoch 172, 100% \t ce: 0.1162 - acc: 0.9300 - ce val: 1.7888 - acc val: 0.7806 - took: 12.43s\n",
      "Epoch 173, 100% \t ce: 0.0962 - acc: 0.9284 - ce val: 0.1274 - acc val: 0.8069 - took: 12.76s\n",
      "Epoch 174, 100% \t ce: 0.0952 - acc: 0.9295 - ce val: 0.3544 - acc val: 0.8060 - took: 12.30s\n",
      "Epoch 175, 100% \t ce: 0.3345 - acc: 0.9291 - ce val: 1.3800 - acc val: 0.7884 - took: 12.51s\n",
      "Epoch 176, 100% \t ce: 0.2623 - acc: 0.9316 - ce val: 0.1387 - acc val: 0.8147 - took: 12.37s\n",
      "Epoch 177, 100% \t ce: 0.1707 - acc: 0.9355 - ce val: 1.2757 - acc val: 0.7750 - took: 12.44s\n",
      "Epoch 178, 100% \t ce: 0.1575 - acc: 0.9329 - ce val: 0.1024 - acc val: 0.8092 - took: 12.44s\n",
      "Epoch 179, 100% \t ce: 0.1451 - acc: 0.9280 - ce val: 0.0275 - acc val: 0.8047 - took: 12.72s\n",
      "Epoch 180, 100% \t ce: 0.2109 - acc: 0.9274 - ce val: 0.2766 - acc val: 0.8027 - took: 12.29s\n",
      "Epoch 181, 100% \t ce: 0.3159 - acc: 0.9340 - ce val: 0.0015 - acc val: 0.8013 - took: 12.58s\n",
      "Epoch 182, 100% \t ce: 0.5733 - acc: 0.9298 - ce val: 0.7554 - acc val: 0.7929 - took: 12.63s\n",
      "Epoch 183, 100% \t ce: 0.1062 - acc: 0.9310 - ce val: 3.6039 - acc val: 0.7895 - took: 12.35s\n",
      "Epoch 184, 100% \t ce: 0.1815 - acc: 0.9331 - ce val: 0.5713 - acc val: 0.8060 - took: 12.35s\n",
      "Epoch 185, 100% \t ce: 0.1257 - acc: 0.9295 - ce val: 1.4513 - acc val: 0.7873 - took: 12.48s\n",
      "Epoch 186, 100% \t ce: 0.0727 - acc: 0.9271 - ce val: 0.0021 - acc val: 0.8136 - took: 12.80s\n",
      "Epoch 187, 100% \t ce: 0.3155 - acc: 0.9299 - ce val: 0.3031 - acc val: 0.7837 - took: 12.53s\n",
      "Epoch 188, 100% \t ce: 0.1895 - acc: 0.9340 - ce val: 0.0616 - acc val: 0.7969 - took: 12.21s\n",
      "Epoch 189, 100% \t ce: 0.0716 - acc: 0.9375 - ce val: 1.1165 - acc val: 0.7752 - took: 12.43s\n",
      "Epoch 190, 100% \t ce: 0.1273 - acc: 0.9354 - ce val: 0.0010 - acc val: 0.8147 - took: 12.90s\n",
      "Epoch 191, 100% \t ce: 0.1113 - acc: 0.9348 - ce val: 0.1269 - acc val: 0.8002 - took: 12.41s\n",
      "Epoch 192, 100% \t ce: 0.0235 - acc: 0.9321 - ce val: 0.9553 - acc val: 0.7949 - took: 12.60s\n",
      "Epoch 193, 100% \t ce: 0.1473 - acc: 0.9368 - ce val: 0.5234 - acc val: 0.7982 - took: 12.44s\n",
      "Epoch 194, 100% \t ce: 0.0643 - acc: 0.9320 - ce val: 1.0215 - acc val: 0.7839 - took: 12.68s\n",
      "Epoch 195, 100% \t ce: 0.2295 - acc: 0.9378 - ce val: 0.2555 - acc val: 0.7781 - took: 12.94s\n",
      "Epoch 196, 100% \t ce: 0.0375 - acc: 0.9370 - ce val: 0.1580 - acc val: 0.7958 - took: 12.56s\n",
      "Epoch 197, 100% \t ce: 0.2856 - acc: 0.9362 - ce val: 3.3140 - acc val: 0.7759 - took: 12.16s\n",
      "Epoch 198, 100% \t ce: 0.1931 - acc: 0.9351 - ce val: 2.7963 - acc val: 0.7982 - took: 12.22s\n",
      "Epoch 199, 100% \t ce: 0.3120 - acc: 0.9377 - ce val: 0.0033 - acc val: 0.8103 - took: 12.15s\n",
      "Epoch 200, 100% \t ce: 0.0136 - acc: 0.9376 - ce val: 1.6513 - acc val: 0.7770 - took: 12.61s\n"
     ]
    }
   ],
   "source": [
    "acc_func = CategoricalAccuracy()\n",
    "\n",
    "for epoch in tqdm.tqdm_notebook(range(nb_epoch)):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    acc_func.reset()\n",
    "\n",
    "    m1.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(training_loader):        \n",
    "        # Transfer to GPU\n",
    "        X = X.cuda().float()\n",
    "        y = y.cuda().long()\n",
    "        \n",
    "        # predict\n",
    "        logits = m1(X)\n",
    "\n",
    "        weak_loss = criterion_bce(logits, y)\n",
    "\n",
    "        total_loss = weak_loss\n",
    "\n",
    "        # calc metrics\n",
    "#         y_pred = torch.log_softmax(logits, dim=1)\n",
    "        _, y_pred = torch.max(logits, 1)\n",
    "        acc = acc_func(y_pred, y)\n",
    "\n",
    "        # ======== back propagation ========\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== history ========\n",
    "        print(\"Epoch {}, {:d}% \\t ce: {:.4f} - acc: {:.4f} - took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            total_loss.item(),\n",
    "            acc,\n",
    "            time.time() - start_time\n",
    "        ),end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\n",
    "    tensorboard.add_scalar('train/ce', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", 100. * acc, epoch )\n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # reset metrics\n",
    "        acc_func.reset()\n",
    "        m1.eval()\n",
    "\n",
    "        for X_val, y_val in val_loader:\n",
    "            # Transfer to GPU\n",
    "            X_val = X_val.cuda().float()\n",
    "            y_val = y_val.cuda().long()\n",
    "\n",
    "\n",
    "#             y_weak_val_pred, _ = model(X_val)\n",
    "            logits = m1(X_val)\n",
    "\n",
    "            # calc loss\n",
    "            weak_loss_val = criterion_bce(logits, y_val)\n",
    "\n",
    "            # metrics\n",
    "#             y_val_pred =torch.log_softmax(logits, dim=1)\n",
    "            _, y_val_pred = torch.max(logits, 1)\n",
    "            acc_val = acc_func(y_val_pred, y_val)\n",
    "\n",
    "            #Print statistics\n",
    "            print(\"Epoch {}, {:d}% \\t ce: {:.4f} - acc: {:.4f} - ce val: {:.4f} - acc val: {:.4f} - took: {:.2f}s\".format(\n",
    "                epoch+1,\n",
    "                int(100 * (i+1) / nb_batch),\n",
    "                total_loss.item(),\n",
    "                acc,\n",
    "                weak_loss_val.item(),\n",
    "                acc_val,\n",
    "                time.time() - start_time\n",
    "            ),end=\"\\r\")\n",
    "\n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('validation/ce', weak_loss_val.item(), epoch)\n",
    "        tensorboard.add_scalar(\"validation/acc\", 100. * acc_val, epoch )\n",
    "\n",
    "    for callback in callbacks:\n",
    "        callback.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
