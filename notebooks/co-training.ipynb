{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import random\n",
    "from random import shuffle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.nn.utils import weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from datasetManager import DatasetManager\n",
    "from generators import Generator, CoTrainingGenerator\n",
    "from samplers import CoTrainingSampler\n",
    "import signal_augmentations as sa \n",
    "\n",
    "from models import cnn\n",
    "from losses import loss_cot, loss_diff, loss_diff, p_loss_diff, p_loss_sup\n",
    "from metrics import CategoricalAccuracy, Ratio\n",
    "from ramps import Warmup, sigmoid_rampup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.sess = \"default\"\n",
    "        self.nb_view = 2\n",
    "        self.batchsize = 100\n",
    "        self.lambda_cot_max = 10\n",
    "        self.lambda_diff_max = 0.5\n",
    "        self.ratio = 0.1\n",
    "        self.seed = 1234\n",
    "        self.epochs = 600\n",
    "        self.warm_up = 80\n",
    "        self.momentum = 0.0\n",
    "        self.decay = 1e-3\n",
    "        self.epsilon = 0.02\n",
    "        self.num_class = 10\n",
    "        self.cifar10_dir = \"/corpus/corpus/UrbanSound8K\"\n",
    "        self.tensorboard_dir = \"tensoboard_cotraining\"\n",
    "        self.checkpoint_dir = \"checkpoint\"\n",
    "        self.base_lr = 0.05\n",
    "        self.resume = False\n",
    "        self.job_name = \"default\"\n",
    "        self.multi_gpu = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Co-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b9b1ca27994334b51e6d608810617a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68879921256040b98df0c4aa9794e8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "dataset = DatasetManager(metadata_root, audio_root, verbose=2)\n",
    "\n",
    "# prepare the sampler with the specified number of supervised file\n",
    "nb_train_file = len(dataset.audio[\"train\"])\n",
    "nb_s_file = int(nb_train_file * args.ratio)\n",
    "nb_s_file = nb_s_file - (nb_s_file % DatasetManager.NB_CLASS)  # need to be a multiple of number of class\n",
    "nb_u_file = nb_train_file - nb_s_file\n",
    "\n",
    "\n",
    "sampler = CoTrainingSampler(args.batchsize, nb_s_file, nb_u_file, nb_view=args.nb_view, ratio=None, method=\"duplicate\") # ratio is manually set here\n",
    "train_dataset = CoTrainingGenerator(dataset, sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = cnn\n",
    "\n",
    "m1, m2 = model_func(), model_func()\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders & adversarial generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3bcf6dedd34f67bbef4bc4402b690f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=837), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, y = train_dataset.validation\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)\n",
    "val_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_sampler=sampler, num_workers=4)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=128, num_workers=4)\n",
    "\n",
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizers & callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = optim.SGD(params, lr=args.base_lr, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "lr_lambda = lambda epoch: (1.0 + math.cos((epoch-1)*math.pi/args.epochs))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "callbacks = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "ratioS = [Ratio(), Ratio()]\n",
    "ratioU = [Ratio(), Ratio()]\n",
    "ratioSU = [Ratio(), Ratio()]\n",
    "accS = [CategoricalAccuracy(), CategoricalAccuracy()]\n",
    "accU = [CategoricalAccuracy(), CategoricalAccuracy()]\n",
    "accSU = [CategoricalAccuracy(), CategoricalAccuracy()]\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warm_up, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warm_up, sigmoid_rampup)\n",
    "\n",
    "def reset_all_metrics():\n",
    "    all_metrics = [*ratioS, *ratioU, *ratioSU, *accS, *accU, *accSU]\n",
    "    for m in all_metrics:\n",
    "        m.reset()\n",
    "        \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "title = \"%s_%s_%slcm_%sldm_%swl\" % (\n",
    "    get_datetime(),\n",
    "    args.job_name,\n",
    "    args.lambda_cot_max,\n",
    "    args.lambda_diff_max,\n",
    "    args.warm_up,\n",
    ")\n",
    "tensorboard = SummaryWriter(\"%s/%s\" % (args.tensorboard_dir, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "    \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        for i in range(len(X_S)):\n",
    "            X_S[i] = X_S[i].cuda()\n",
    "            y_S[i] = y_S[i].cuda()\n",
    "        X_U, y_U = X_U.cuda(), y_U.cuda()\n",
    "\n",
    "        logits_S1 = m1(X_S[0])\n",
    "        logits_S2 = m2(X_S[1])\n",
    "        logits_U1 = m1(X_U)\n",
    "        logits_U2 = m2(X_U)\n",
    "\n",
    "        _, pred_S1 = torch.max(logits_S1, 1)\n",
    "        _, pred_S2 = torch.max(logits_S2, 1)\n",
    "\n",
    "        # pseudo labels of U \n",
    "        _, pred_U1 = torch.max(logits_U1, 1)\n",
    "        _, pred_U2 = torch.max(logits_U2, 1)\n",
    "\n",
    "        # ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "\n",
    "        #generate adversarial examples ----\n",
    "        adv_data_S1 = adv_generator_1.perturb(X_S[0], y_S[0])\n",
    "        adv_data_U1 = adv_generator_1.perturb(X_U, pred_U1)\n",
    "\n",
    "        adv_data_S2 = adv_generator_2.perturb(X_S[1], y_S[1])\n",
    "        adv_data_U2 = adv_generator_2.perturb(X_U, pred_U2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        adv_logits_S1 = m1(adv_data_S2)\n",
    "        adv_logits_S2 = m2(adv_data_S1)\n",
    "\n",
    "        adv_logits_U1 = m1(adv_data_U2)\n",
    "        adv_logits_U2 = m2(adv_data_U1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "\n",
    "        # losses ----\n",
    "        Loss_sup_S1, Loss_sup_S2, Loss_sup = p_loss_sup(logits_S1, logits_S2, y_S[0], y_S[1])\n",
    "        Loss_cot = loss_cot(logits_U1, logits_U2)\n",
    "        pld_S, pld_U, Loss_diff = p_loss_diff(logits_S1, logits_S2, adv_logits_S1, adv_logits_S2, logits_U1, logits_U2, adv_logits_U1, adv_logits_U2)\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot.next() * Loss_cot + lambda_diff.next() * Loss_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        # accuracies ----\n",
    "        pred_SU1 = torch.cat((pred_S1, pred_U1), 0)\n",
    "        pred_SU2 = torch.cat((pred_S2, pred_U2), 0)\n",
    "        y_SU1 = torch.cat((y_S[0], y_U), 0)\n",
    "        y_SU2 = torch.cat((y_S[1], y_U), 0)\n",
    "\n",
    "        acc_S1 = accS[0](pred_S1, y_S[0])\n",
    "        acc_S2 = accS[1](pred_S2, y_S[1])\n",
    "        acc_U1 = accU[0](pred_U1, y_U)\n",
    "        acc_U2 = accU[1](pred_U2, y_U)\n",
    "        acc_SU1 = accSU[0](pred_SU1, y_SU1)\n",
    "        acc_SU2 = accSU[1](pred_SU2, y_SU2)\n",
    "        \n",
    "        # ratios  ----\n",
    "        _, adv_pred_S1 = torch.max(adv_logits_S1, 1)\n",
    "        _, adv_pred_S2 = torch.max(adv_logits_S2, 1)\n",
    "        _, adv_pred_U1 = torch.max(adv_logits_U1, 1)\n",
    "        _, adv_pred_U2 = torch.max(adv_logits_U2, 1)\n",
    "\n",
    "        adv_pred_SU1 = torch.cat((adv_pred_S1, adv_pred_U1), 0)\n",
    "        adv_pred_SU2 = torch.cat((adv_pred_S2, adv_pred_U2), 0)\n",
    "        adv_y_SU1 = torch.cat((y_S[0], pred_U1), 0)\n",
    "        adv_y_SU2 = torch.cat((y_S[1], pred_U2), 0)\n",
    "\n",
    "        ratio_S1 = ratioS[0](adv_pred_S1, y_S[0])\n",
    "        ratio_S2 = ratioS[1](adv_pred_S2, y_S[1])\n",
    "        ratio_U1 = ratioU[0](adv_pred_U1, pred_U1)\n",
    "        ratio_U2 = ratioU[1](adv_pred_U2, pred_U2)\n",
    "        ratio_SU1 = ratioSU[0](adv_pred_SU1, adv_y_SU1)\n",
    "        ratio_SU2 = ratioSU[1](adv_pred_SU2, adv_y_SU2)\n",
    "        # ========\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # print statistics\n",
    "        print(\"Epoch %s: %.2f%% : train acc: %.3f %.3f - Loss: %.3f %.3f %.3f %.3f - time: %.2f\" % (\n",
    "            epoch, (batch / len(sampler)) * 100,\n",
    "            acc_SU1, acc_SU2,\n",
    "            running_loss/(batch+1), ls/(batch+1), lc/(batch+1), ld/(batch+1),\n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_SU1, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_SU2, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_loss/Lsus S1\", Loss_sup_S1.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Lsus S2\", Loss_sup_S2.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Ldiff S\", pld_S.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Ldiff U\", pld_U.item(), epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc S1\", acc_S1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc S2\", acc_S2, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc U1\", acc_U1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc U2\", acc_U2, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio S1\", ratio_S1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio S2\", ratio_S2, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio U1\", ratio_U1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio U2\", ratio_U2, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio SU1\", ratio_SU1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio SU2\", ratio_SU2, epoch)\n",
    "    \n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            outputs1 = m1(inputs)\n",
    "            predicted1 = outputs1.max(1)\n",
    "            total1 += targets.size(0)\n",
    "            correct1 += predicted1[1].eq(targets).sum().item()\n",
    "\n",
    "            outputs2 = m2(inputs)\n",
    "            predicted2 = outputs2.max(1)\n",
    "            total2 += targets.size(0)\n",
    "            correct2 += predicted2[1].eq(targets).sum().item()\n",
    "\n",
    "    print('\\nnet1 test acc: %.3f%% (%d/%d) | net2 test acc: %.3f%% (%d/%d)'\n",
    "        % (100.*correct1/total1, correct1, total1, 100.*correct2/total2, correct2, total2))\n",
    "    \n",
    "    tensorboard.add_scalar(\"val/acc 1\", correct1 / total1, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc 2\", correct2 / total2, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "    total_loss = train(epoch)\n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "    test(epoch)\n",
    "\n",
    "# tensorboard.export_scalars_to_json('./' + args.tensorboard_dir + 'output.json')\n",
    "# tensorboard.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Supervised training (same ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1114d8c3564f56a4c0b4bcccc2fb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff088b7b1ce484aabcbcdf37e8753b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "dataset = DatasetManager(metadata_root, audio_root, verbose=2)\n",
    "\n",
    "# prepare the sampler with the specified number of supervised file\n",
    "nb_train_file = len(dataset.audio[\"train\"])\n",
    "nb_s_file = int(nb_train_file * args.ratio)\n",
    "nb_s_file = nb_s_file - (nb_s_file % DatasetManager.NB_CLASS)  # need to be a multiple of number of class\n",
    "nb_u_file = 0\n",
    "\n",
    "\n",
    "sampler = CoTrainingSampler(args.batchsize, nb_s_file, nb_u_file, nb_view=args.nb_view, ratio=None, method=\"duplicate\") # ratio is manually set here\n",
    "train_dataset = CoTrainingGenerator(dataset, sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = cnn\n",
    "\n",
    "m1 = model_func()\n",
    "\n",
    "m1 = m1.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders & adversarial generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x, y = train_dataset.validation\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)\n",
    "val_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_sampler=sampler, num_workers=4)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=128, num_workers=4)\n",
    "\n",
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizers & callbacks & criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = m1.parameters()\n",
    "args.base_lr = 0.01\n",
    "optimizer = optim.SGD(params, lr=args.base_lr, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "lr_lambda = lambda epoch: (1.0 + math.cos((epoch-1)*math.pi/args.epochs))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "callbacks = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU5dn/8c93JglhX0JYA7Ir+xY2ca1LcQ22qKBFtCi2xVbr87TFp+tjl1+trVorLiiurQKlLnlsK1VAWxSBsCibSNgju0DYlyTX74852GlIyACByUyu9+s1r8y5z31OrluHfOfsMjOcc865aKF4F+Ccc67q8XBwzjl3DA8H55xzx/BwcM45dwwPB+ecc8dIiXcBlaFx48bWpk2beJfhnHMJZf78+dvNLLOseUkRDm3atCEvLy/eZTjnXEKRtK68eb5byTnn3DE8HJxzzh3Dw8E559wxPBycc84dw8PBOefcMWIKB0lDJK2QlC9pXBnza0iaHMyfI6lN0H6ZpPmSFgc/vxS1TN+gPV/So5IUtDeS9LaklcHPhpUzVOecc7GqMBwkhYHxwBVAF2CEpC6luo0GdppZB+Bh4IGgfTtwjZl1B0YBL0Ut8wQwBugYvIYE7eOA6WbWEZgeTDvnnDuDYrnOoT+Qb2arASRNAnKAZVF9coCfBe+nAo9JkpktjOqzFEiXVANoBNQzs9nBOl8EhgJ/D9Z1UbDMC8C7wA9OdGCxmLd2B/9auZ3UkAiHRWooREpY1E5LoV7NVOoHr4a1U8msU4OUsO+Fc85VD7GEQ0tgQ9R0ATCgvD5mViSpEMggsuVw1FeBhWZ2SFLLYD3R62wZvG9qZpuCdW2S1KSsoiSNIbLlQevWrWMYxrEWrNvJo9NXxtQ3JGhaL53m9dNp3qAmrRvVomOTOnRoUof2mXWoXSMprid0zjkgtnBQGW2lnxB03D6SuhLZ1XT5CazzuMxsAjABIDs7+6SeWHTnhe0Zc0E7ikuMohLjSHEJRcXGvsNF7Np/hN0HjlB44Ag79h9mc+FBNu46yMZdB1j6WSHTlmymqOTfv7Zlg5p0a1mPnq0a0DOrAd2z6lMvPfVkynLOubiLJRwKgFZR01nAxnL6FEhKAeoDOwAkZQGvAbeY2aqo/lnlrHOLpObBVkNzYOsJjOeESSIlLFLCkJ4aBqBh7TSyKjgMfqS4hHWf7yN/615WbtnLp1v3srhgF9OWbvmiT8cmdRjUPoNB7TIY2C6DhrXTTudQnHOu0sQSDvOAjpLaAp8Bw4GbSvXJJXLAeTYwDJhhZiapAfBX4D4ze/9o5+AP/x5JA4E5wC3AH0qt69fBzzdOdnCnU2o4RIcmdenQpC5Duv27fdf+w3xcUMhHG3aRt24nU+cX8OLsyO1LOjevx4WdMrm0cxN6t25IOFTWBpRzzsWfYnmGtKQrgUeAMPCsmf1S0v1AnpnlSkonciZSbyJbDMPNbLWkHwH3AdE79i83s62SsoHngZpEDkR/OwiUDGAK0BpYD1xvZjuOV192drZV1RvvHSku4eOCXcxe9Tmz8reTt3YnRSVGo9ppXHx2Ey7r0pSLzs78YqvFOefOFEnzzSy7zHmxhENVV5XDobTCA0d479NtTF++hZmfbGX3wSLq1kjh8q7NyOnVgnPbZ/hZUc65M8LDoYo6UlzCh6s/J3fRRt5aspk9h4poXCeNq3u04MZ+rejcvF68S3TOJTEPhwRw8Egx767YSu5HG3ln+VYOF5XQq1UDbhrQmqt7NKdWmp8q65yrXB4OCWbnvsO8uvAzXpm7nvyte6lbI4WhvVty6+A2tM+sE+/ynHNJwsMhQZkZeet28vKc9fx18SYOF5VwyTlNuP38dgxs14jgdlTOOXdSPBySwPa9h3hp9jpe+nAdO/YdplvLetx+Xjuu7tHcD2A7506Kh0MSOXikmNcWfsYz/1rNqm37aN2oFndd3IHr+rQk1UPCOXcCPBySUEmJMf2Trfx++qcs+Ww3rRrVZOxFHfhKnyzSUjwknHMV83BIYmbGjE+28vvpK/m4oJCWDWpy96Ud+WqfLL8C2zl3XMcLB/+KmeAkcUnnprwxdjDP3daPjDppfH/qx1z5+38x45MtJEP4O+fOPA+HJCGJi89uwhtjBzP+pj4cKirm68/nMXzChyxcvzPe5TnnEoyHQ5KRxFU9mvP2vRfy85yurNq2l+se/4DvvLKQTYUH4l2ecy5BeDgkqdRwiJGD2vDe9y7mO1/qwFtLN3PJ795j/Mx8DhUVx7s851wV5+GQ5GrXSOHey89m+r0XckHHTB6ctoLLH/4n05f78QjnXPk8HKqJVo1q8eTIvrw0uj+p4RCjX8jjtufnsWHH/niX5pyrgjwcqpnzO2by97vP50dXdWbemh1c9vB7TPjnKoqKS+JdmnOuCvFwqIZSwyFuP78db997Ied1aMyv/vYJOePfZ3FBYbxLc85VETGFg6QhklZIypc0roz5NSRNDubPkdQmaM+QNFPSXkmPRfWvK2lR1Gu7pEeCebdK2hY17/bKGaorrUWDmjx9SzaP39yHrXsOkTN+Fr94cxn7DxfFuzTnXJxV+JAASWFgPHAZUADMk5RrZsuiuo0GdppZB0nDgQeAG4GDwI+BbsELADPbA/SK+h3zgVej1jfZzO466VG5mEniyu7NGdyhMQ+89QnPzFrD28u38Nvre9KvTaN4l+eci5NYthz6A/lmttrMDgOTgJxSfXKAF4L3U4FLJMnM9pnZLCIhUSZJHYEmwL9OuHpXaerXTOVX13Vn8piBlJhxw1Oz+eVfl3HwiJ/26lx1FEs4tAQ2RE0XBG1l9jGzIqAQyIixhhFEthSiz6v8qqSPJU2V1KqshSSNkZQnKW/btm0x/ipXkQHtMnjr7gu4qX9rnv7XGq7+wyw+LtgV77Kcc2dYLOFQ1t3bSp8gH0uf8gwHXoma/j+gjZn1AN7h31sk/7lyswlmlm1m2ZmZmTH+KheL2jVS+OV13Xnh6/3Ze7CI6x7/gIf+sYIjfkaTc9VGLOFQAER/e88CNpbXR1IKUB/YUdGKJfUEUsxs/tE2M/vczA4Fk08DfWOo0Z0GF3bKZNp3LyCnVwsenZHPsCdns/5zvy7CueoglnCYB3SU1FZSGpFv+rml+uQCo4L3w4AZFtvltyP4z60GJDWPmrwWWB7DetxpUr9mKg/d0IvHb+7Dmm17ufLRf/H6ws/iXZZz7jSr8GwlMyuSdBcwDQgDz5rZUkn3A3lmlgtMBF6SlE9ki2H40eUlrQXqAWmShgKXR53pdANwZalf+R1J1wJFwbpuPYXxuUpyZffm9Miqzz2TFnHP5EX8c+U27s/pRp0aFX6EnHMJyB/2405IUXEJf5iRzx9mrKR1o1o8OqI3PbIaxLss59xJ8If9uEqTEg7x3cs6MWnMIA4XlfCVxz9g4qw1fhM/55KMh4M7Kf3bNuLvd1/Axec04edvLmPsywvYc/BIvMtyzlUSDwd30urXSmXCyL7cd8U5TFu6hZzH3ueTzbvjXZZzrhJ4OLhTIok7L2zPy7cPYM+hIoaOf5+/zC+Id1nOuVPk4eAqxYB2Gfz1O+fRq1UD/uvPH3Hfq4v91hvOJTAPB1dpmtRN54+jB/DNi9rzytz13DjhQzYXlntbLedcFebh4CpVSjjED4acw1Mj+5K/ZQ/XPDaL+esqvFjeOVfFeDi40+LLXZvx2tjB1EoLM3zCh0yauz7eJTnnToCHgzttOjWtyxtjBzOwXQbjXl3MT95Y4jfvcy5BeDi406pBrTSeu7UfYy5ox4uz1/G1Z+bw+d5DFS/onIsrDwd32qWEQ/zPlZ155MZeLNqwi2v9egjnqjwPB3fGDO3dkqnfOJeikhKGPTGbd1dsjXdJzrlyeDi4M6p7Vn3eGHseZ2XU4uvPz+PF2WvjXZJzrgweDu6Ma1Y/nSl3DuJL5zTlJ28s5We5Syku8Rv3OVeVeDi4uKhdI4WnRvbl9vPa8vwHa7njxTz2HiqKd1nOuYCHg4ubcEj86Oou/GJoN977dBvDnviAjbsOxLss5xwxhoOkIZJWSMqXNK6M+TUkTQ7mz5HUJmjPkDRT0l5Jj5Va5t1gnYuCV5Pjrcslr68NPIvnbu3HZzsPkDP+fZZ8Vhjvkpyr9ioMB0lhYDxwBdAFGCGpS6luo4GdZtYBeBh4IGg/CPwY+O9yVn+zmfUKXkdPXSlvXS6JXdApk79861xSQ+LGp2bzr5Xb4l2Sc9VaLFsO/YF8M1ttZoeBSUBOqT45wAvB+6nAJZJkZvvMbBaRkIhVmes6geVdgurUtC6vfmswrRrV4rbn5vHqAr/1t3PxEks4tAQ2RE0XBG1l9jGzIqAQyIhh3c8Fu5R+HBUAMa1L0hhJeZLytm3zb5nJoln9dKZ8YxD92jTi3ikf8cS7q/wRpM7FQSzhUNa39tL/WmPpU9rNZtYdOD94jTyRdZnZBDPLNrPszMzMCn6VSyT10lN5/uv9uLZnCx546xM/1dW5OIglHAqAVlHTWcDG8vpISgHqA8e9T7OZfRb83AO8TGT31UmtyyWfGilhHrmxF2MuaMcLs9cx9k8L/OFBzp1BsYTDPKCjpLaS0oDhQG6pPrnAqOD9MGCGHWdfgKQUSY2D96nA1cCSk1mXS16hkPifKzvz46u7MG3ZZkZOnMOu/YfjXZZz1UJKRR3MrEjSXcA0IAw8a2ZLJd0P5JlZLjAReElSPpFv+cOPLi9pLVAPSJM0FLgcWAdMC4IhDLwDPB0sUu66XPU0+ry2NK1Xg3snf8T1T87mpdEDaFY/Pd5lOZfUlAxfyrOzsy0vLy/eZbjT7INV2xnz4nwa1Erlj6MH0KZx7XiX5FxCkzTfzLLLmudXSLuEcW77xrxyx0D2Hy5m2JOzWbbRb/vt3Oni4eASSves+ky5cxCpYXHjhNnkrfVzFZw7HTwcXMLp0KQOU795Lpl1avC1iXOY6c+FcK7SeTi4hNSyQU2mfGMQ7TPrcMcLeeR+VPrsaufcqfBwcAmrcZ0avDJmIH3Oasjdkxbyxw/Xxbsk55KGh4NLaPXSU3nx6/350tlN+NHrSxg/Mz/eJTmXFDwcXMJLTw3z5Mi+DO3VggenreDBaZ/4/ZicO0UVXgTnXCJIDYd46IZe1ExLYfzMVew/XMxPru6C39DXuZPj4eCSRigkfnVdN2qkhHju/bUcPFLCL4d2IxTygHDuRHk4uKQiiZ9e04VaaWEef3cVh44U85thPUgJ+x5U506Eh4NLOpL4/pBzqJka5ndvf8qhohIevrEXaSkeEM7FysPBJa1vX9KR9NQwv/zbcg4VFfPYTX1ITw3HuyznEoJ/lXJJ7Y4L2vHznK68s3wrd7yYx4HD/kwI52Lh4eCS3shBbfjNsB68n7+dUc/NZe+honiX5FyV5+HgqoUbslvxyPDezF+3k689M4fCA0fiXZJzVVpM4SBpiKQVkvIljStjfg1Jk4P5cyS1CdozJM2UtFfSY1H9a0n6q6RPJC2V9OuoebdK2iZpUfC6/dSH6Rxc27MFj9/ch6UbCxk5cQ6F+z0gnCtPheEgKQyMB64AugAjJHUp1W00sNPMOgAPAw8E7QeBHwP/Xcaqf2tm5wC9gcGSroiaN9nMegWvZ05oRM4dx5e7NuOJm/uyfNNuvuYB4Vy5Ytly6A/km9lqMzsMTAJySvXJAV4I3k8FLpEkM9tnZrOIhMQXzGy/mc0M3h8GFgBZpzAO52J2aZemPPm1vqzYvIebJ37oz6V2rgyxhENLYEPUdEHQVmYfMysCCoGMWAqQ1AC4Bpge1fxVSR9LmiqpVTnLjZGUJylv27Ztsfwq575wSeemPDWyL59u3stNT89h5z4PCOeixRIOZd17oPRdzWLpc+yKpRTgFeBRM1sdNP8f0MbMegDv8O8tkv9cudkEM8s2s+zMzMyKfpVzx7j4nCZMuKUv+dv2ctMzc9jhAeHcF2IJhwIg+tt7FlD6ySpf9An+4NcHYnl+4wRgpZk9crTBzD43s0PB5NNA3xjW49xJuejsJjxzSzart+3lpqc/5PO9hypeyLlqIJZwmAd0lNRWUhowHMgt1ScXGBW8HwbMsArumSzpF0RC5J5S7c2jJq8FlsdQo3Mn7YJOmTwzKps12/dx09Nz2O4B4VzF4RAcQ7gLmEbkD/UUM1sq6X5J1wbdJgIZkvKBe4EvTneVtBZ4CLhVUoGkLpKygB8SOftpQalTVr8TnN76EfAd4NbKGKhzx3N+x0yevbUf63bs46anP2TbHg8IV70pGR6Kkp2dbXl5efEuwyWBD/K38/UX5pHVsBYv3zGAJnXT412Sc6eNpPlmll3WPL9C2rko53ZozHO39ueznQcYMeFDtu4+WPFCziUhDwfnShnUPoPnb+vHpsKDDH/aA8JVTx4OzpVhQLsMnr+tP5sLDzLi6Q/ZuscDwlUvHg7OlaN/20Y8d2s/Nu46yM1+FpOrZjwcnDuOAe0yePbWfmzYuZ+bn57j10G4asPDwbkKDGqfwcRR/Vj7+T5ufsZvteGqBw8H52IwuENjnr4lm9XbIwHhN+tzyc7DwbkYXdApkwkj+5K/da/f7tslPQ8H507ARWc34amRkdt9j3zWnyjnkpeHg3Mn6OJzmnzxwKBRz85lz0EPCJd8PBycOwmXdmnK+Jv6sOSzQkY9O5e9h4riXZJzlcrDwbmTdHnXZjx2U28+Kijktufmss8DwiURDwfnTsGQbs15dHhvFqzfxW3Pz2P/YQ8Ilxw8HJw7RVf1aM4jN/Yib+0Ovv78PA4cLo53Sc6dMg8H5yrBNT1b8PCNvZi7Zge3vziPg0c8IFxi83BwrpLk9GrJb6/vyQerPueOF/M8IFxCiykcJA2RtEJSvqRxZcyvIWlyMH+OpDZBe4akmZL2Snqs1DJ9JS0OlnlUkoL2RpLelrQy+Nnw1Ifp3JnxlT5Z/OarPZiVv507X5rvAeESVoXhICkMjAeuIPJYzxGSupTqNhrYaWYdgIeBB4L2g8CPgf8uY9VPAGOAjsFrSNA+DphuZh2B6UQ9ctS5RHB9dit+/ZXuvPfpNr75x/kcKvKAcIknli2H/kC+ma02s8PAJCCnVJ8c4IXg/VTgEkkys31mNotISHxBUnOgnpnNtshzSl8Ehpaxrhei2p1LGDf2a82vruvOzBXbGPunBRwuKol3Sc6dkFjCoSWwIWq6IGgrs4+ZFQGFQEYF6ywoZ51NzWxTsK5NQJOyViBpjKQ8SXnbtm2LYRjOnVk3DWjNz4d2453lWxn7sgeESyyxhIPKaLOT6HMq/Y/tbDbBzLLNLDszM/NEFnXujBk58Czuz+nK28u28O1XFnCk2APCJYZYwqEAaBU1nQVsLK+PpBSgPrCjgnVmlbPOLcFup6O7n7bGUKNzVdYtg9rw02u6MG3pFu6etNADwiWEWMJhHtBRUltJacBwILdUn1xgVPB+GDAjOJZQpmB30R5JA4OzlG4B3ihjXaOi2p1LWLcNbsuPrurM3xZv5p7JiyjygHBVXEpFHcysSNJdwDQgDDxrZksl3Q/kmVkuMBF4SVI+kS2G4UeXl7QWqAekSRoKXG5my4BvAs8DNYG/By+AXwNTJI0G1gPXV8ZAnYu3289vhxn88m/LCUk8fENPUsJ+qZGrmnScL/gJIzs72/Ly8uJdhnMxefK9Vfz675+Q06sFD93Qi3CorENwzp1+kuabWXZZ8yrccnDOVa5vXNie4hLjwWkrCEs8eH1PDwhX5Xg4OBcHYy/uQEmJ8bu3PyUUEr/5ag9CHhCuCvFwcC5Ovn1JR4rNeOSdlYQEv/6KB4SrOjwcnIujey7tREmJ8eiMfMIh8cuh3T0gXJXg4eBcnH33sk4UmzF+5ipCEr8Y2o3gPpTOxY2Hg3NxJon/vvxsSgyeeDcSEPfndPWAcHHl4eBcFSCJ73/5bEpKjKf+uZqQ4GfXekC4+PFwcK6KkMS4K86huMR4ZtYaQiHxk6u7eEC4uPBwcK4KkcQPr+pMsRnPvb+WcDDtAeHONA8H56oYKbLFYAbPzFpDOBTZovCAcGeSh4NzVZAkfnpNF4qPHoMIRY5JeEC4M8XDwbkqShL/e21XSsx44t1VhCX+6/JOHhDujPBwcK4KC4XEz3O6UWLGYzPzCYXEvZd1indZrhrwcHCuigsFV06XlMCj0yO32rjnUg8Id3p5ODiXAEIh8f++0v2LezEB3H1JR9/F5E4bDwfnEkQoJB74ag8AHnlnJUXF5scg3GkT02OoJA2RtEJSvqRxZcyvIWlyMH+OpDZR8+4L2ldI+nLQdrakRVGv3ZLuCeb9TNJnUfOurJyhOpf4wsHtvYf3a8VjM/N54K0VJMMDu1zVU+GWg6QwMB64DCgA5knKDR71edRoYKeZdZA0HHgAuFFSFyKPDO0KtADekdTJzFYAvaLW/xnwWtT6Hjaz35768JxLPqGQ+NV13UkJiyffW0VRcYlfKOcqXSy7lfoD+Wa2GkDSJCAHiA6HHOBnwfupwGOKfFJzgElmdghYEzxjuj8wO2rZS4BVZrbuVAbiXHVy9CymlFCIZ2atoajE+Ok1fqsNV3li2a3UEtgQNV0QtJXZx8yKgEIgI8ZlhwOvlGq7S9LHkp6V1LCsoiSNkZQnKW/btm0xDMO55HL0QrnR57Xl+Q/W8uM3llBS4ruYXOWIJRzK+ipS+hNYXp/jLispDbgW+HPU/CeA9kR2O20CfldWUWY2wcyyzSw7MzOz/OqdS2KS+NFVnbnzwnb88cP1/PD1xR4QrlLEslupAGgVNZ0FbCynT4GkFKA+sCOGZa8AFpjZlqMN0e8lPQ28GUONzlVbkhg35BxSQyEem5lPUbHx66/2IOxPlHOnIJYth3lAR0ltg2/6w4HcUn1ygVHB+2HADIucQpELDA/OZmoLdATmRi03glK7lCQ1j5q8DlgS62Ccq64U3Frjnks78uf5BXzvzx9R7FsQ7hRUuOVgZkWS7gKmAWHgWTNbKul+IM/McoGJwEvBAecdRAKEoN8UIgevi4CxZlYMIKkWkTOg7iz1K38jqReR3U9ry5jvnCuDJO65tBNhid+9/SlFJcZDN/QkJRzTGevO/QclwznS2dnZlpeXF+8ynKsynnh3FQ+89QlXdW/OI8N7keoB4cogab6ZZZc1z6+Qdi4JffOi9qSGxS/+upzDxSX8YURv0lPD8S7LJRD/OuFckrr9/Hbcn9OVt5dt4Y4X89h/uCjeJbkE4uHgXBK7ZVAbHhzWg/fzt3PLxLnsPngk3iW5BOHh4FySuz67FY/d1IePCnZx09MfsmPf4XiX5BKAh4Nz1cCV3ZszYWQ2K7fs5canZrNl98F4l+SqOA8H56qJi89pwvO39WfjrgNc/+RsNuzYH++SXBXm4eBcNTKofQZ/vH0AhQeOcP2Ts8nfujfeJbkqysPBuWqmd+uGTBozkKKSEm58ajbLNu6Od0muCvJwcK4a6ty8HlPuHESNlBDDJ8xm/rqd8S7JVTEeDs5VU+0y6zDlG4NoVDuNrz0zh5krtsa7JFeFeDg4V41lNazFn79xLm0b1+aOF/J4feFn8S7JVREeDs5Vc5l1azDpzoFkt2nIPZMXMXHWmniX5KoADwfnHPXSU3n+tv4M6dqMn7+5jN+89QnJcFNOd/I8HJxzAKSnhhl/cx9uGtCax99dxbi/LKaouCTeZbk48buyOue+EA6JXw7tRuPaaTw6I58d+w/7HV2rqZi2HCQNkbRCUr6kcWXMryFpcjB/jqQ2UfPuC9pXSPpyVPtaSYslLZKUF9XeSNLbklYGPxue2hCdcydCEvdefjb/e21X3lm+hVuenUvhAb9hX3VTYThICgPjiTzvuQswQlKXUt1GAzvNrAPwMPBAsGwXIk+F6woMAR4P1nfUxWbWq9TDJsYB082sIzA9mHbOnWGjzm3D74f3ZuH6ndz41Gw2FR6Id0nuDIply6E/kG9mq83sMDAJyCnVJwd4IXg/FbhEkoL2SWZ2yMzWAPnB+o4nel0vAENjqNE5dxpc27MFz93an4KdBxg6/n2/mroaiSUcWgIboqYLgrYy+5hZEVAIZFSwrAH/kDRf0pioPk3NbFOwrk1Ak9iG4pw7Hc7r2Jip3xxESOKGp2bz3qfb4l2SOwNiCQeV0Vb6HLfy+hxv2cFm1ofI7qqxki6IoZZ//0JpjKQ8SXnbtvmH1bnT6Zxm9XjtW4Np1agWX39+HpPnrY93Se40iyUcCoBWUdNZwMby+khKAeoDO463rJkd/bkVeI1/727aIql5sK7mQJnX9JvZBDPLNrPszMzMGIbhnDsVzeqnM+XOgQzu0Jgf/GUxv522wq+FSGKxhMM8oKOktpLSiBxgzi3VJxcYFbwfBsywyKcmFxgenM3UFugIzJVUW1JdAEm1gcuBJWWsaxTwxskNzTlX2eqmpzJxVDbD+7XisZn53DvlIw4X+bUQyajC6xzMrEjSXcA0IAw8a2ZLJd0P5JlZLjAReElSPpEthuHBskslTQGWAUXAWDMrltQUeC1yzJoU4GUzeyv4lb8GpkgaDawHrq/E8TrnTlFqOMT/+0p3WjWqxYPTVrCp8ABPfS2b+rVS412aq0RKhs3C7Oxsy8vLq7ijc65Svb7wM7439SNaNazFM6OyaZdZJ94luRMgaX6pSwm+4LfPcM6dtKG9W/LyHQPZdeAIQ8e/z79W+skhycLDwTl3Svq1acQbYwfTokFNbn1uHi98sNYPVCcBDwfn3Clr1agWU795Lhef3YSf5i7lh68v4YjftC+heTg45ypFnRopTBjZl29e1J6X56xn5MQ57Nx3ON5luZPk4eCcqzShkPjBkHN46IaeLFi3i6GPv8+nW/bEuyx3EjwcnHOV7it9sph050D2HSpm6Pj3+evHm+JdkjtBHg7OudOiT+uGvPnt8zi7WV3GvryAX/1tuT88KIF4ODjnTptm9dOZPGYQIweexYR/rmbkxLls33so3mW5GHg4OOdOq7SUED8f2o3fXt+TBet3cs0fZrFow654l+Uq4OHgnDsjhvXN4i/fPJdwSNzw5GxenrPer4eowjwcnHNnTLeW9fm/u85jYPsM/ue1xXx38iL2HiqKd1muDB4OzmfhFp4AAA1USURBVLkzqmHtNJ67tR/3XtaJ3I82cs0fZrF0Y2G8y3KleDg45864cEh855KOvHzHQPYfLuK6xz/gpdl+242qxMPBORc3A9tl8LfvnM+57TP48RtL+dafFlB44Ei8y3J4ODjn4iyjTg2eHdWP+644h7eXbeGqR/9F3tod8S6r2vNwcM7FXSgk7rywPZPvHATADU/N5sFpn/hT5uIopnCQNETSCkn5ksaVMb+GpMnB/DmS2kTNuy9oXyHpy0FbK0kzJS2XtFTS3VH9fybpM0mLgteVpz5M51wi6HtWQ/5+9/kM65vF+Jmr+MoT75O/1e/NFA8VhoOkMDAeuALoAoyQ1KVUt9HATjPrADwMPBAs24XII0O7AkOAx4P1FQH/ZWadgYHA2FLrfNjMegWvv53SCJ1zCaVueiq/GdaTJ7/Wl427DnLVo7N47v01lJT4weozKZYth/5AvpmtNrPDwCQgp1SfHOCF4P1U4BJFHhCdA0wys0NmtgbIB/qb2SYzWwBgZnuA5UDLUx+Ocy5ZDOnWjLfuiRys/t//W8ao5+aycdeBeJdVbcQSDi2BDVHTBRz7h/yLPmZWBBQCGbEsG+yC6g3MiWq+S9LHkp6V1LCsoiSNkZQnKW/bNn80oXPJqEnddJ69tR+/vK4beWt3cvnD/+SPH67zrYgzIJZwUBltpf/PlNfnuMtKqgP8BbjHzHYHzU8A7YFewCbgd2UVZWYTzCzbzLIzMzOPPwLnXMKSxM0DzuIf372Anq3q86PXlzDi6Q9Zu31fvEtLarGEQwHQKmo6C9hYXh9JKUB9YMfxlpWUSiQY/mRmrx7tYGZbzKzYzEqAp4ns1nLOVXOtGtXij6MH8MBXu7Ns026+/Mg/mfDPVRT7VsRpEUs4zAM6SmorKY3IAebcUn1ygVHB+2HADItc6pgLDA/OZmoLdATmBscjJgLLzeyh6BVJah41eR2w5EQH5ZxLTpK4sV9r3rn3Qs7vmMmv/vYJ1z3+Ph8X+F1eK1uF4RAcQ7gLmEbkwPEUM1sq6X5J1wbdJgIZkvKBe4FxwbJLgSnAMuAtYKyZFQODgZHAl8o4ZfU3khZL+hi4GPhuZQ3WOZccmtZL5+lb+vKHEb3ZVHiQnPHv88PXFrNrvz+zurIoGe5lkp2dbXl5efEuwzkXB7sPHuHhtz/lhQ/W0qBWGuOGnMOwvlmEQmUd8nTRJM03s+yy5vkV0s65hFYvPZWfXtOVN799Pm0b1+b7f/mY65+azeICv9PrqfBwcM4lhS4t6vHnOwfxm2E9WLN9H9c8Not7Jy/yayNOkoeDcy5phELihuxWvPu9i/jGhe15c/EmLv7tuzw47RN/qNAJ8nBwziWdeumpjLviHKbfeyFDujVj/MxVXPTgTP744TqOFPvN/GLh4eCcS1qtGtXi98N788bYwbRrXIcfvb6EL/3uXf6ct4EiD4nj8nBwziW9nq0aMPnOgTx3az8a1Ezje1M/5tKH3uO1hQV+EV05/FRW51y1Yma8vWwLD7+zkuWbdtM+szZ3Xtieob1akpZSvb4vH+9UVg8H51y1VFJiTFu6mUdn5LN8026a1Uvn6+e1YUT/1tRNT413eWeEh4NzzpXDzPjnyu089d4qPlj1OXXTU7h5wFmMHHQWLRvUjHd5p5WHg3POxeDjgl089d5q/r5kEwCXdm7KyEFnMbh946S84trDwTnnTsCGHft5Ze56Js/bwOf7DtO2cW1uHtCa63q3JKNOjXiXV2k8HJxz7iQcKirm74s389KH65i/bicpIXHR2Zlc1zuLSzo3IT01HO8ST8nxwiHlTBfjnHOJokZKmKG9WzK0d0tWbN7DqwsKeH3RZ7yzfCt101O4ukdzrureggHtGpEaTq4znXzLwTnnTkBxiTF71ee8uqCAt5ZuZv/hYuqlp3BJ56Z8uWtTLuiUSa20xPje7buVnHPuNDhwuJhZ+duZtnQz05dvYef+I9RICTGgXQbndchgcIfGdG5Wr8oezPZwcM6506youIR5a3fyj2WbmbVyOyu37gWgUe00BrXLoM9ZDendugFdW9SjRkrVOFZxysccJA0Bfg+EgWfM7Nel5tcAXgT6Ap8DN5rZ2mDefcBooBj4jplNO946g8eJTgIaAQuAkWbmj3dyzlVpKeEQg9pnMKh9BgBbdh/k/fztzMrfzpzVO/jr4sjpsWnhEF1a1KN7y/p0alaXs5tGXvVrVa0L7yrccpAUBj4FLgMKiDxTeoSZLYvq8y2gh5l9Q9Jw4Dozu1FSF+AVoD/QAngH6BQsVuY6JU0BXjWzSZKeBD4ysyeOV6NvOTjnqrotuw+ycP0uFq7fycL1u1i+aTd7om4j3rReDVo3qkVWw1pkNaxJVsOaNKmXTqNaaTSqnUbD2mnUTgsjVd4uqlPdcugP5JvZ6mBlk4AcIs+FPioH+FnwfirwmCIjyAEmmdkhYE3wjOn+Qb9j1ilpOfAl4KagzwvBeo8bDs45V9U1rZfOkG7NGNKtGRC5MntT4UFWbNnDis17WLllLwU79zN3zQ5yPzpY5g0BU8MiPTVMemqYGikhaqSEuOfSTlzTs0Wl1xtLOLQENkRNFwADyutjZkWSCoGMoP3DUsu2DN6Xtc4MYJeZFZXR/z9IGgOMAWjdunUMw3DOuapDEi0a1KRFg5pcfHaT/5hXVFzCpsKDbN1ziF37D7Nj32F27j/Mzv1HOHikmINHSjhUVMyhohIanKbdUbGEQ1nbMKUjrbw+5bWXdULw8fof22g2AZgAkd1KZfVxzrlElBIO0apRLVo1qhW3GmK5aqMAaBU1nQVsLK+PpBSgPrDjOMuW174daBCso7zf5Zxz7jSLJRzmAR0ltZWUBgwHckv1yQVGBe+HATMscqQ7FxguqUZwFlJHYG556wyWmRmsg2Cdb5z88Jxzzp2MCncrBccQ7gKmETnt9FkzWyrpfiDPzHKBicBLwQHnHUT+2BP0m0Lk4HURMNbMigHKWmfwK38ATJL0C2BhsG7nnHNnkF8E55xz1dTxTmVNrjtFOeecqxQeDs45547h4eCcc+4YHg7OOeeOkRQHpCVtA9ad5OKNiVxfkQx8LFWTj6XqSZZxwKmN5SwzyyxrRlKEw6mQlFfe0fpE42OpmnwsVU+yjANO31h8t5JzzrljeDg455w7hodDcPO+JOFjqZp8LFVPsowDTtNYqv0xB+ecc8fyLQfnnHPH8HBwzjl3jGodDpKGSFohKV/SuHjXUxFJz0raKmlJVFsjSW9LWhn8bBi0S9Kjwdg+ltQnfpX/J0mtJM2UtFzSUkl3B+2JOJZ0SXMlfRSM5X+D9raS5gRjmRzcmp7g9vWTg7HMkdQmnvWXRVJY0kJJbwbTCTkWSWslLZa0SFJe0JaIn7EGkqZK+iT4NzPoTIyj2oaDpDAwHrgC6AKMkNQlvlVV6HlgSKm2ccB0M+sITA+mITKujsFrDFXrOdxFwH+ZWWdgIDA2+G+fiGM5BHzJzHoCvYAhkgYCDwAPB2PZCYwO+o8GdppZB+DhoF9VczewPGo6kcdysZn1iroOIBE/Y78H3jKzc4CeRP7fnP5xmFm1fAGDgGlR0/cB98W7rhjqbgMsiZpeATQP3jcHVgTvnwJGlNWvqr2IPNDpskQfC1ALWEDkeejbgZTSnzUizzAZFLxPCfop3rVHjSEr+GPzJeBNIo/uTdSxrAUal2pLqM8YUA9YU/q/65kYR7XdcgBaAhuipguCtkTT1Mw2AQQ/jz6pPCHGF+yK6A3MIUHHEuyGWQRsBd4GVgG7zKwo6BJd7xdjCeYXAhlntuLjegT4PlASTGeQuGMx4B+S5ksaE7Ql2mesHbANeC7Y1feMpNqcgXFU53BQGW3JdF5vlR+fpDrAX4B7zGz38bqW0VZlxmJmxWbWi8i37v5A57K6BT+r7FgkXQ1sNbP50c1ldK3yYwkMNrM+RHa1jJV0wXH6VtWxpAB9gCfMrDewj3/vQipLpY2jOodDAdAqajoL2BinWk7FFknNAYKfW4P2Kj0+SalEguFPZvZq0JyQYznKzHYB7xI5jtJA0tHH8EbX+8VYgvn1iTxatyoYDFwraS0wiciupUdIzLFgZhuDn1uB14gEd6J9xgqAAjObE0xPJRIWp30c1Tkc5gEdgzMx0og89zo3zjWdjFxgVPB+FJH990fbbwnOXhgIFB7dDI03SSLybPDlZvZQ1KxEHEumpAbB+5rApUQOGM4EhgXdSo/l6BiHATMs2Dkcb2Z2n5llmVkbIv8eZpjZzSTgWCTVllT36HvgcmAJCfYZM7PNwAZJZwdNlwDLOBPjiPcBlzgf7LkS+JTIPuIfxrueGOp9BdgEHCHyDWE0kX2804GVwc9GQV8RORtrFbAYyI53/VHjOI/Ipu7HwKLgdWWCjqUHsDAYyxLgJ0F7O2AukA/8GagRtKcH0/nB/HbxHkM547oIeDNRxxLU/FHwWnr033eCfsZ6AXnBZ+x1oOGZGIffPsM559wxqvNuJeecc+XwcHDOOXcMDwfnnHPH8HBwzjl3DA8H55xzx/BwcM45dwwPB+ecc8f4/4xXy8v2annSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_lambda = lambda epoch: (1.0 + math.cos((epoch-1)*math.pi/args.epochs))\n",
    "\n",
    "x = np.arange(600)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([args.base_lr * lr_lambda(i) for i in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "acc_func = CategoricalAccuracy()\n",
    "\n",
    "def reset_all_metrics():\n",
    "    acc_func.reset()\n",
    "        \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "title = \"%s_supervised\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(\"%s/%s\" % (args.tensorboard_dir, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    m1.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "    \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        # Only one view interesting, no U\n",
    "        X_S = X_S[0]\n",
    "        y_S = y_S[0]\n",
    "        \n",
    "        X_S, y_S = X_S.cuda(), y_S.cuda()\n",
    "\n",
    "        # ======== perform prediction ========\n",
    "        logits_S = m1(X_S)\n",
    "        _, pred_S = torch.max(logits_S, 1)\n",
    "\n",
    "\n",
    "        # ======== calculate loss ========\n",
    "        loss_sup = criterion(logits_S, y_S)\n",
    "        total_loss = loss_sup\n",
    "        \n",
    "        # ======== backpropagation =======\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        acc = acc_func(pred_S, y_S)\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        # print statistics\n",
    "        print(\"Epoch %s: %.2f%% : train acc: %.3f - Loss: %.3f - time: %.2f\" % (\n",
    "            epoch, (batch / len(sampler)) * 100,\n",
    "            acc,\n",
    "            running_loss / (batch+1),\n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar('train/acc', acc, epoch)\n",
    "    \n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    m1.eval()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(val_loader):\n",
    "            X_S = X.cuda()\n",
    "            y_S = y.cuda()\n",
    "\n",
    "            logits_S = m1(X_S)\n",
    "            _, pred_S = torch.max(logits_S, 1)\n",
    "            \n",
    "            loss_val = criterion(logits_S, y_S)\n",
    "            \n",
    "            acc_val = acc_func(pred_S, y_S)\n",
    "        \n",
    "        print(\"\\nEpoch %s: Val acc: %.3f - loss: %.3f\" % (\n",
    "            epoch,\n",
    "            acc_val,\n",
    "            loss_val.item()\n",
    "        ))\n",
    "    \n",
    "    tensorboard.add_scalar(\"val/acc\", acc_val, epoch)\n",
    "    tensorboard.add_scalar(\"val/loss\", loss_val.item(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: 71.43% : train acc: 0.147 - Loss: 2.273 - time: 3.14\n",
      "Epoch 0: Val acc: 0.095 - loss: 2.288\n",
      "\n",
      "Epoch 1: 71.43% : train acc: 0.196 - Loss: 2.180 - time: 2.65\n",
      "Epoch 1: Val acc: 0.137 - loss: 2.261\n",
      "\n",
      "Epoch 2: 71.43% : train acc: 0.233 - Loss: 2.075 - time: 2.79\n",
      "Epoch 2: Val acc: 0.165 - loss: 2.226\n",
      "\n",
      "Epoch 3: 71.43% : train acc: 0.272 - Loss: 1.970 - time: 2.89\n",
      "Epoch 3: Val acc: 0.197 - loss: 2.175\n",
      "\n",
      "Epoch 4: 71.43% : train acc: 0.298 - Loss: 1.849 - time: 2.81\n",
      "Epoch 4: Val acc: 0.214 - loss: 2.117\n",
      "\n",
      "Epoch 5: 71.43% : train acc: 0.320 - Loss: 1.746 - time: 2.82\n",
      "Epoch 5: Val acc: 0.248 - loss: 2.050\n",
      "\n",
      "Epoch 6: 71.43% : train acc: 0.346 - Loss: 1.622 - time: 2.83\n",
      "Epoch 6: Val acc: 0.238 - loss: 1.997\n",
      "\n",
      "Epoch 7: 71.43% : train acc: 0.348 - Loss: 1.496 - time: 2.75\n",
      "Epoch 7: Val acc: 0.233 - loss: 1.956\n",
      "\n",
      "Epoch 8: 71.43% : train acc: 0.365 - Loss: 1.390 - time: 2.66\n",
      "Epoch 8: Val acc: 0.242 - loss: 1.923\n",
      "\n",
      "Epoch 9: 71.43% : train acc: 0.384 - Loss: 1.299 - time: 2.82\n",
      "Epoch 9: Val acc: 0.319 - loss: 1.904\n",
      "\n",
      "Epoch 10: 71.43% : train acc: 0.451 - Loss: 1.203 - time: 2.73\n",
      "Epoch 10: Val acc: 0.336 - loss: 1.892\n",
      "\n",
      "Epoch 11: 71.43% : train acc: 0.477 - Loss: 1.103 - time: 2.96\n",
      "Epoch 11: Val acc: 0.345 - loss: 1.889\n",
      "\n",
      "Epoch 12: 71.43% : train acc: 0.502 - Loss: 1.016 - time: 2.82\n",
      "Epoch 12: Val acc: 0.373 - loss: 1.924\n",
      "\n",
      "Epoch 13: 71.43% : train acc: 0.539 - Loss: 0.948 - time: 2.81\n",
      "Epoch 13: Val acc: 0.413 - loss: 1.927\n",
      "\n",
      "Epoch 14: 71.43% : train acc: 0.571 - Loss: 0.871 - time: 2.83\n",
      "Epoch 14: Val acc: 0.398 - loss: 1.924\n",
      "\n",
      "Epoch 15: 71.43% : train acc: 0.575 - Loss: 0.818 - time: 2.89\n",
      "Epoch 15: Val acc: 0.397 - loss: 1.945\n",
      "\n",
      "Epoch 16: 71.43% : train acc: 0.570 - Loss: 0.768 - time: 2.69\n",
      "Epoch 16: Val acc: 0.404 - loss: 1.967\n",
      "\n",
      "Epoch 17: 71.43% : train acc: 0.583 - Loss: 0.726 - time: 2.67\n",
      "Epoch 17: Val acc: 0.393 - loss: 1.975\n",
      "\n",
      "Epoch 18: 71.43% : train acc: 0.579 - Loss: 0.660 - time: 2.75\n",
      "Epoch 18: Val acc: 0.392 - loss: 1.965\n",
      "\n",
      "Epoch 19: 71.43% : train acc: 0.586 - Loss: 0.629 - time: 2.68\n",
      "Epoch 19: Val acc: 0.419 - loss: 1.972\n",
      "\n",
      "Epoch 20: 71.43% : train acc: 0.607 - Loss: 0.594 - time: 2.63\n",
      "Epoch 20: Val acc: 0.406 - loss: 2.068\n",
      "\n",
      "Epoch 21: 71.43% : train acc: 0.604 - Loss: 0.588 - time: 2.80\n",
      "Epoch 21: Val acc: 0.438 - loss: 1.957\n",
      "\n",
      "Epoch 22: 71.43% : train acc: 0.628 - Loss: 0.523 - time: 2.79\n",
      "Epoch 22: Val acc: 0.449 - loss: 1.946\n",
      "\n",
      "Epoch 23: 71.43% : train acc: 0.650 - Loss: 0.458 - time: 2.85\n",
      "Epoch 23: Val acc: 0.429 - loss: 2.169\n",
      "\n",
      "Epoch 24: 71.43% : train acc: 0.637 - Loss: 0.461 - time: 2.82\n",
      "Epoch 24: Val acc: 0.439 - loss: 2.045\n",
      "\n",
      "Epoch 25: 71.43% : train acc: 0.642 - Loss: 0.439 - time: 2.85\n",
      "Epoch 25: Val acc: 0.423 - loss: 2.118\n",
      "\n",
      "Epoch 26: 71.43% : train acc: 0.635 - Loss: 0.424 - time: 2.70\n",
      "Epoch 26: Val acc: 0.445 - loss: 2.054\n",
      "\n",
      "Epoch 27: 71.43% : train acc: 0.654 - Loss: 0.374 - time: 2.78\n",
      "Epoch 27: Val acc: 0.454 - loss: 1.970\n",
      "\n",
      "Epoch 28: 71.43% : train acc: 0.665 - Loss: 0.362 - time: 2.86\n",
      "Epoch 28: Val acc: 0.429 - loss: 2.240\n",
      "\n",
      "Epoch 29: 71.43% : train acc: 0.661 - Loss: 0.338 - time: 2.69\n",
      "Epoch 29: Val acc: 0.457 - loss: 1.915\n",
      "\n",
      "Epoch 30: 71.43% : train acc: 0.669 - Loss: 0.313 - time: 2.77\n",
      "Epoch 30: Val acc: 0.456 - loss: 2.104\n",
      "\n",
      "Epoch 31: 71.43% : train acc: 0.676 - Loss: 0.301 - time: 2.76\n",
      "Epoch 31: Val acc: 0.430 - loss: 2.144\n",
      "\n",
      "Epoch 32: 71.43% : train acc: 0.663 - Loss: 0.278 - time: 2.69\n",
      "Epoch 32: Val acc: 0.475 - loss: 1.823\n",
      "\n",
      "Epoch 33: 71.43% : train acc: 0.692 - Loss: 0.256 - time: 2.87\n",
      "Epoch 33: Val acc: 0.459 - loss: 2.001\n",
      "\n",
      "Epoch 34: 71.43% : train acc: 0.682 - Loss: 0.236 - time: 2.85\n",
      "Epoch 34: Val acc: 0.445 - loss: 2.198\n",
      "\n",
      "Epoch 35: 71.43% : train acc: 0.671 - Loss: 0.250 - time: 2.90\n",
      "Epoch 35: Val acc: 0.443 - loss: 1.917\n",
      "\n",
      "Epoch 36: 71.43% : train acc: 0.678 - Loss: 0.225 - time: 2.96\n",
      "Epoch 36: Val acc: 0.484 - loss: 1.910\n",
      "\n",
      "Epoch 37: 71.43% : train acc: 0.704 - Loss: 0.197 - time: 2.86\n",
      "Epoch 37: Val acc: 0.438 - loss: 2.234\n",
      "\n",
      "Epoch 38: 71.43% : train acc: 0.682 - Loss: 0.187 - time: 2.72\n",
      "Epoch 38: Val acc: 0.476 - loss: 1.947\n",
      "\n",
      "Epoch 39: 71.43% : train acc: 0.699 - Loss: 0.178 - time: 2.84\n",
      "Epoch 39: Val acc: 0.480 - loss: 1.945\n",
      "\n",
      "Epoch 40: 71.43% : train acc: 0.700 - Loss: 0.178 - time: 2.76\n",
      "Epoch 40: Val acc: 0.467 - loss: 2.071\n",
      "\n",
      "Epoch 41: 71.43% : train acc: 0.696 - Loss: 0.173 - time: 2.65\n",
      "Epoch 41: Val acc: 0.456 - loss: 2.328\n",
      "\n",
      "Epoch 42: 71.43% : train acc: 0.696 - Loss: 0.140 - time: 2.81\n",
      "Epoch 42: Val acc: 0.473 - loss: 2.144\n",
      "\n",
      "Epoch 43: 71.43% : train acc: 0.706 - Loss: 0.132 - time: 2.89\n",
      "Epoch 43: Val acc: 0.455 - loss: 2.171\n",
      "\n",
      "Epoch 44: 71.43% : train acc: 0.694 - Loss: 0.129 - time: 2.72\n",
      "Epoch 44: Val acc: 0.462 - loss: 2.145\n",
      "\n",
      "Epoch 45: 71.43% : train acc: 0.682 - Loss: 0.263 - time: 2.87\n",
      "Epoch 45: Val acc: 0.263 - loss: 4.372\n",
      "\n",
      "Epoch 46: 71.43% : train acc: 0.536 - Loss: 0.436 - time: 2.86\n",
      "Epoch 46: Val acc: 0.428 - loss: 2.310\n",
      "\n",
      "Epoch 47: 71.43% : train acc: 0.671 - Loss: 0.199 - time: 2.82\n",
      "Epoch 47: Val acc: 0.447 - loss: 2.227\n",
      "\n",
      "Epoch 48: 71.43% : train acc: 0.688 - Loss: 0.142 - time: 2.69\n",
      "Epoch 48: Val acc: 0.463 - loss: 2.131\n",
      "\n",
      "Epoch 49: 71.43% : train acc: 0.700 - Loss: 0.133 - time: 2.87\n",
      "Epoch 49: Val acc: 0.475 - loss: 2.167\n",
      "\n",
      "Epoch 50: 71.43% : train acc: 0.713 - Loss: 0.098 - time: 2.68\n",
      "Epoch 50: Val acc: 0.473 - loss: 2.195\n",
      "\n",
      "Epoch 51: 71.43% : train acc: 0.708 - Loss: 0.106 - time: 2.85\n",
      "Epoch 51: Val acc: 0.490 - loss: 2.032\n",
      "\n",
      "Epoch 52: 71.43% : train acc: 0.717 - Loss: 0.100 - time: 2.71\n",
      "Epoch 52: Val acc: 0.449 - loss: 2.343\n",
      "\n",
      "Epoch 53: 71.43% : train acc: 0.697 - Loss: 0.091 - time: 2.81\n",
      "Epoch 53: Val acc: 0.483 - loss: 2.197\n",
      "\n",
      "Epoch 54: 71.43% : train acc: 0.719 - Loss: 0.074 - time: 2.71\n",
      "Epoch 54: Val acc: 0.479 - loss: 2.221\n",
      "\n",
      "Epoch 55: 71.43% : train acc: 0.713 - Loss: 0.079 - time: 2.65\n",
      "Epoch 55: Val acc: 0.484 - loss: 2.191\n",
      "\n",
      "Epoch 56: 71.43% : train acc: 0.715 - Loss: 0.079 - time: 2.75\n",
      "Epoch 56: Val acc: 0.480 - loss: 2.194\n",
      "\n",
      "Epoch 57: 71.43% : train acc: 0.715 - Loss: 0.070 - time: 2.75\n",
      "Epoch 57: Val acc: 0.489 - loss: 2.166\n",
      "\n",
      "Epoch 58: 71.43% : train acc: 0.722 - Loss: 0.068 - time: 2.80\n",
      "Epoch 58: Val acc: 0.481 - loss: 2.251\n",
      "\n",
      "Epoch 59: 71.43% : train acc: 0.718 - Loss: 0.053 - time: 2.70\n",
      "Epoch 59: Val acc: 0.483 - loss: 2.169\n",
      "\n",
      "Epoch 60: 71.43% : train acc: 0.719 - Loss: 0.056 - time: 2.80\n",
      "Epoch 60: Val acc: 0.476 - loss: 2.213\n",
      "\n",
      "Epoch 61: 71.43% : train acc: 0.716 - Loss: 0.048 - time: 2.65\n",
      "Epoch 61: Val acc: 0.493 - loss: 2.046\n",
      "\n",
      "Epoch 62: 71.43% : train acc: 0.726 - Loss: 0.048 - time: 2.81\n",
      "Epoch 62: Val acc: 0.494 - loss: 2.097\n",
      "\n",
      "Epoch 63: 71.43% : train acc: 0.726 - Loss: 0.051 - time: 2.70\n",
      "Epoch 63: Val acc: 0.495 - loss: 2.080\n",
      "\n",
      "Epoch 64: 71.43% : train acc: 0.728 - Loss: 0.044 - time: 2.66\n",
      "Epoch 64: Val acc: 0.502 - loss: 2.048\n",
      "\n",
      "Epoch 65: 71.43% : train acc: 0.730 - Loss: 0.047 - time: 2.70\n",
      "Epoch 65: Val acc: 0.479 - loss: 2.214\n",
      "\n",
      "Epoch 66: 71.43% : train acc: 0.716 - Loss: 0.049 - time: 2.74\n",
      "Epoch 66: Val acc: 0.494 - loss: 2.240\n",
      "\n",
      "Epoch 67: 71.43% : train acc: 0.728 - Loss: 0.039 - time: 2.88\n",
      "Epoch 67: Val acc: 0.485 - loss: 2.297\n",
      "\n",
      "Epoch 68: 71.43% : train acc: 0.723 - Loss: 0.036 - time: 2.70\n",
      "Epoch 68: Val acc: 0.491 - loss: 2.232\n",
      "\n",
      "Epoch 69: 71.43% : train acc: 0.725 - Loss: 0.034 - time: 2.69\n",
      "Epoch 69: Val acc: 0.492 - loss: 2.176\n",
      "\n",
      "Epoch 70: 71.43% : train acc: 0.726 - Loss: 0.035 - time: 2.88\n",
      "Epoch 70: Val acc: 0.495 - loss: 2.164\n",
      "\n",
      "Epoch 71: 71.43% : train acc: 0.728 - Loss: 0.036 - time: 2.67\n",
      "Epoch 71: Val acc: 0.496 - loss: 2.117\n",
      "\n",
      "Epoch 72: 71.43% : train acc: 0.728 - Loss: 0.029 - time: 2.73\n",
      "Epoch 72: Val acc: 0.498 - loss: 2.038\n",
      "\n",
      "Epoch 73: 71.43% : train acc: 0.730 - Loss: 0.031 - time: 2.81\n",
      "Epoch 73: Val acc: 0.486 - loss: 2.196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# .llll||=||llll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5f8108d8118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkurtosis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kurtosis\n",
    "\n",
    "for batch, (X, y) in enumerate(train_loader):\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "    \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        X_U = X_U.numpy()\n",
    "        print(X_U.shape)\n",
    "        \n",
    "        # max\n",
    "        val = X_U.max(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        # min\n",
    "        print(\"------------------\")\n",
    "        val = X_U.min(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        # mean\n",
    "        print(\"------------------\")\n",
    "        val = X_U.mean(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        # std\n",
    "        print(\"------------------\")\n",
    "        val = X_U.std(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
