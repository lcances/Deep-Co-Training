{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from datasetManager import DatasetManager\n",
    "from generators import Generator, CoTrainingGenerator\n",
    "from samplers import CoTrainingSampler\n",
    "import signal_augmentations as sa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.973823Z",
     "start_time": "2019-11-12T15:36:12.893994Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.value = 0\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.count += 1\n",
    "\n",
    "        \n",
    "class BinaryAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = (y_pred>0.5).float()\n",
    "            correct = (y_pred == y_true).float().sum()\n",
    "            self.value = correct/ (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "        \n",
    "class CategoricalAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.value = torch.mean((y_true == y_pred).float())\n",
    "            self.accumulate_value += self.value\n",
    "\n",
    "            return self.accumulate_value / self.count\n",
    "\n",
    "        \n",
    "class Ratio(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_adv_pred):\n",
    "        super().__call__()\n",
    "        \n",
    "        results = zip(y_pred, y_adv_pred)\n",
    "        results_bool = [int(r[0] != r[1]) for r in results]\n",
    "        self.value = sum(results_bool) / len(results_bool) * 100\n",
    "        self.accumulate_value += self.value\n",
    "        \n",
    "        return self.accumulate_value / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.997511Z",
     "start_time": "2019-11-12T15:36:12.975482Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:13.020782Z",
     "start_time": "2019-11-12T15:36:13.000410Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:34.259332Z",
     "start_time": "2019-11-12T15:36:34.233822Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "https://arxiv.org/pdf/1608.04363.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvPoolReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.MaxPool2d(kernel_size=(4, 2), stride=(4, 2)),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvPoolReLU(1, 24, 3, 1, 1),\n",
    "            ConvPoolReLU(24, 48, 3, 1, 1),\n",
    "            ConvPoolReLU(48, 48, 3, 1, 1),\n",
    "            ConvReLU(48, 48, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1008, 10),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== Co-Training ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lsup(logit_S1, logit_S2, labels_S1, labels_S2):\n",
    "    ce = nn.CrossEntropyLoss() \n",
    "    loss1 = ce(logit_S1, labels_S1)\n",
    "    loss2 = ce(logit_S2, labels_S2) \n",
    "    return (loss1+loss2)\n",
    "\n",
    "def Lcot(U_p1, U_p2):\n",
    "# the Jensen-Shannon divergence between p1(x) and p2(x)\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    U_batch_size = U_p1.size()[0]\n",
    "    \n",
    "    a1 = 0.5 * (S(U_p1) + S(U_p2))\n",
    "    loss1 = a1 * torch.log(a1)\n",
    "    loss1 = -torch.sum(loss1)\n",
    "    loss2 = S(U_p1) * LS(U_p1)\n",
    "    loss2 = -torch.sum(loss2)\n",
    "    loss3 = S(U_p2) * LS(U_p2)\n",
    "    loss3 = -torch.sum(loss3)\n",
    "\n",
    "    return (loss1 - 0.5 * (loss2 + loss3))/U_batch_size\n",
    "\n",
    "\n",
    "def Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2):\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    batch_size = logit_S1.size()[0] + logit_U1.size()[0]\n",
    "    \n",
    "    \n",
    "    a = S(logit_S2) * LS(perturbed_logit_S1)\n",
    "    a = torch.sum(a)\n",
    "\n",
    "    b = S(logit_S1) * LS(perturbed_logit_S2)\n",
    "    b = torch.sum(b)\n",
    "\n",
    "    c = S(logit_U2) * LS(perturbed_logit_U1)\n",
    "    c = torch.sum(c)\n",
    "\n",
    "    d = S(logit_U1) * LS(perturbed_logit_U2)\n",
    "    d = torch.sum(d)\n",
    "\n",
    "    return -(a+b+c+d)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lamda(epoch):\n",
    "    epoch = epoch + 1\n",
    "    global lambda_cot\n",
    "    global lambda_diff\n",
    "    if epoch <= 80:\n",
    "        lambda_cot = lambda_cot_max*np.exp(-5*(1-epoch/80)**2)\n",
    "        lambda_diff = lambda_diff_max*np.exp(-5*(1-epoch/80)**2)\n",
    "    else: \n",
    "        lambda_cot = lambda_cot_max\n",
    "        lambda_diff = lambda_diff_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = cnn\n",
    "m1 = model_func()\n",
    "m2 = model_func()\n",
    "\n",
    "m1.cuda()\n",
    "m2.cuda()\n",
    "\n",
    "multi_gpu = True\n",
    "if multi_gpu:\n",
    "    m1 = nn.DataParallel(m1)\n",
    "    m2 = nn.DataParallel(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "\n",
    "dataset = DatasetManager(metadata_root, audio_root, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion_bce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# optimizer\n",
    "parameters = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = torch.optim.SGD(\n",
    "    parameters,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    "    lr=0.05\n",
    ")\n",
    "\n",
    "# Augmentation to use\n",
    "# ps1 = sa.PitchShift(0.5, DatasetManager.SR, (-2, 3))\n",
    "# n = sa.Noise(0.5, (0.05, 0.2))\n",
    "augments = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_gen_1 = GradientSignAttack( \n",
    "                m1,\n",
    "                loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "                eps=0.02, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    "            )\n",
    "\n",
    "adv_gen_2 = GradientSignAttack( \n",
    "                m2,\n",
    "                loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "                eps=0.02, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_cot_max = 10\n",
    "lambda_diff_max = 0.5\n",
    "lambda_cot = 0.0\n",
    "lambda_diff = 0.0\n",
    "best_acc = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 837/837 [00:04<00:00, 189.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "ratio = 0.1\n",
    "batch_size = 100\n",
    "nb_epoch = 600\n",
    "\n",
    "# prepare the sampler with the specified ratio of supervised fime and a specific batch size\n",
    "nb_train_file = len(dataset.audio[\"train\"])\n",
    "nb_s_file = int(nb_train_file * ratio)   # theorical number of supervised file\n",
    "nb_s_file = nb_s_file - (nb_s_file % DatasetManager.NB_CLASS)  # need to be a multiple of number of class\n",
    "nb_u_file = nb_train_file - nb_s_file\n",
    "sampler = CoTrainingSampler(batch_size, nb_s_file, nb_u_file, nb_view=2, ratio=None, method=\"duplicate\")\n",
    "\n",
    "# create the generator and the loader\n",
    "generator = CoTrainingGenerator(dataset, sampler, augments=augments)\n",
    "train_loader = data.DataLoader(generator, batch_sampler=sampler)\n",
    "\n",
    "# val loader\n",
    "x, y = generator.validation\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)\n",
    "val_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# scheduler\n",
    "lr_lambda = lambda epoch: 0.5 * (np.cos(np.pi * epoch / nb_epoch) + 1)\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "callbacks = [lr_scheduler]\n",
    "# callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"%s_cnn_Cosd-lr_sgd-0.05lr-wd0.0001-m0.9_%de_noaugment\" % ( get_datetime(), nb_epoch )\n",
    "tensorboard = SummaryWriter(log_dir=\"cotraining/%s\" % title, comment=model_func.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 77/78 [00:00<00:00, 126750.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693 693\n",
      "693 693\n",
      "6930 6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "S1, S2, U = [], [], []\n",
    "for batch in tqdm.tqdm(sampler):\n",
    "    s1, s2, u = batch[0][0], batch[0][1], batch[0][2]\n",
    "    S1.extend(s1)\n",
    "    S2.extend(s2)\n",
    "    U.extend(u)\n",
    "    \n",
    "print(len(S1), len(np.unique(S1)))\n",
    "print(len(S2), len(np.unique(S2)))\n",
    "print(len(U), len(np.unique(U)))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1_func = CategoricalAccuracy()\n",
    "acc2_func = CategoricalAccuracy()\n",
    "   \n",
    "def SU_train(epoch, train_loader):\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    adjust_lamda(epoch)\n",
    "    \n",
    "    acc1_func.reset()\n",
    "    acc2_func.reset()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        # output add one extra dimension (1, B, ...) instead of (B, ...)\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "        \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        # move to GPU\n",
    "        X_S = [xs.cuda().float() for xs in X_S]\n",
    "        y_S = [ys.cuda().long() for ys in y_S]\n",
    "        X_U = X_U.cuda().float()\n",
    "        y_U = y_U.cuda().long()\n",
    "        \n",
    "        # Prediction\n",
    "        logits_S1 = m1(X_S[0])\n",
    "        logits_S2 = m2(X_S[1])\n",
    "        logits_U1 = m1(X_U)\n",
    "        logits_U2 = m2(X_U)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        # TODO pseudo labels ? how many class ?\n",
    "        _, pred_U1 = torch.max(logits_U1, 1)\n",
    "        _, pred_U2 = torch.max(logits_U2, 1)\n",
    "\n",
    "        # fix batchnorm & dropout\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "        \n",
    "        #generate adversarial examples\n",
    "        # Multi-target doesn't work with advertorch. Using the pseudo label of the prediction\n",
    "        perturbed_data_S1 = adv_gen_1.perturb(X_S[0], y_S[0])\n",
    "        perturbed_data_S2 = adv_gen_2.perturb(X_S[1], y_S[1])\n",
    "        \n",
    "        perturbed_data_U1 = adv_gen_1.perturb(X_U, pred_U1)\n",
    "        perturbed_data_U2 = adv_gen_2.perturb(X_U, pred_U2)\n",
    "        \n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict using adversarial samples\n",
    "        perturbed_logits_S1 = m1(perturbed_data_S2)\n",
    "        perturbed_logits_S2 = m2(perturbed_data_S1)\n",
    "\n",
    "        perturbed_logits_U1 = m1(perturbed_data_U2)\n",
    "        perturbed_logits_U2 = m2(perturbed_data_U1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "        \n",
    "        Loss_sup = Lsup(logits_S1, logits_S2, y_S[0], y_S[1])\n",
    "        Loss_cot = Lcot(logits_U1, logits_U2)\n",
    "        Loss_diff = Ldiff(\n",
    "            logits_S1, logits_S2, perturbed_logits_S1, perturbed_logits_S2,\n",
    "            logits_U1, logits_U2, perturbed_logits_U1, perturbed_logits_U2\n",
    "        )\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc the metrics\n",
    "#         pred_S1 = torch.softmax(logits_S1, dim=1)\n",
    "#         pred_S2 = torch.softmax(logits_S2, dim=1)\n",
    "        _, pred_S1 = torch.max(pred_S1, 1)\n",
    "        _, pred_S2 = torch.max(pred_S2, 1)\n",
    "        acc_1 = acc1_func(pred_S1, y_S[0])\n",
    "        acc_2 = acc2_func(pred_S2, y_S[1])\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        print(\"Epoch {:4}, {:3d}% \\t acc: {:3.4e} {:3.4e} - loss {:3.4e} {:3.4e} {:3.4e} {:3.4e} took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / sampler.nb_batch),\n",
    "            \n",
    "            acc_1, acc_2,\n",
    "            running_loss/(i+1), ls/(i+1), lc/(i+1), ld/(i+1), \n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "            \n",
    "    # using tensorboard to monitor loss and acc\n",
    "    tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_1, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_2, epoch )\n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    \n",
    "    acc1_func.reset()\n",
    "    acc2_func.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "\n",
    "            logits_S1 = m1(X)\n",
    "            logits_S2 = m2(X)\n",
    "\n",
    "            # calc the metrics\n",
    "#             pred_S1 = torch.softmax(logits_S1, dim=1)\n",
    "#             pred_S2 = torch.softmax(logits_S2, dim=1)\n",
    "            _, pred_S1 = torch.max(pred_S1, 1)\n",
    "            _, pred_S2 = torch.max(pred_S2, 1)\n",
    "            acc_1_val = acc1_func(pred_S1, y)\n",
    "            acc_2_val = acc2_func(pred_S2, y)\n",
    "            \n",
    "    tensorboard.add_scalar('val/acc_1', acc_1_val, epoch)\n",
    "    tensorboard.add_scalar('val/acc_2', acc_2_val, epoch)\n",
    "    \n",
    "    for callback in callbacks:\n",
    "        callback.step()\n",
    "\n",
    "    print('\\nnet1 test acc: %.3e | net2 test acc: %.3e' % (acc_1_val, acc_2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1,  98% \t acc: 1.8470e-01 1.7605e-01 - loss 4.5511e+00 4.5268e+00 6.8578e-02 5.0059e+00 took: 51.70s\n",
      "net1 test acc: 1.053e-01 | net2 test acc: 1.109e-01\n",
      "Epoch    2,  98% \t acc: 1.1111e-01 1.2698e-01 - loss 4.6088e+00 4.5861e+00 2.4825e-02 4.7559e+00 took: 45.32s\n",
      "net1 test acc: 1.209e-01 | net2 test acc: 1.804e-01\n",
      "Epoch    3,  98% \t acc: 8.6580e-02 1.3709e-01 - loss 4.5852e+00 4.5592e+00 2.9029e-02 4.7562e+00 took: 44.31s\n",
      "net1 test acc: 6.568e-02 | net2 test acc: 2.189e-01\n",
      "Epoch    4,  98% \t acc: 1.2554e-01 1.5440e-01 - loss 4.5174e+00 4.4871e+00 3.6703e-02 4.7839e+00 took: 44.37s\n",
      "net1 test acc: 1.787e-01 | net2 test acc: 1.052e-01\n",
      "Epoch    5,  98% \t acc: 1.5296e-01 1.8903e-01 - loss 4.4310e+00 4.3940e+00 5.3192e-02 4.9257e+00 took: 44.35s\n",
      "net1 test acc: 1.825e-01 | net2 test acc: 2.411e-01\n",
      "Epoch    6,  98% \t acc: 1.7605e-01 2.0202e-01 - loss 4.3261e+00 4.2866e+00 5.0039e-02 4.6991e+00 took: 47.37s\n",
      "net1 test acc: 1.925e-01 | net2 test acc: 2.990e-01\n",
      "Epoch    7,  98% \t acc: 1.8759e-01 2.2511e-01 - loss 4.3326e+00 4.2861e+00 6.1586e-02 4.7547e+00 took: 45.08s\n",
      "net1 test acc: 2.194e-01 | net2 test acc: 2.121e-01\n",
      "Epoch    8,  98% \t acc: 1.7027e-01 2.4098e-01 - loss 4.2170e+00 4.1665e+00 5.6969e-02 4.6545e+00 took: 47.93s\n",
      "net1 test acc: 1.991e-01 | net2 test acc: 2.813e-01\n",
      "Epoch    9,  98% \t acc: 1.6306e-01 2.4387e-01 - loss 4.2083e+00 4.1499e+00 6.3844e-02 4.7227e+00 took: 48.07s\n",
      "net1 test acc: 1.961e-01 | net2 test acc: 1.826e-01\n",
      "Epoch   10,  98% \t acc: 1.7460e-01 2.3810e-01 - loss 4.2512e+00 4.1874e+00 5.8751e-02 4.6885e+00 took: 44.50s\n",
      "net1 test acc: 1.918e-01 | net2 test acc: 2.332e-01\n",
      "Epoch   11,  98% \t acc: 1.7027e-01 2.4098e-01 - loss 4.3345e+00 4.2571e+00 7.5473e-02 4.8752e+00 took: 45.75s\n",
      "net1 test acc: 2.186e-01 | net2 test acc: 2.423e-01\n",
      "Epoch   12,  98% \t acc: 1.6595e-01 2.5685e-01 - loss 4.2695e+00 4.1866e+00 6.6291e-02 4.8166e+00 took: 44.50s\n",
      "net1 test acc: 1.920e-01 | net2 test acc: 2.165e-01\n",
      "Epoch   13,  98% \t acc: 1.7893e-01 2.7994e-01 - loss 4.2300e+00 4.1438e+00 5.5492e-02 4.6412e+00 took: 47.76s\n",
      "net1 test acc: 1.883e-01 | net2 test acc: 2.058e-01\n",
      "Epoch   14,  98% \t acc: 1.6739e-01 2.3954e-01 - loss 4.2403e+00 4.1426e+00 5.6479e-02 4.7423e+00 took: 48.54s\n",
      "net1 test acc: 1.914e-01 | net2 test acc: 2.208e-01\n",
      "Epoch   15,  98% \t acc: 1.6306e-01 2.5541e-01 - loss 4.2407e+00 4.1319e+00 5.8785e-02 4.7317e+00 took: 44.10s\n",
      "net1 test acc: 2.117e-01 | net2 test acc: 7.934e-02\n",
      "Epoch   16,  98% \t acc: 1.8182e-01 2.8571e-01 - loss 4.1341e+00 4.0128e+00 6.2878e-02 4.6955e+00 took: 47.91s\n",
      "net1 test acc: 1.629e-01 | net2 test acc: 2.835e-01\n",
      "Epoch   17,  98% \t acc: 1.7316e-01 2.4531e-01 - loss 4.3117e+00 4.1621e+00 8.2180e-02 5.0001e+00 took: 44.32s\n",
      "net1 test acc: 2.230e-01 | net2 test acc: 1.317e-01\n",
      "Epoch   18,  98% \t acc: 1.7172e-01 1.9769e-01 - loss 4.3866e+00 4.2219e+00 7.6534e-02 5.1059e+00 took: 44.44s\n",
      "net1 test acc: 2.196e-01 | net2 test acc: 1.320e-01\n",
      "Epoch   19,  98% \t acc: 1.7172e-01 2.0346e-01 - loss 4.3957e+00 4.2114e+00 8.0056e-02 5.1435e+00 took: 44.28s\n",
      "net1 test acc: 2.162e-01 | net2 test acc: 1.235e-01\n",
      "Epoch   20,  98% \t acc: 1.7316e-01 2.4964e-01 - loss 4.3082e+00 4.1050e+00 8.2342e-02 5.1203e+00 took: 44.74s\n",
      "net1 test acc: 2.088e-01 | net2 test acc: 1.795e-01\n",
      "Epoch   21,  98% \t acc: 1.7605e-01 2.7417e-01 - loss 4.3422e+00 4.1212e+00 8.1404e-02 5.0785e+00 took: 47.52s\n",
      "net1 test acc: 1.899e-01 | net2 test acc: 1.280e-01\n",
      "Epoch   22,  98% \t acc: 1.8615e-01 3.0736e-01 - loss 4.2633e+00 4.0137e+00 8.8722e-02 5.1390e+00 took: 48.40s\n",
      "net1 test acc: 2.066e-01 | net2 test acc: 2.029e-01\n",
      "Epoch   23,  98% \t acc: 1.8326e-01 3.0447e-01 - loss 4.2833e+00 4.0202e+00 8.3040e-02 4.9991e+00 took: 47.58s\n",
      "net1 test acc: 1.958e-01 | net2 test acc: 2.074e-01\n",
      "Epoch   24,  98% \t acc: 1.9048e-01 2.8571e-01 - loss 4.3334e+00 4.0374e+00 8.9141e-02 5.0781e+00 took: 47.68s\n",
      "net1 test acc: 1.802e-01 | net2 test acc: 8.012e-02\n",
      "Epoch   25,  98% \t acc: 1.9048e-01 2.5397e-01 - loss 4.3990e+00 4.0821e+00 8.5372e-02 5.0280e+00 took: 44.32s\n",
      "net1 test acc: 1.939e-01 | net2 test acc: 1.566e-01\n",
      "Epoch   26,  98% \t acc: 1.7316e-01 2.2944e-01 - loss 4.4875e+00 4.1586e+00 7.2339e-02 4.9737e+00 took: 47.85s\n",
      "net1 test acc: 2.014e-01 | net2 test acc: 1.612e-01\n",
      "Epoch   27,  98% \t acc: 2.0635e-01 2.4675e-01 - loss 4.4197e+00 4.0484e+00 7.9500e-02 5.0756e+00 took: 44.95s\n",
      "net1 test acc: 2.076e-01 | net2 test acc: 1.480e-01\n",
      "Epoch   28,  98% \t acc: 1.8615e-01 2.9582e-01 - loss 4.3919e+00 3.9927e+00 7.8159e-02 5.0381e+00 took: 48.76s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.881e-01\n",
      "Epoch   29,  98% \t acc: 2.0635e-01 3.0880e-01 - loss 4.3849e+00 3.9449e+00 8.5591e-02 5.0029e+00 took: 47.61s\n",
      "net1 test acc: 2.130e-01 | net2 test acc: 1.888e-01\n",
      "Epoch   30,  98% \t acc: 1.8759e-01 3.2035e-01 - loss 4.4408e+00 3.9643e+00 8.4982e-02 5.0208e+00 took: 46.38s\n",
      "net1 test acc: 1.896e-01 | net2 test acc: 1.787e-01\n",
      "Epoch   31,  98% \t acc: 2.5397e-01 2.9004e-01 - loss 4.4349e+00 3.9318e+00 8.0423e-02 4.9572e+00 took: 47.05s\n",
      "net1 test acc: 2.077e-01 | net2 test acc: 1.168e-01\n",
      "Epoch   32,  98% \t acc: 2.1356e-01 2.5541e-01 - loss 4.5662e+00 4.0428e+00 6.9447e-02 4.9438e+00 took: 51.90s\n",
      "net1 test acc: 2.246e-01 | net2 test acc: 1.780e-01\n",
      "Epoch   33,  98% \t acc: 2.5974e-01 2.5397e-01 - loss 4.5684e+00 4.0287e+00 6.0395e-02 4.8551e+00 took: 45.84s\n",
      "net1 test acc: 1.932e-01 | net2 test acc: 1.836e-01\n",
      "Epoch   34,  98% \t acc: 2.2078e-01 2.4675e-01 - loss 4.6523e+00 4.0697e+00 6.0798e-02 4.8700e+00 took: 46.51s\n",
      "net1 test acc: 1.910e-01 | net2 test acc: 1.214e-01\n",
      "Epoch   35,  98% \t acc: 2.6551e-01 2.4675e-01 - loss 4.6354e+00 4.0400e+00 5.0292e-02 4.7880e+00 took: 46.33s\n",
      "net1 test acc: 2.200e-01 | net2 test acc: 1.909e-01\n",
      "Epoch   36,  98% \t acc: 2.3810e-01 2.4387e-01 - loss 4.6361e+00 3.9912e+00 5.3021e-02 4.7931e+00 took: 44.23s\n",
      "net1 test acc: 2.225e-01 | net2 test acc: 1.844e-01\n",
      "Epoch   37,  98% \t acc: 2.6263e-01 2.8283e-01 - loss 4.5996e+00 3.9179e+00 5.3087e-02 4.7188e+00 took: 44.10s\n",
      "net1 test acc: 2.196e-01 | net2 test acc: 1.680e-01\n",
      "Epoch   38,  98% \t acc: 2.6118e-01 2.5253e-01 - loss 4.7153e+00 3.9722e+00 5.7156e-02 4.7528e+00 took: 44.62s\n",
      "net1 test acc: 1.821e-01 | net2 test acc: 1.447e-01\n",
      "Epoch   39,  98% \t acc: 2.3232e-01 2.4820e-01 - loss 4.7884e+00 4.0173e+00 5.2315e-02 4.6878e+00 took: 43.97s\n",
      "net1 test acc: 1.838e-01 | net2 test acc: 2.131e-01\n",
      "Epoch   40,  98% \t acc: 2.6984e-01 3.0447e-01 - loss 4.4921e+00 3.7421e+00 4.2105e-02 4.3937e+00 took: 44.22s\n",
      "net1 test acc: 2.062e-01 | net2 test acc: 2.085e-01\n",
      "Epoch   41,  98% \t acc: 2.6551e-01 2.7706e-01 - loss 4.6755e+00 3.8369e+00 5.1676e-02 4.4698e+00 took: 44.72s\n",
      "net1 test acc: 2.346e-01 | net2 test acc: 2.519e-01\n",
      "Epoch   42,  98% \t acc: 2.6696e-01 2.9004e-01 - loss 4.6588e+00 3.8133e+00 3.9771e-02 4.4292e+00 took: 44.68s\n",
      "net1 test acc: 1.780e-01 | net2 test acc: 1.859e-01\n",
      "Epoch   43,  98% \t acc: 2.5541e-01 3.0159e-01 - loss 4.6614e+00 3.7843e+00 3.6286e-02 4.3861e+00 took: 46.27s\n",
      "net1 test acc: 2.237e-01 | net2 test acc: 1.647e-01\n",
      "Epoch   44,  98% \t acc: 2.3521e-01 2.7273e-01 - loss 4.9090e+00 3.9716e+00 3.6471e-02 4.4304e+00 took: 44.71s\n",
      "net1 test acc: 2.364e-01 | net2 test acc: 1.774e-01\n",
      "Epoch   45,  98% \t acc: 2.3954e-01 2.9293e-01 - loss 4.8350e+00 3.8656e+00 3.3501e-02 4.3788e+00 took: 45.31s\n",
      "net1 test acc: 2.072e-01 | net2 test acc: 1.972e-01\n",
      "Epoch   46,  98% \t acc: 2.5397e-01 2.8716e-01 - loss 4.8541e+00 3.8196e+00 3.4058e-02 4.4238e+00 took: 46.47s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 1.850e-01\n",
      "Epoch   47,  98% \t acc: 2.6407e-01 2.7417e-01 - loss 4.9401e+00 3.8376e+00 3.8218e-02 4.3989e+00 took: 45.27s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 2.325e-01\n",
      "Epoch   48,  98% \t acc: 2.8716e-01 3.0592e-01 - loss 4.8846e+00 3.7443e+00 3.5670e-02 4.3621e+00 took: 44.31s\n",
      "net1 test acc: 2.383e-01 | net2 test acc: 1.660e-01\n",
      "Epoch   49,  98% \t acc: 2.7706e-01 2.8139e-01 - loss 4.8885e+00 3.7205e+00 3.2136e-02 4.3067e+00 took: 44.20s\n",
      "net1 test acc: 2.564e-01 | net2 test acc: 2.151e-01\n",
      "Epoch   50,  98% \t acc: 2.7994e-01 3.0736e-01 - loss 4.9480e+00 3.7155e+00 3.3528e-02 4.3088e+00 took: 44.12s\n",
      "net1 test acc: 2.296e-01 | net2 test acc: 1.999e-01\n",
      "Epoch   51,  98% \t acc: 2.7561e-01 2.8139e-01 - loss 5.1154e+00 3.8120e+00 3.4056e-02 4.3475e+00 took: 44.44s\n",
      "net1 test acc: 2.218e-01 | net2 test acc: 2.085e-01\n",
      "Epoch   52,  98% \t acc: 2.8716e-01 3.0303e-01 - loss 5.1020e+00 3.7432e+00 3.3236e-02 4.3494e+00 took: 45.79s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 2.362e-01\n",
      "Epoch   53,  98% \t acc: 2.3665e-01 2.6407e-01 - loss 5.2959e+00 3.8997e+00 2.9994e-02 4.3355e+00 took: 48.78s\n",
      "net1 test acc: 2.311e-01 | net2 test acc: 3.198e-01\n",
      "Epoch   54,  98% \t acc: 2.7273e-01 3.0880e-01 - loss 5.1696e+00 3.7634e+00 2.4384e-02 4.2813e+00 took: 45.21s\n",
      "net1 test acc: 2.221e-01 | net2 test acc: 1.771e-01\n",
      "Epoch   55,  98% \t acc: 2.6118e-01 2.7850e-01 - loss 5.2278e+00 3.7801e+00 2.3494e-02 4.2484e+00 took: 44.51s\n",
      "net1 test acc: 2.326e-01 | net2 test acc: 2.004e-01\n",
      "Epoch   56,  98% \t acc: 2.5541e-01 2.7417e-01 - loss 5.3478e+00 3.8426e+00 2.4216e-02 4.2367e+00 took: 44.55s\n",
      "net1 test acc: 2.295e-01 | net2 test acc: 2.083e-01\n",
      "Epoch   57,  98% \t acc: 2.5830e-01 2.8860e-01 - loss 5.3817e+00 3.8010e+00 2.5874e-02 4.2619e+00 took: 47.51s\n",
      "net1 test acc: 2.437e-01 | net2 test acc: 2.006e-01\n",
      "Epoch   58,  98% \t acc: 2.5830e-01 2.4675e-01 - loss 5.5575e+00 3.9133e+00 2.4608e-02 4.3076e+00 took: 47.90s\n",
      "net1 test acc: 2.226e-01 | net2 test acc: 1.469e-01\n",
      "Epoch   59,  98% \t acc: 2.5830e-01 2.9149e-01 - loss 5.5127e+00 3.7668e+00 2.9716e-02 4.3338e+00 took: 48.36s\n",
      "net1 test acc: 2.311e-01 | net2 test acc: 2.857e-01\n",
      "Epoch   60,  98% \t acc: 2.7417e-01 2.5974e-01 - loss 5.5658e+00 3.8138e+00 2.5465e-02 4.2799e+00 took: 45.02s\n",
      "net1 test acc: 2.567e-01 | net2 test acc: 1.948e-01\n",
      "Epoch   61,  98% \t acc: 3.0159e-01 2.8139e-01 - loss 5.4688e+00 3.6575e+00 2.8162e-02 4.2397e+00 took: 47.89s\n",
      "net1 test acc: 2.470e-01 | net2 test acc: 1.869e-01\n",
      "Epoch   62,  98% \t acc: 3.1313e-01 2.8427e-01 - loss 5.5324e+00 3.7172e+00 2.5086e-02 4.1744e+00 took: 48.52s\n",
      "net1 test acc: 2.195e-01 | net2 test acc: 1.839e-01\n",
      "Epoch   63,  98% \t acc: 2.6696e-01 2.8427e-01 - loss 5.6555e+00 3.7777e+00 2.6629e-02 4.1743e+00 took: 50.71s\n",
      "net1 test acc: 1.762e-01 | net2 test acc: 1.804e-01\n",
      "Epoch   64,  98% \t acc: 2.7994e-01 2.7561e-01 - loss 5.6182e+00 3.7366e+00 2.3001e-02 4.1365e+00 took: 45.23s\n",
      "net1 test acc: 2.290e-01 | net2 test acc: 1.908e-01\n",
      "Epoch   65,  98% \t acc: 2.5253e-01 2.9437e-01 - loss 5.7198e+00 3.7974e+00 2.2695e-02 4.1299e+00 took: 48.62s\n",
      "net1 test acc: 2.418e-01 | net2 test acc: 1.818e-01\n",
      "Epoch   66,  98% \t acc: 2.5397e-01 2.5685e-01 - loss 5.8256e+00 3.8326e+00 2.4224e-02 4.1611e+00 took: 45.41s\n",
      "net1 test acc: 2.394e-01 | net2 test acc: 1.758e-01\n",
      "Epoch   67,  98% \t acc: 2.6263e-01 2.7706e-01 - loss 5.7939e+00 3.7720e+00 2.3420e-02 4.1460e+00 took: 44.48s\n",
      "net1 test acc: 2.277e-01 | net2 test acc: 2.169e-01\n",
      "Epoch   68,  98% \t acc: 2.7994e-01 2.7706e-01 - loss 5.8398e+00 3.7590e+00 2.4403e-02 4.1690e+00 took: 45.18s\n",
      "net1 test acc: 2.266e-01 | net2 test acc: 1.769e-01\n",
      "Epoch   69,  98% \t acc: 2.6551e-01 2.5830e-01 - loss 5.9987e+00 3.8485e+00 2.3588e-02 4.2550e+00 took: 45.49s\n",
      "net1 test acc: 2.269e-01 | net2 test acc: 2.302e-01\n",
      "Epoch   70,  98% \t acc: 2.4964e-01 2.5830e-01 - loss 5.9537e+00 3.7969e+00 2.1577e-02 4.2327e+00 took: 47.99s\n",
      "net1 test acc: 2.174e-01 | net2 test acc: 2.229e-01\n",
      "Epoch   71,  98% \t acc: 2.7850e-01 2.5685e-01 - loss 6.0024e+00 3.8285e+00 2.2392e-02 4.1840e+00 took: 48.76s\n",
      "net1 test acc: 2.225e-01 | net2 test acc: 2.025e-01\n",
      "Epoch   72,  98% \t acc: 2.6263e-01 2.7417e-01 - loss 5.9393e+00 3.7912e+00 1.9128e-02 4.1338e+00 took: 48.30s\n",
      "net1 test acc: 2.366e-01 | net2 test acc: 1.939e-01\n",
      "Epoch   73,  98% \t acc: 2.6407e-01 2.9149e-01 - loss 5.9065e+00 3.7188e+00 2.0707e-02 4.1320e+00 took: 47.99s\n",
      "net1 test acc: 2.336e-01 | net2 test acc: 1.709e-01\n",
      "Epoch   74,  98% \t acc: 2.9004e-01 2.4531e-01 - loss 5.9914e+00 3.7797e+00 2.0329e-02 4.1430e+00 took: 48.16s\n",
      "net1 test acc: 2.355e-01 | net2 test acc: 2.370e-01\n",
      "Epoch   75,  98% \t acc: 2.7850e-01 2.4820e-01 - loss 6.0634e+00 3.7881e+00 2.3345e-02 4.1734e+00 took: 47.31s\n",
      "net1 test acc: 2.437e-01 | net2 test acc: 1.855e-01\n",
      "Epoch   76,  98% \t acc: 2.5685e-01 2.4820e-01 - loss 6.0629e+00 3.8084e+00 2.2093e-02 4.1240e+00 took: 48.32s\n",
      "net1 test acc: 1.975e-01 | net2 test acc: 1.792e-01\n",
      "Epoch   77,  98% \t acc: 2.7561e-01 2.7128e-01 - loss 5.9517e+00 3.7361e+00 1.9432e-02 4.0739e+00 took: 48.05s\n",
      "net1 test acc: 2.232e-01 | net2 test acc: 1.779e-01\n",
      "Epoch   78,  98% \t acc: 2.9149e-01 2.7561e-01 - loss 5.9478e+00 3.7138e+00 1.8593e-02 4.1101e+00 took: 48.11s\n",
      "net1 test acc: 2.144e-01 | net2 test acc: 2.121e-01\n",
      "Epoch   79,  98% \t acc: 2.4387e-01 2.5685e-01 - loss 6.1190e+00 3.8527e+00 1.9036e-02 4.1554e+00 took: 47.67s\n",
      "net1 test acc: 2.025e-01 | net2 test acc: 1.661e-01\n",
      "Epoch   80,  98% \t acc: 2.4531e-01 2.7273e-01 - loss 6.0667e+00 3.8296e+00 1.7414e-02 4.1259e+00 took: 48.18s\n",
      "net1 test acc: 2.244e-01 | net2 test acc: 1.606e-01\n",
      "Epoch   81,  98% \t acc: 2.3954e-01 2.5830e-01 - loss 6.0939e+00 3.8653e+00 1.6656e-02 4.1241e+00 took: 48.10s\n",
      "net1 test acc: 2.026e-01 | net2 test acc: 1.918e-01\n",
      "Epoch   82,  98% \t acc: 2.6263e-01 2.9437e-01 - loss 6.0820e+00 3.7893e+00 2.2658e-02 4.1324e+00 took: 48.02s\n",
      "net1 test acc: 2.362e-01 | net2 test acc: 2.004e-01\n",
      "Epoch   83,  98% \t acc: 2.5974e-01 2.7273e-01 - loss 6.1270e+00 3.8321e+00 2.2013e-02 4.1496e+00 took: 48.10s\n",
      "net1 test acc: 2.207e-01 | net2 test acc: 2.211e-01\n",
      "Epoch   84,  98% \t acc: 2.8283e-01 2.7850e-01 - loss 6.0635e+00 3.7759e+00 2.1362e-02 4.1480e+00 took: 47.83s\n",
      "net1 test acc: 2.377e-01 | net2 test acc: 1.861e-01\n",
      "Epoch   85,  98% \t acc: 2.7561e-01 2.6840e-01 - loss 6.0852e+00 3.8314e+00 1.8967e-02 4.1281e+00 took: 48.01s\n",
      "net1 test acc: 2.167e-01 | net2 test acc: 2.126e-01\n",
      "Epoch   86,  98% \t acc: 2.5253e-01 2.4242e-01 - loss 6.2958e+00 3.9970e+00 1.9729e-02 4.2031e+00 took: 47.89s\n",
      "net1 test acc: 2.446e-01 | net2 test acc: 2.259e-01\n",
      "Epoch   87,  98% \t acc: 2.8571e-01 2.7417e-01 - loss 5.9633e+00 3.6897e+00 1.8390e-02 4.1794e+00 took: 47.39s\n",
      "net1 test acc: 2.400e-01 | net2 test acc: 1.785e-01\n",
      "Epoch   88,  98% \t acc: 2.6407e-01 2.6696e-01 - loss 6.0893e+00 3.7639e+00 2.2619e-02 4.1983e+00 took: 47.17s\n",
      "net1 test acc: 2.242e-01 | net2 test acc: 1.787e-01\n",
      "Epoch   89,  98% \t acc: 2.8283e-01 2.7706e-01 - loss 6.0220e+00 3.7552e+00 1.8783e-02 4.1579e+00 took: 48.69s\n",
      "net1 test acc: 2.409e-01 | net2 test acc: 2.227e-01\n",
      "Epoch   90,  98% \t acc: 2.7561e-01 2.5974e-01 - loss 6.1000e+00 3.8107e+00 2.0924e-02 4.1600e+00 took: 48.13s\n",
      "net1 test acc: 2.241e-01 | net2 test acc: 2.230e-01\n",
      "Epoch   91,  98% \t acc: 2.7706e-01 2.6118e-01 - loss 5.9918e+00 3.7599e+00 1.7383e-02 4.1161e+00 took: 47.73s\n",
      "net1 test acc: 2.288e-01 | net2 test acc: 2.229e-01\n",
      "Epoch   92,  98% \t acc: 2.7850e-01 2.5541e-01 - loss 5.9971e+00 3.7193e+00 2.0568e-02 4.1443e+00 took: 47.90s\n",
      "net1 test acc: 2.302e-01 | net2 test acc: 1.672e-01\n",
      "Epoch   93,  98% \t acc: 2.7273e-01 2.7128e-01 - loss 5.9941e+00 3.7802e+00 1.6428e-02 4.0993e+00 took: 48.06s\n",
      "net1 test acc: 1.885e-01 | net2 test acc: 1.774e-01\n",
      "Epoch   94,  98% \t acc: 2.6407e-01 2.6407e-01 - loss 6.0975e+00 3.7803e+00 2.2891e-02 4.1765e+00 took: 48.01s\n",
      "net1 test acc: 2.121e-01 | net2 test acc: 1.783e-01\n",
      "Epoch   95,  98% \t acc: 2.8139e-01 2.8571e-01 - loss 5.9958e+00 3.7266e+00 2.0480e-02 4.1288e+00 took: 48.05s\n",
      "net1 test acc: 2.402e-01 | net2 test acc: 1.806e-01\n",
      "Epoch   96,  98% \t acc: 2.7128e-01 2.6551e-01 - loss 6.2105e+00 3.9128e+00 2.1594e-02 4.1635e+00 took: 48.52s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 2.044e-01\n",
      "Epoch   97,  98% \t acc: 2.4531e-01 2.3088e-01 - loss 6.2567e+00 3.9465e+00 2.1966e-02 4.1810e+00 took: 48.00s\n",
      "net1 test acc: 2.341e-01 | net2 test acc: 1.730e-01\n",
      "Epoch   98,  98% \t acc: 2.6984e-01 2.6984e-01 - loss 6.0484e+00 3.7535e+00 2.1179e-02 4.1663e+00 took: 48.24s\n",
      "net1 test acc: 1.921e-01 | net2 test acc: 1.850e-01\n",
      "Epoch   99,  98% \t acc: 2.9726e-01 2.6551e-01 - loss 5.9514e+00 3.6999e+00 1.9782e-02 4.1074e+00 took: 48.27s\n",
      "net1 test acc: 2.128e-01 | net2 test acc: 1.795e-01\n",
      "Epoch  100,  98% \t acc: 2.6118e-01 2.5397e-01 - loss 6.1197e+00 3.8487e+00 1.9747e-02 4.1471e+00 took: 44.97s\n",
      "net1 test acc: 2.464e-01 | net2 test acc: 2.300e-01\n",
      "Epoch  101,  98% \t acc: 2.5253e-01 2.6840e-01 - loss 6.0827e+00 3.8365e+00 1.7862e-02 4.1351e+00 took: 44.98s\n",
      "net1 test acc: 2.272e-01 | net2 test acc: 2.147e-01\n",
      "Epoch  102,  98% \t acc: 2.6407e-01 2.6551e-01 - loss 6.0086e+00 3.7402e+00 2.0650e-02 4.1237e+00 took: 45.39s\n",
      "net1 test acc: 1.907e-01 | net2 test acc: 1.766e-01\n",
      "Epoch  103,  98% \t acc: 2.7561e-01 2.6263e-01 - loss 5.9550e+00 3.7013e+00 2.0062e-02 4.1061e+00 took: 48.42s\n",
      "net1 test acc: 2.255e-01 | net2 test acc: 2.095e-01\n",
      "Epoch  104,  98% \t acc: 2.9582e-01 2.7706e-01 - loss 5.9657e+00 3.6965e+00 2.2246e-02 4.0935e+00 took: 46.44s\n",
      "net1 test acc: 2.345e-01 | net2 test acc: 1.758e-01\n",
      "Epoch  105,  98% \t acc: 2.5974e-01 2.8571e-01 - loss 6.0784e+00 3.7639e+00 2.3892e-02 4.1510e+00 took: 48.51s\n",
      "net1 test acc: 2.237e-01 | net2 test acc: 2.199e-01\n",
      "Epoch  106,  98% \t acc: 2.7128e-01 2.8139e-01 - loss 5.9375e+00 3.6809e+00 2.1858e-02 4.0760e+00 took: 48.05s\n",
      "net1 test acc: 2.280e-01 | net2 test acc: 2.291e-01\n",
      "Epoch  107,  98% \t acc: 2.7128e-01 2.8283e-01 - loss 6.0088e+00 3.7438e+00 2.0820e-02 4.1136e+00 took: 47.60s\n",
      "net1 test acc: 1.423e-01 | net2 test acc: 1.879e-01\n",
      "Epoch  108,  98% \t acc: 2.9437e-01 2.5830e-01 - loss 5.9609e+00 3.7180e+00 1.8197e-02 4.1219e+00 took: 47.82s\n",
      "net1 test acc: 2.386e-01 | net2 test acc: 1.967e-01\n",
      "Epoch  109,  98% \t acc: 2.6263e-01 2.8427e-01 - loss 5.9647e+00 3.6925e+00 2.1450e-02 4.1155e+00 took: 47.71s\n",
      "net1 test acc: 2.255e-01 | net2 test acc: 1.802e-01\n",
      "Epoch  110,  98% \t acc: 2.8716e-01 2.5253e-01 - loss 5.9766e+00 3.6788e+00 2.2495e-02 4.1457e+00 took: 47.23s\n",
      "net1 test acc: 2.281e-01 | net2 test acc: 2.370e-01\n",
      "Epoch  111,  98% \t acc: 2.9004e-01 2.9437e-01 - loss 5.9141e+00 3.6891e+00 1.8943e-02 4.0711e+00 took: 48.12s\n",
      "net1 test acc: 2.308e-01 | net2 test acc: 2.197e-01\n",
      "Epoch  112,  98% \t acc: 2.7850e-01 2.8427e-01 - loss 5.9420e+00 3.7090e+00 2.0206e-02 4.0617e+00 took: 46.36s\n",
      "net1 test acc: 2.236e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  113,  98% \t acc: 2.9149e-01 2.8571e-01 - loss 5.8387e+00 3.6263e+00 1.9140e-02 4.0420e+00 took: 46.25s\n",
      "net1 test acc: 2.304e-01 | net2 test acc: 1.628e-01\n",
      "Epoch  114,  98% \t acc: 2.9870e-01 2.6263e-01 - loss 5.9226e+00 3.7004e+00 1.9514e-02 4.0542e+00 took: 44.26s\n",
      "net1 test acc: 2.326e-01 | net2 test acc: 1.761e-01\n",
      "Epoch  115,  98% \t acc: 2.8139e-01 2.6118e-01 - loss 5.9557e+00 3.7435e+00 1.7801e-02 4.0684e+00 took: 44.51s\n",
      "net1 test acc: 1.471e-01 | net2 test acc: 1.582e-01\n",
      "Epoch  116,  98% \t acc: 2.9726e-01 2.6696e-01 - loss 5.8058e+00 3.6495e+00 1.4110e-02 4.0303e+00 took: 44.05s\n",
      "net1 test acc: 2.356e-01 | net2 test acc: 1.900e-01\n",
      "Epoch  117,  98% \t acc: 2.9870e-01 2.8716e-01 - loss 5.8751e+00 3.6358e+00 1.9626e-02 4.0861e+00 took: 47.10s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 1.742e-01\n",
      "Epoch  118,  98% \t acc: 3.1025e-01 3.1025e-01 - loss 5.7185e+00 3.5160e+00 1.7386e-02 4.0573e+00 took: 45.65s\n",
      "net1 test acc: 2.315e-01 | net2 test acc: 1.744e-01\n",
      "Epoch  119,  98% \t acc: 2.8427e-01 2.8427e-01 - loss 5.8966e+00 3.7014e+00 1.7525e-02 4.0399e+00 took: 44.31s\n",
      "net1 test acc: 2.161e-01 | net2 test acc: 1.707e-01\n",
      "Epoch  120,  98% \t acc: 2.8139e-01 2.9870e-01 - loss 5.9153e+00 3.6708e+00 2.1508e-02 4.0589e+00 took: 44.59s\n",
      "net1 test acc: 2.177e-01 | net2 test acc: 1.780e-01\n",
      "Epoch  121,  98% \t acc: 2.9293e-01 2.6263e-01 - loss 5.9625e+00 3.6717e+00 2.0637e-02 4.1688e+00 took: 44.60s\n",
      "net1 test acc: 2.210e-01 | net2 test acc: 2.188e-01\n",
      "Epoch  122,  98% \t acc: 2.9149e-01 2.9726e-01 - loss 5.8681e+00 3.6342e+00 1.9481e-02 4.0783e+00 took: 44.37s\n",
      "net1 test acc: 2.316e-01 | net2 test acc: 2.308e-01\n",
      "Epoch  123,  98% \t acc: 3.2468e-01 3.0736e-01 - loss 5.6522e+00 3.4642e+00 1.7634e-02 4.0234e+00 took: 43.85s\n",
      "net1 test acc: 2.017e-01 | net2 test acc: 2.161e-01\n",
      "Epoch  124,  98% \t acc: 3.0303e-01 2.9437e-01 - loss 5.8277e+00 3.6175e+00 1.8476e-02 4.0510e+00 took: 43.66s\n",
      "net1 test acc: 2.307e-01 | net2 test acc: 1.791e-01\n",
      "Epoch  125,  98% \t acc: 3.1746e-01 2.9870e-01 - loss 5.7773e+00 3.5759e+00 1.7812e-02 4.0466e+00 took: 44.26s\n",
      "net1 test acc: 2.207e-01 | net2 test acc: 1.531e-01\n",
      "Epoch  126,  98% \t acc: 3.0303e-01 2.9004e-01 - loss 5.9494e+00 3.6757e+00 2.2447e-02 4.0985e+00 took: 43.73s\n",
      "net1 test acc: 2.232e-01 | net2 test acc: 2.344e-01\n",
      "Epoch  127,  98% \t acc: 3.0014e-01 3.0880e-01 - loss 5.7697e+00 3.5579e+00 1.8527e-02 4.0530e+00 took: 43.77s\n",
      "net1 test acc: 2.281e-01 | net2 test acc: 2.459e-01\n",
      "Epoch  128,  98% \t acc: 2.8716e-01 2.9726e-01 - loss 5.8716e+00 3.5877e+00 2.4540e-02 4.0770e+00 took: 47.87s\n",
      "net1 test acc: 1.720e-01 | net2 test acc: 1.788e-01\n",
      "Epoch  129,  98% \t acc: 3.1313e-01 3.1602e-01 - loss 5.7454e+00 3.5446e+00 1.9418e-02 4.0132e+00 took: 47.63s\n",
      "net1 test acc: 2.274e-01 | net2 test acc: 2.199e-01\n",
      "Epoch  130,  98% \t acc: 3.0880e-01 3.2323e-01 - loss 5.7453e+00 3.5001e+00 2.2009e-02 4.0501e+00 took: 47.42s\n",
      "net1 test acc: 2.204e-01 | net2 test acc: 2.037e-01\n",
      "Epoch  131,  98% \t acc: 2.9726e-01 2.9004e-01 - loss 5.9864e+00 3.7162e+00 2.0542e-02 4.1296e+00 took: 47.85s\n",
      "net1 test acc: 2.286e-01 | net2 test acc: 1.848e-01\n",
      "Epoch  132,  98% \t acc: 3.1025e-01 3.1457e-01 - loss 5.7639e+00 3.5588e+00 1.7380e-02 4.0626e+00 took: 44.72s\n",
      "net1 test acc: 2.181e-01 | net2 test acc: 2.204e-01\n",
      "Epoch  133,  98% \t acc: 3.1890e-01 3.0159e-01 - loss 5.6847e+00 3.4926e+00 1.8037e-02 4.0237e+00 took: 48.93s\n",
      "net1 test acc: 2.200e-01 | net2 test acc: 2.237e-01\n",
      "Epoch  134,  98% \t acc: 3.3766e-01 3.2756e-01 - loss 5.6898e+00 3.4615e+00 2.0592e-02 4.0448e+00 took: 44.77s\n",
      "net1 test acc: 2.246e-01 | net2 test acc: 2.087e-01\n",
      "Epoch  135,  98% \t acc: 3.2323e-01 3.2323e-01 - loss 5.6387e+00 3.4990e+00 1.4253e-02 3.9942e+00 took: 45.42s\n",
      "net1 test acc: 2.264e-01 | net2 test acc: 1.759e-01\n",
      "Epoch  136,  98% \t acc: 3.3189e-01 3.2323e-01 - loss 5.5635e+00 3.3873e+00 1.8271e-02 3.9869e+00 took: 45.44s\n",
      "net1 test acc: 2.283e-01 | net2 test acc: 2.104e-01\n",
      "Epoch  137,  98% \t acc: 3.3622e-01 3.1890e-01 - loss 5.6098e+00 3.4441e+00 1.7601e-02 3.9794e+00 took: 46.47s\n",
      "net1 test acc: 2.255e-01 | net2 test acc: 1.679e-01\n",
      "Epoch  138,  98% \t acc: 3.1169e-01 2.9582e-01 - loss 5.7538e+00 3.5629e+00 1.8254e-02 4.0166e+00 took: 47.28s\n",
      "net1 test acc: 2.126e-01 | net2 test acc: 1.699e-01\n",
      "Epoch  139,  98% \t acc: 3.0303e-01 3.0736e-01 - loss 5.8200e+00 3.6350e+00 1.7827e-02 4.0135e+00 took: 45.67s\n",
      "net1 test acc: 2.262e-01 | net2 test acc: 2.415e-01\n",
      "Epoch  140,  98% \t acc: 3.2612e-01 2.8716e-01 - loss 5.8781e+00 3.6473e+00 2.1410e-02 4.0334e+00 took: 45.89s\n",
      "net1 test acc: 2.262e-01 | net2 test acc: 2.240e-01\n",
      "Epoch  141,  98% \t acc: 3.0736e-01 3.3045e-01 - loss 5.6806e+00 3.4795e+00 1.9426e-02 4.0136e+00 took: 44.99s\n",
      "net1 test acc: 2.285e-01 | net2 test acc: 2.285e-01\n",
      "Epoch  142,  98% \t acc: 3.2035e-01 3.2035e-01 - loss 5.6302e+00 3.4383e+00 1.9332e-02 3.9972e+00 took: 45.56s\n",
      "net1 test acc: 2.505e-01 | net2 test acc: 2.323e-01\n",
      "Epoch  143,  98% \t acc: 3.0592e-01 3.2900e-01 - loss 5.6882e+00 3.4906e+00 1.9772e-02 3.9997e+00 took: 45.49s\n",
      "net1 test acc: 1.501e-01 | net2 test acc: 1.579e-01\n",
      "Epoch  144,  98% \t acc: 3.3478e-01 3.5354e-01 - loss 5.6116e+00 3.4238e+00 2.1144e-02 3.9527e+00 took: 48.46s\n",
      "net1 test acc: 2.155e-01 | net2 test acc: 2.147e-01\n",
      "Epoch  145,  98% \t acc: 3.1025e-01 3.3189e-01 - loss 5.7858e+00 3.5564e+00 2.0907e-02 4.0407e+00 took: 45.64s\n",
      "net1 test acc: 2.541e-01 | net2 test acc: 2.389e-01\n",
      "Epoch  146,  98% \t acc: 2.8283e-01 3.2179e-01 - loss 5.7823e+00 3.5751e+00 1.9772e-02 4.0190e+00 took: 45.06s\n",
      "net1 test acc: 2.095e-01 | net2 test acc: 1.855e-01\n",
      "Epoch  147,  98% \t acc: 3.5354e-01 3.2179e-01 - loss 5.6337e+00 3.4268e+00 2.0560e-02 4.0026e+00 took: 45.14s\n",
      "net1 test acc: 2.289e-01 | net2 test acc: 1.669e-01\n",
      "Epoch  148,  98% \t acc: 3.1746e-01 3.2612e-01 - loss 5.7039e+00 3.4922e+00 2.1535e-02 3.9928e+00 took: 45.75s\n",
      "net1 test acc: 2.370e-01 | net2 test acc: 2.370e-01\n",
      "Epoch  149,  98% \t acc: 3.2900e-01 3.2612e-01 - loss 5.7516e+00 3.5135e+00 2.2144e-02 4.0333e+00 took: 48.11s\n",
      "net1 test acc: 2.460e-01 | net2 test acc: 2.297e-01\n",
      "Epoch  150,  98% \t acc: 3.0447e-01 3.3622e-01 - loss 5.6924e+00 3.4898e+00 2.0282e-02 3.9995e+00 took: 47.40s\n",
      "net1 test acc: 2.319e-01 | net2 test acc: 1.777e-01\n",
      "Epoch  151,  98% \t acc: 3.1313e-01 3.3911e-01 - loss 5.6607e+00 3.4904e+00 1.8189e-02 3.9768e+00 took: 47.76s\n",
      "net1 test acc: 2.136e-01 | net2 test acc: 1.787e-01\n",
      "Epoch  152,  98% \t acc: 3.2756e-01 3.5931e-01 - loss 5.5535e+00 3.4111e+00 1.7522e-02 3.9345e+00 took: 47.97s\n",
      "net1 test acc: 2.221e-01 | net2 test acc: 1.750e-01\n",
      "Epoch  153,  98% \t acc: 3.3622e-01 3.6508e-01 - loss 5.5520e+00 3.3703e+00 2.1147e-02 3.9404e+00 took: 47.92s\n",
      "net1 test acc: 2.237e-01 | net2 test acc: 2.434e-01\n",
      "Epoch  154,  98% \t acc: 3.5065e-01 3.4199e-01 - loss 5.5818e+00 3.4231e+00 1.9216e-02 3.9331e+00 took: 45.82s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 2.299e-01\n",
      "Epoch  155,  98% \t acc: 3.5498e-01 3.4343e-01 - loss 5.5235e+00 3.3641e+00 1.8189e-02 3.9549e+00 took: 48.02s\n",
      "net1 test acc: 2.294e-01 | net2 test acc: 1.775e-01\n",
      "Epoch  156,  98% \t acc: 3.3766e-01 3.4921e-01 - loss 5.5502e+00 3.4064e+00 1.7907e-02 3.9294e+00 took: 47.92s\n",
      "net1 test acc: 2.180e-01 | net2 test acc: 2.280e-01\n",
      "Epoch  157,  98% \t acc: 3.3622e-01 3.5498e-01 - loss 5.5454e+00 3.3841e+00 1.8285e-02 3.9570e+00 took: 47.95s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 2.191e-01\n",
      "Epoch  158,  98% \t acc: 3.3189e-01 3.3045e-01 - loss 5.8037e+00 3.5599e+00 2.3544e-02 4.0167e+00 took: 46.45s\n",
      "net1 test acc: 2.272e-01 | net2 test acc: 1.542e-01\n",
      "Epoch  159,  98% \t acc: 3.2900e-01 3.4343e-01 - loss 5.7841e+00 3.5687e+00 2.0384e-02 4.0230e+00 took: 45.47s\n",
      "net1 test acc: 2.345e-01 | net2 test acc: 2.144e-01\n",
      "Epoch  160,  98% \t acc: 3.3622e-01 3.3911e-01 - loss 5.7253e+00 3.5152e+00 1.9423e-02 4.0317e+00 took: 45.51s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 2.196e-01\n",
      "Epoch  161,  98% \t acc: 3.4343e-01 3.3766e-01 - loss 5.5274e+00 3.4167e+00 1.4816e-02 3.9250e+00 took: 47.64s\n",
      "net1 test acc: 2.128e-01 | net2 test acc: 2.269e-01\n",
      "Epoch  162,  98% \t acc: 3.4343e-01 3.4343e-01 - loss 5.6070e+00 3.4215e+00 2.0885e-02 3.9532e+00 took: 45.75s\n",
      "net1 test acc: 2.375e-01 | net2 test acc: 2.423e-01\n",
      "Epoch  163,  98% \t acc: 3.4199e-01 3.7085e-01 - loss 5.4873e+00 3.3432e+00 1.9369e-02 3.9010e+00 took: 48.08s\n",
      "net1 test acc: 2.371e-01 | net2 test acc: 2.430e-01\n",
      "Epoch  164,  98% \t acc: 3.5209e-01 3.5642e-01 - loss 5.5144e+00 3.3867e+00 1.7785e-02 3.8998e+00 took: 47.21s\n",
      "net1 test acc: 2.125e-01 | net2 test acc: 2.265e-01\n",
      "Epoch  165,  98% \t acc: 3.4776e-01 3.4343e-01 - loss 5.4719e+00 3.3416e+00 1.8487e-02 3.8909e+00 took: 48.05s\n",
      "net1 test acc: 2.125e-01 | net2 test acc: 2.106e-01\n",
      "Epoch  166,  98% \t acc: 3.5354e-01 3.4055e-01 - loss 5.4970e+00 3.3883e+00 1.7633e-02 3.8647e+00 took: 48.02s\n",
      "net1 test acc: 2.548e-01 | net2 test acc: 2.292e-01\n",
      "Epoch  167,  98% \t acc: 3.5065e-01 3.4488e-01 - loss 5.4869e+00 3.3602e+00 1.8371e-02 3.8859e+00 took: 48.56s\n",
      "net1 test acc: 2.250e-01 | net2 test acc: 2.139e-01\n",
      "Epoch  168,  98% \t acc: 3.4632e-01 3.4921e-01 - loss 5.4388e+00 3.3298e+00 1.7711e-02 3.8638e+00 took: 48.12s\n",
      "net1 test acc: 2.207e-01 | net2 test acc: 2.240e-01\n",
      "Epoch  169,  98% \t acc: 3.6075e-01 3.8095e-01 - loss 5.3547e+00 3.2624e+00 1.6793e-02 3.8487e+00 took: 47.78s\n",
      "net1 test acc: 1.777e-01 | net2 test acc: 1.772e-01\n",
      "Epoch  170,  98% \t acc: 3.5931e-01 3.6508e-01 - loss 5.3754e+00 3.2638e+00 1.7833e-02 3.8664e+00 took: 48.25s\n",
      "net1 test acc: 2.360e-01 | net2 test acc: 2.349e-01\n",
      "Epoch  171,  98% \t acc: 3.4343e-01 3.4921e-01 - loss 5.7135e+00 3.4764e+00 2.4036e-02 3.9934e+00 took: 46.12s\n",
      "net1 test acc: 2.535e-01 | net2 test acc: 2.237e-01\n",
      "Epoch  172,  98% \t acc: 3.3478e-01 3.4632e-01 - loss 5.6511e+00 3.4247e+00 2.3130e-02 3.9901e+00 took: 48.19s\n",
      "net1 test acc: 2.553e-01 | net2 test acc: 1.569e-01\n",
      "Epoch  173,  98% \t acc: 3.4921e-01 3.3045e-01 - loss 5.5239e+00 3.3899e+00 1.8515e-02 3.8978e+00 took: 43.84s\n",
      "net1 test acc: 2.306e-01 | net2 test acc: 2.480e-01\n",
      "Epoch  174,  98% \t acc: 3.2323e-01 3.5354e-01 - loss 5.5036e+00 3.3833e+00 1.7315e-02 3.8943e+00 took: 46.62s\n",
      "net1 test acc: 2.335e-01 | net2 test acc: 2.368e-01\n",
      "Epoch  175,  98% \t acc: 3.4632e-01 3.6219e-01 - loss 5.4640e+00 3.3194e+00 1.8910e-02 3.9109e+00 took: 46.18s\n",
      "net1 test acc: 2.527e-01 | net2 test acc: 2.405e-01\n",
      "Epoch  176,  98% \t acc: 3.6508e-01 3.5354e-01 - loss 5.4191e+00 3.2901e+00 1.9626e-02 3.8654e+00 took: 48.71s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 1.627e-01\n",
      "Epoch  177,  98% \t acc: 3.3189e-01 3.5354e-01 - loss 5.5151e+00 3.3853e+00 1.8154e-02 3.8966e+00 took: 48.46s\n",
      "net1 test acc: 1.993e-01 | net2 test acc: 2.174e-01\n",
      "Epoch  178,  98% \t acc: 3.2323e-01 3.6508e-01 - loss 5.5446e+00 3.3645e+00 2.2191e-02 3.9164e+00 took: 48.16s\n",
      "net1 test acc: 2.264e-01 | net2 test acc: 2.440e-01\n",
      "Epoch  179,  98% \t acc: 3.2612e-01 3.6508e-01 - loss 5.5459e+00 3.3633e+00 2.1300e-02 3.9392e+00 took: 47.79s\n",
      "net1 test acc: 2.144e-01 | net2 test acc: 2.172e-01\n",
      "Epoch  180,  98% \t acc: 3.4343e-01 3.5498e-01 - loss 5.5188e+00 3.3390e+00 2.1262e-02 3.9342e+00 took: 48.24s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 1.680e-01\n",
      "Epoch  181,  98% \t acc: 3.6219e-01 3.7951e-01 - loss 5.3759e+00 3.2762e+00 1.6977e-02 3.8599e+00 took: 46.54s\n",
      "net1 test acc: 2.077e-01 | net2 test acc: 2.207e-01\n",
      "Epoch  182,  98% \t acc: 3.6652e-01 3.7807e-01 - loss 5.4107e+00 3.2607e+00 2.0185e-02 3.8963e+00 took: 46.65s\n",
      "net1 test acc: 2.245e-01 | net2 test acc: 2.445e-01\n",
      "Epoch  183,  98% \t acc: 3.4776e-01 3.4632e-01 - loss 5.4380e+00 3.3641e+00 1.6230e-02 3.8232e+00 took: 48.28s\n",
      "net1 test acc: 2.316e-01 | net2 test acc: 2.283e-01\n",
      "Epoch  184,  98% \t acc: 3.5498e-01 3.8240e-01 - loss 5.3940e+00 3.2892e+00 1.7369e-02 3.8621e+00 took: 49.12s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 1.745e-01\n",
      "Epoch  185,  98% \t acc: 3.4632e-01 3.5931e-01 - loss 5.4761e+00 3.3346e+00 1.8697e-02 3.9091e+00 took: 48.20s\n",
      "net1 test acc: 2.217e-01 | net2 test acc: 2.087e-01\n",
      "Epoch  186,  98% \t acc: 3.4199e-01 3.8961e-01 - loss 5.4421e+00 3.2904e+00 2.0037e-02 3.9028e+00 took: 47.85s\n",
      "net1 test acc: 2.119e-01 | net2 test acc: 2.227e-01\n",
      "Epoch  187,  98% \t acc: 3.5065e-01 3.7951e-01 - loss 5.4772e+00 3.3450e+00 1.9907e-02 3.8663e+00 took: 47.82s\n",
      "net1 test acc: 2.378e-01 | net2 test acc: 2.192e-01\n",
      "Epoch  188,  98% \t acc: 3.3045e-01 3.8240e-01 - loss 5.4600e+00 3.3182e+00 1.9748e-02 3.8886e+00 took: 47.23s\n",
      "net1 test acc: 1.826e-01 | net2 test acc: 1.729e-01\n",
      "Epoch  189,  98% \t acc: 3.7374e-01 3.5498e-01 - loss 5.3700e+00 3.2811e+00 1.7117e-02 3.8353e+00 took: 45.18s\n",
      "net1 test acc: 2.180e-01 | net2 test acc: 1.750e-01\n",
      "Epoch  190,  98% \t acc: 3.7662e-01 3.7662e-01 - loss 5.3316e+00 3.2414e+00 1.6769e-02 3.8450e+00 took: 46.69s\n",
      "net1 test acc: 1.583e-01 | net2 test acc: 1.658e-01\n",
      "Epoch  191,  98% \t acc: 3.7229e-01 4.0260e-01 - loss 5.4160e+00 3.2693e+00 2.0264e-02 3.8882e+00 took: 45.40s\n",
      "net1 test acc: 2.348e-01 | net2 test acc: 2.245e-01\n",
      "Epoch  192,  98% \t acc: 3.8095e-01 4.0693e-01 - loss 5.2912e+00 3.2054e+00 1.9352e-02 3.7846e+00 took: 45.15s\n",
      "net1 test acc: 2.413e-01 | net2 test acc: 2.205e-01\n",
      "Epoch  193,  98% \t acc: 3.6941e-01 4.2136e-01 - loss 5.1719e+00 3.1172e+00 1.7376e-02 3.7619e+00 took: 45.63s\n",
      "net1 test acc: 2.371e-01 | net2 test acc: 2.327e-01\n",
      "Epoch  194,  98% \t acc: 3.8961e-01 4.1270e-01 - loss 5.1677e+00 3.1176e+00 1.8093e-02 3.7384e+00 took: 45.20s\n",
      "net1 test acc: 2.353e-01 | net2 test acc: 1.729e-01\n",
      "Epoch  195,  98% \t acc: 3.8240e-01 3.6941e-01 - loss 5.3491e+00 3.2670e+00 1.8594e-02 3.7923e+00 took: 47.23s\n",
      "net1 test acc: 2.332e-01 | net2 test acc: 2.138e-01\n",
      "Epoch  196,  98% \t acc: 3.8095e-01 4.0115e-01 - loss 5.3144e+00 3.2482e+00 1.8811e-02 3.7561e+00 took: 47.20s\n",
      "net1 test acc: 2.210e-01 | net2 test acc: 2.021e-01\n",
      "Epoch  197,  98% \t acc: 3.7518e-01 3.9250e-01 - loss 5.3339e+00 3.2685e+00 1.7630e-02 3.7783e+00 took: 49.06s\n",
      "net1 test acc: 2.268e-01 | net2 test acc: 2.246e-01\n",
      "Epoch  198,  98% \t acc: 3.9105e-01 3.7374e-01 - loss 5.2436e+00 3.1786e+00 1.7739e-02 3.7752e+00 took: 45.78s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.255e-01\n",
      "Epoch  199,  98% \t acc: 3.6219e-01 3.9250e-01 - loss 5.2867e+00 3.2197e+00 1.8742e-02 3.7591e+00 took: 48.43s\n",
      "net1 test acc: 2.149e-01 | net2 test acc: 2.145e-01\n",
      "Epoch  200,  98% \t acc: 3.9971e-01 4.0404e-01 - loss 5.2131e+00 3.1769e+00 1.6920e-02 3.7340e+00 took: 45.62s\n",
      "net1 test acc: 2.185e-01 | net2 test acc: 1.651e-01\n",
      "Epoch  201,  98% \t acc: 3.8961e-01 3.9538e-01 - loss 5.2357e+00 3.2096e+00 1.6814e-02 3.7158e+00 took: 47.21s\n",
      "net1 test acc: 2.020e-01 | net2 test acc: 2.214e-01\n",
      "Epoch  202,  98% \t acc: 3.9394e-01 3.9827e-01 - loss 5.1803e+00 3.1480e+00 1.7447e-02 3.7158e+00 took: 45.75s\n",
      "net1 test acc: 2.020e-01 | net2 test acc: 2.261e-01\n",
      "Epoch  203,  98% \t acc: 3.9250e-01 3.9827e-01 - loss 5.2911e+00 3.2303e+00 2.0120e-02 3.7191e+00 took: 46.67s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.095e-01\n",
      "Epoch  204,  98% \t acc: 3.8817e-01 4.0260e-01 - loss 5.1663e+00 3.1692e+00 1.7049e-02 3.6533e+00 took: 45.48s\n",
      "net1 test acc: 2.020e-01 | net2 test acc: 2.061e-01\n",
      "Epoch  205,  98% \t acc: 3.9538e-01 4.1270e-01 - loss 5.1319e+00 3.1381e+00 1.7111e-02 3.6454e+00 took: 47.04s\n",
      "net1 test acc: 2.308e-01 | net2 test acc: 2.260e-01\n",
      "Epoch  206,  98% \t acc: 3.8528e-01 4.1126e-01 - loss 5.1889e+00 3.1719e+00 1.8520e-02 3.6635e+00 took: 45.82s\n",
      "net1 test acc: 2.281e-01 | net2 test acc: 2.185e-01\n",
      "Epoch  207,  98% \t acc: 3.6941e-01 4.0548e-01 - loss 5.2084e+00 3.2202e+00 1.7879e-02 3.6188e+00 took: 46.04s\n",
      "net1 test acc: 2.367e-01 | net2 test acc: 2.259e-01\n",
      "Epoch  208,  98% \t acc: 4.0115e-01 3.6941e-01 - loss 5.3680e+00 3.2888e+00 2.2056e-02 3.7173e+00 took: 45.62s\n",
      "net1 test acc: 2.266e-01 | net2 test acc: 2.244e-01\n",
      "Epoch  209,  98% \t acc: 3.9971e-01 3.9250e-01 - loss 5.2640e+00 3.2369e+00 2.0078e-02 3.6527e+00 took: 45.31s\n",
      "net1 test acc: 2.158e-01 | net2 test acc: 2.132e-01\n",
      "Epoch  210,  98% \t acc: 3.7807e-01 4.0693e-01 - loss 5.1977e+00 3.1971e+00 1.7859e-02 3.6440e+00 took: 45.60s\n",
      "net1 test acc: 2.300e-01 | net2 test acc: 2.162e-01\n",
      "Epoch  211,  98% \t acc: 3.6364e-01 4.1847e-01 - loss 5.1900e+00 3.1748e+00 1.8689e-02 3.6566e+00 took: 45.38s\n",
      "net1 test acc: 2.012e-01 | net2 test acc: 1.680e-01\n",
      "Epoch  212,  98% \t acc: 3.8095e-01 4.1703e-01 - loss 5.1909e+00 3.1723e+00 1.8217e-02 3.6728e+00 took: 46.49s\n",
      "net1 test acc: 2.315e-01 | net2 test acc: 2.337e-01\n",
      "Epoch  213,  98% \t acc: 3.9250e-01 4.1126e-01 - loss 5.1690e+00 3.1715e+00 1.8872e-02 3.6176e+00 took: 45.25s\n",
      "net1 test acc: 2.126e-01 | net2 test acc: 2.200e-01\n",
      "Epoch  214,  98% \t acc: 3.7518e-01 4.2713e-01 - loss 5.1080e+00 3.1300e+00 1.7528e-02 3.6054e+00 took: 47.48s\n",
      "net1 test acc: 2.070e-01 | net2 test acc: 1.647e-01\n",
      "Epoch  215,  98% \t acc: 4.0260e-01 4.0260e-01 - loss 5.0492e+00 3.0842e+00 1.5802e-02 3.6139e+00 took: 51.80s\n",
      "net1 test acc: 2.021e-01 | net2 test acc: 2.174e-01\n",
      "Epoch  216,  98% \t acc: 3.8240e-01 4.1414e-01 - loss 5.1816e+00 3.1600e+00 1.9041e-02 3.6624e+00 took: 50.25s\n",
      "net1 test acc: 2.257e-01 | net2 test acc: 1.838e-01\n",
      "Epoch  217,  98% \t acc: 4.0693e-01 4.2280e-01 - loss 5.0907e+00 3.1265e+00 1.7023e-02 3.5879e+00 took: 47.70s\n",
      "net1 test acc: 2.319e-01 | net2 test acc: 2.405e-01\n",
      "Epoch  218,  98% \t acc: 4.0693e-01 4.1558e-01 - loss 5.1544e+00 3.1505e+00 2.0610e-02 3.5956e+00 took: 47.45s\n",
      "net1 test acc: 2.118e-01 | net2 test acc: 2.285e-01\n",
      "Epoch  219,  98% \t acc: 4.2280e-01 4.2713e-01 - loss 4.9934e+00 3.0784e+00 1.6833e-02 3.4934e+00 took: 48.35s\n",
      "net1 test acc: 2.032e-01 | net2 test acc: 2.132e-01\n",
      "Epoch  220,  98% \t acc: 4.3290e-01 4.1414e-01 - loss 4.9579e+00 3.0930e+00 1.6464e-02 3.4006e+00 took: 45.21s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 2.318e-01\n",
      "Epoch  221,  98% \t acc: 4.2136e-01 3.8961e-01 - loss 5.0737e+00 3.2266e+00 1.7537e-02 3.3434e+00 took: 48.45s\n",
      "net1 test acc: 2.055e-01 | net2 test acc: 2.162e-01\n",
      "Epoch  222,  98% \t acc: 3.6652e-01 4.1414e-01 - loss 5.1395e+00 3.2183e+00 1.9773e-02 3.4469e+00 took: 44.99s\n",
      "net1 test acc: 2.253e-01 | net2 test acc: 2.065e-01\n",
      "Epoch  223,  98% \t acc: 3.9971e-01 3.7807e-01 - loss 5.1628e+00 3.2599e+00 1.7421e-02 3.4574e+00 took: 44.62s\n",
      "net1 test acc: 2.254e-01 | net2 test acc: 2.265e-01\n",
      "Epoch  224,  98% \t acc: 4.0837e-01 4.1270e-01 - loss 5.0550e+00 3.1479e+00 1.7479e-02 3.4646e+00 took: 45.10s\n",
      "net1 test acc: 2.091e-01 | net2 test acc: 2.069e-01\n",
      "Epoch  225,  98% \t acc: 4.2280e-01 4.3001e-01 - loss 5.0223e+00 3.1525e+00 1.6317e-02 3.4134e+00 took: 44.76s\n",
      "net1 test acc: 2.145e-01 | net2 test acc: 2.175e-01\n",
      "Epoch  226,  98% \t acc: 4.3434e-01 4.1126e-01 - loss 4.9771e+00 3.1655e+00 1.5871e-02 3.3058e+00 took: 44.87s\n",
      "net1 test acc: 2.083e-01 | net2 test acc: 2.095e-01\n",
      "Epoch  227,  85% \t acc: 4.4279e-01 4.6103e-01 - loss 4.8503e+00 3.0349e+00 2.0973e-02 3.2114e+00 took: 38.78s\r"
     ]
    }
   ],
   "source": [
    "# Full Co-training\n",
    "nb_epoch = 600\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    SU_train(epoch, train_loader)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
