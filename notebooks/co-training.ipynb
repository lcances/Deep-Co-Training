{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import random\n",
    "from random import shuffle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.nn.utils import weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from datasetManager import DatasetManager\n",
    "from generators import Dataset, CoTrainingDataset\n",
    "from samplers import CoTrainingSampler\n",
    "import signal_augmentations as sa \n",
    "\n",
    "from models import cnn\n",
    "from losses import loss_cot, loss_diff, loss_diff, p_loss_diff, p_loss_sup\n",
    "from metrics import CategoricalAccuracy, Ratio\n",
    "from ramps import Warmup, sigmoid_rampup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.sess = \"default\"\n",
    "        self.nb_view = 2\n",
    "        self.batchsize = 100\n",
    "        self.lambda_cot_max = 10\n",
    "        self.lambda_diff_max = 0.5\n",
    "        self.ratio = 0.1\n",
    "        self.seed = 1234\n",
    "        self.epochs = 600\n",
    "        self.warm_up = 80\n",
    "        self.momentum = 0.0\n",
    "        self.decay = 1e-3\n",
    "        self.epsilon = 0.02\n",
    "        self.num_class = 10\n",
    "        self.cifar10_dir = \"/corpus/corpus/UrbanSound8K\"\n",
    "        self.tensorboard_dir = \"tensoboard_cotraining\"\n",
    "        self.checkpoint_dir = \"checkpoint\"\n",
    "        self.base_lr = 0.05\n",
    "        self.resume = False\n",
    "        self.job_name = \"default\"\n",
    "        self.multi_gpu = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Co-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7abc85e6fa4c2f8f085575dd9dc6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nb file loaded: 7895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07de8a6a1094b0ea6ca8c29b9f40f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nb file loaded: 837\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "manager = DatasetManager(metadata_root, audio_root, subsampling=1, subsampling_method=\"balance\", verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the sampler with the specified number of supervised file\n",
    "train_dataset = CoTrainingDataset(manager, args.ratio, train=True, val=False, cached=False)\n",
    "val_dataset = CoTrainingDataset(manager, 1.0, train=False, val=True, cached=False)\n",
    "\n",
    "train_sampler = CoTrainingSampler(train_dataset, args.batchsize, nb_class=10, nb_view=args.nb_view, ratio=None, method=\"duplicate\") # ratio is manually set here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = cnn\n",
    "\n",
    "m1, m2 = model_func(), model_func()\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders & adversarial generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=10)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=128, num_workers=4)\n",
    "\n",
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizers, Warmup & callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = optim.SGD(params, lr=args.base_lr, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "lr_lambda = lambda epoch: (1.0 + math.cos((epoch-1)*math.pi/args.epochs))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warm_up, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warm_up, sigmoid_rampup)\n",
    "\n",
    "callbacks = [lr_scheduler, lambda_cot, lambda_diff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "ratioS = [Ratio(), Ratio()]\n",
    "ratioU = [Ratio(), Ratio()]\n",
    "ratioSU = [Ratio(), Ratio()]\n",
    "accS = [CategoricalAccuracy(), CategoricalAccuracy()]\n",
    "accU = [CategoricalAccuracy(), CategoricalAccuracy()]\n",
    "accSU = [CategoricalAccuracy(), CategoricalAccuracy()]\n",
    "\n",
    "def reset_all_metrics():\n",
    "    all_metrics = [*ratioS, *ratioU, *ratioSU, *accS, *accU, *accSU]\n",
    "    for m in all_metrics:\n",
    "        m.reset()\n",
    "        \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "title = \"%s_%s_%slcm_%sldm_%swl\" % (\n",
    "    get_datetime(),\n",
    "    args.job_name,\n",
    "    args.lambda_cot_max,\n",
    "    args.lambda_diff_max,\n",
    "    args.warm_up,\n",
    ")\n",
    "tensorboard = SummaryWriter(\"%s/%s\" % (args.tensorboard_dir, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "    \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        for i in range(len(X_S)):\n",
    "            X_S[i] = X_S[i].cuda()\n",
    "            y_S[i] = y_S[i].cuda()\n",
    "        X_U, y_U = X_U.cuda(), y_U.cuda()\n",
    "\n",
    "        logits_S1 = m1(X_S[0])\n",
    "        logits_S2 = m2(X_S[1])\n",
    "        logits_U1 = m1(X_U)\n",
    "        logits_U2 = m2(X_U)\n",
    "\n",
    "        _, pred_S1 = torch.max(logits_S1, 1)\n",
    "        _, pred_S2 = torch.max(logits_S2, 1)\n",
    "\n",
    "        # pseudo labels of U \n",
    "        _, pred_U1 = torch.max(logits_U1, 1)\n",
    "        _, pred_U2 = torch.max(logits_U2, 1)\n",
    "\n",
    "        # ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "\n",
    "        #generate adversarial examples ----\n",
    "        adv_data_S1 = adv_generator_1.perturb(X_S[0], y_S[0])\n",
    "        adv_data_U1 = adv_generator_1.perturb(X_U, pred_U1)\n",
    "\n",
    "        adv_data_S2 = adv_generator_2.perturb(X_S[1], y_S[1])\n",
    "        adv_data_U2 = adv_generator_2.perturb(X_U, pred_U2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        adv_logits_S1 = m1(adv_data_S2)\n",
    "        adv_logits_S2 = m2(adv_data_S1)\n",
    "\n",
    "        adv_logits_U1 = m1(adv_data_U2)\n",
    "        adv_logits_U2 = m2(adv_data_U1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "\n",
    "        # losses ----\n",
    "        Loss_sup_S1, Loss_sup_S2, Loss_sup = p_loss_sup(logits_S1, logits_S2, y_S[0], y_S[1])\n",
    "        Loss_cot = loss_cot(logits_U1, logits_U2)\n",
    "        pld_S, pld_U, Loss_diff = p_loss_diff(logits_S1, logits_S2, adv_logits_S1, adv_logits_S2, logits_U1, logits_U2, adv_logits_U1, adv_logits_U2)\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot() * Loss_cot + lambda_diff() * Loss_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        # accuracies ----\n",
    "        pred_SU1 = torch.cat((pred_S1, pred_U1), 0)\n",
    "        pred_SU2 = torch.cat((pred_S2, pred_U2), 0)\n",
    "        y_SU1 = torch.cat((y_S[0], y_U), 0)\n",
    "        y_SU2 = torch.cat((y_S[1], y_U), 0)\n",
    "\n",
    "        acc_S1 = accS[0](pred_S1, y_S[0])\n",
    "        acc_S2 = accS[1](pred_S2, y_S[1])\n",
    "        acc_U1 = accU[0](pred_U1, y_U)\n",
    "        acc_U2 = accU[1](pred_U2, y_U)\n",
    "        acc_SU1 = accSU[0](pred_SU1, y_SU1)\n",
    "        acc_SU2 = accSU[1](pred_SU2, y_SU2)\n",
    "        \n",
    "        # ratios  ----\n",
    "        _, adv_pred_S1 = torch.max(adv_logits_S1, 1)\n",
    "        _, adv_pred_S2 = torch.max(adv_logits_S2, 1)\n",
    "        _, adv_pred_U1 = torch.max(adv_logits_U1, 1)\n",
    "        _, adv_pred_U2 = torch.max(adv_logits_U2, 1)\n",
    "\n",
    "        adv_pred_SU1 = torch.cat((adv_pred_S1, adv_pred_U1), 0)\n",
    "        adv_pred_SU2 = torch.cat((adv_pred_S2, adv_pred_U2), 0)\n",
    "        adv_y_SU1 = torch.cat((y_S[0], pred_U1), 0)\n",
    "        adv_y_SU2 = torch.cat((y_S[1], pred_U2), 0)\n",
    "\n",
    "        ratio_S1 = ratioS[0](adv_pred_S1, y_S[0])\n",
    "        ratio_S2 = ratioS[1](adv_pred_S2, y_S[1])\n",
    "        ratio_U1 = ratioU[0](adv_pred_U1, pred_U1)\n",
    "        ratio_U2 = ratioU[1](adv_pred_U2, pred_U2)\n",
    "        ratio_SU1 = ratioSU[0](adv_pred_SU1, adv_y_SU1)\n",
    "        ratio_SU2 = ratioSU[1](adv_pred_SU2, adv_y_SU2)\n",
    "        # ========\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # print statistics\n",
    "        msg = \"Epoch %s: %.2f%% : train acc: %.3f %.3f - Loss: %.3f %.3f %.3f %.3f - time: %.2f\" % (\n",
    "            epoch, (batch / len(train_sampler)) * 100,\n",
    "            acc_SU1, acc_SU2,\n",
    "            running_loss/(batch+1), ls/(batch+1), lc/(batch+1), ld/(batch+1),\n",
    "            time.time() - start_time,\n",
    "        )\n",
    "        print(msg, end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_SU1, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_SU2, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_loss/Lsus S1\", Loss_sup_S1.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Lsus S2\", Loss_sup_S2.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Ldiff S\", pld_S.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Ldiff U\", pld_U.item(), epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc S1\", acc_S1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc S2\", acc_S2, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc U1\", acc_U1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc U2\", acc_U2, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio S1\", ratio_S1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio S2\", ratio_S2, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio U1\", ratio_U1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio U2\", ratio_U2, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio SU1\", ratio_SU1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio SU2\", ratio_SU2, epoch)\n",
    "    \n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item(), msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    global best_acc\n",
    "    \n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    \n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(val_loader):\n",
    "            X = X.squeeze()\n",
    "            y = y.squeeze()\n",
    "\n",
    "            # separate Supervised (S) and Unsupervised (U) parts\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            outputs1 = m1(X)\n",
    "            predicted1 = outputs1.max(1)\n",
    "            total1 += y.size(0)\n",
    "            correct1 += predicted1[1].eq(y).sum().item()\n",
    "\n",
    "            outputs2 = m2(X)\n",
    "            predicted2 = outputs2.max(1)\n",
    "            total2 += y.size(0)\n",
    "            correct2 += predicted2[1].eq(y).sum().item()\n",
    "\n",
    "    msg += '\\tnet1 test acc: %.3f%% (%d/%d) | net2 test acc: %.3f%% (%d/%d)' % (\n",
    "        100.*correct1/total1, correct1, total1, 100.*correct2/total2, correct2, total2)\n",
    "    print(msg, end=\"\")\n",
    "    \n",
    "    tensorboard.add_scalar(\"val/acc 1\", correct1 / total1, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc 2\", correct2 / total2, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "    total_loss, msg = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch, msg)\n",
    "\n",
    "# tensorboard.export_scalars_to_json('./' + args.tensorboard_dir + 'output.json')\n",
    "# tensorboard.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Supervised training (same ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6676ea53d769450e901a8a208d7956ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nb file loaded: 7895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae66090ed0064908b90edb38cb4eefb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nb file loaded: 837\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "manager = DatasetManager(metadata_root, audio_root, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep dataset\n",
    "train_dataset = CoTrainingDataset(manager, args.ratio, train=True, val=False, cached=False)\n",
    "val_dataset = CoTrainingDataset(manager, 1.0, train=False, val=True, cached=False)\n",
    "\n",
    "train_sampler = CoTrainingSampler(train_dataset, args.batchsize, nb_class=10, nb_view=args.nb_view, ratio=None, method=\"duplicate\") # ratio is manually set here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = cnn\n",
    "\n",
    "m1 = model_func()\n",
    "\n",
    "m1 = m1.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders & adversarial generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=10)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=128, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizers & callbacks & criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = m1.parameters()\n",
    "optimizer = optim.SGD(params, lr=args.base_lr, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "lr_lambda = lambda epoch: (1.0 + math.cos((epoch-1)*math.pi/args.epochs))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "callbacks = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "acc_func = CategoricalAccuracy()\n",
    "\n",
    "def reset_all_metrics():\n",
    "    acc_func.reset()\n",
    "        \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "title = \"%s_supervised\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(\"%s/%s\" % (args.tensorboard_dir, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    m1.train()\n",
    "    reset_all_metrics()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "    \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        # Only one view interesting, no U\n",
    "        X_S = X_S[0]\n",
    "        y_S = y_S[0]\n",
    "        \n",
    "        X_S, y_S = X_S.cuda(), y_S.cuda()\n",
    "\n",
    "        # ======== perform prediction ========\n",
    "        logits_S = m1(X_S)\n",
    "        _, pred_S = torch.max(logits_S, 1)\n",
    "\n",
    "        # ======== calculate loss ========\n",
    "        loss_sup = criterion(logits_S, y_S)\n",
    "        total_loss = loss_sup\n",
    "        \n",
    "        # ======== backpropagation =======\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        acc = acc_func(pred_S, y_S)\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        # print statistics\n",
    "        msg = \"Epoch %s: %.2f%% : train acc: %.3f - Loss: %.3f - time: %.2f\" % (\n",
    "            epoch, (batch / len(train_sampler)) * 100,\n",
    "            acc,\n",
    "            running_loss / (batch+1),\n",
    "            time.time() - start_time,\n",
    "        )\n",
    "        print(msg, end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar('train/acc', acc, epoch)\n",
    "    \n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item(), msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg=\"\"):\n",
    "    m1.eval()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(val_loader):\n",
    "            X = X.squeeze()\n",
    "            y = y.squeeze()\n",
    "\n",
    "            # separate Supervised (S) and Unsupervised (U) parts\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits = m1(X)\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            \n",
    "            loss_val = criterion(logits, y)\n",
    "            \n",
    "            acc_val = acc_func(pred, y)\n",
    "        \n",
    "        msg += \"\\nEpoch %s: Val acc: %.3f - loss: %.3f\" % (\n",
    "            epoch,\n",
    "            acc_val,\n",
    "            loss_val.item()\n",
    "        )\n",
    "        print(msg, end=\"\")\n",
    "    \n",
    "    tensorboard.add_scalar(\"val/acc\", acc_val, epoch)\n",
    "    tensorboard.add_scalar(\"val/loss\", loss_val.item(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: 97.44% : train acc: 0.288 - Loss: 2.005 - time: 9.38\n",
      "Epoch 0: Val acc: 0.184 - loss: 2.093\n",
      "Epoch 1: 97.44% : train acc: 0.445 - Loss: 1.631 - time: 9.67\n",
      "Epoch 1: Val acc: 0.399 - loss: 2.094\n",
      "Epoch 2: 97.44% : train acc: 0.491 - Loss: 1.557 - time: 9.75\n",
      "Epoch 2: Val acc: 0.320 - loss: 2.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f8d89278320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcances/.miniconda3/envs/dl/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _releaseLock at 0x7f8d89278320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcances/.miniconda3/envs/dl/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 32534, 32535, 32537, 32538, 32539, 32540, 32542, 32543) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-1e7cfdd301b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Losses are NaN, stoping the training here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-135b6fff0239>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 32534, 32535, 32537, 32538, 32539, 32540, 32542, 32543) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "    total_loss, msg = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5f8108d8118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkurtosis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kurtosis\n",
    "\n",
    "for batch, (X, y) in enumerate(train_loader):\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "    \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        X_U = X_U.numpy()\n",
    "        print(X_U.shape)\n",
    "        \n",
    "        # max\n",
    "        val = X_U.max(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        # min\n",
    "        print(\"------------------\")\n",
    "        val = X_U.min(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        # mean\n",
    "        print(\"------------------\")\n",
    "        val = X_U.mean(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        # std\n",
    "        print(\"------------------\")\n",
    "        val = X_U.std(axis=(1, 2))\n",
    "        print(\"max max: \", val.max())\n",
    "        print(\"min max: \", val.min())\n",
    "        print(\"mean max: \", val.mean())\n",
    "        print(\"std max: \", val.std())\n",
    "        print(\"kurosis: \", kurtosis(val))\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
