{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from datasetManager import DatasetManager\n",
    "from generators import Generator, CoTrainingGenerator\n",
    "from samplers import CoTrainingSampler\n",
    "import signal_augmentations as sa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.973823Z",
     "start_time": "2019-11-12T15:36:12.893994Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.value = 0\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.count += 1\n",
    "\n",
    "        \n",
    "class BinaryAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = (y_pred>0.5).float()\n",
    "            correct = (y_pred == y_true).float().sum()\n",
    "            self.value = correct/ (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "        \n",
    "class CategoricalAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.value = torch.mean((y_true == y_pred).float())\n",
    "            self.accumulate_value += self.value\n",
    "\n",
    "            return self.accumulate_value / self.count\n",
    "\n",
    "        \n",
    "class Ratio(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_adv_pred):\n",
    "        super().__call__()\n",
    "        \n",
    "        results = zip(y_pred, y_adv_pred)\n",
    "        results_bool = [int(r[0] != r[1]) for r in results]\n",
    "        self.value = sum(results_bool) / len(results_bool) * 100\n",
    "        self.accumulate_value += self.value\n",
    "        \n",
    "        return self.accumulate_value / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:12.997511Z",
     "start_time": "2019-11-12T15:36:12.975482Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:13.020782Z",
     "start_time": "2019-11-12T15:36:13.000410Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:36:34.259332Z",
     "start_time": "2019-11-12T15:36:34.233822Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "https://arxiv.org/pdf/1608.04363.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvPoolReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.MaxPool2d(kernel_size=(4, 2), stride=(4, 2)),\n",
    "            nn.BatchNorm2d(out_size),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "        \n",
    "class ConvReLU(nn.Sequential):\n",
    "    def __init__(self, in_size, out_size, kernel_size, stride, padding):\n",
    "        super(ConvReLU, self).__init__(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            ConvPoolReLU(1, 24, 3, 1, 1),\n",
    "            ConvPoolReLU(24, 48, 3, 1, 1),\n",
    "            ConvPoolReLU(48, 48, 3, 1, 1),\n",
    "            ConvReLU(48, 48, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1008, 10),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64, 10),\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, *x.shape[1:])\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== Co-Training ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lsup(logit_S1, logit_S2, labels_S1, labels_S2):\n",
    "    ce = nn.CrossEntropyLoss() \n",
    "    loss1 = ce(logit_S1, labels_S1)\n",
    "    loss2 = ce(logit_S2, labels_S2) \n",
    "    return (loss1+loss2)\n",
    "\n",
    "def Lcot(U_p1, U_p2):\n",
    "# the Jensen-Shannon divergence between p1(x) and p2(x)\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    U_batch_size = U_p1.size()[0]\n",
    "    \n",
    "    a1 = 0.5 * (S(U_p1) + S(U_p2))\n",
    "    loss1 = a1 * torch.log(a1)\n",
    "    loss1 = -torch.sum(loss1)\n",
    "    loss2 = S(U_p1) * LS(U_p1)\n",
    "    loss2 = -torch.sum(loss2)\n",
    "    loss3 = S(U_p2) * LS(U_p2)\n",
    "    loss3 = -torch.sum(loss3)\n",
    "\n",
    "    return (loss1 - 0.5 * (loss2 + loss3))/U_batch_size\n",
    "\n",
    "\n",
    "def Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2):\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    batch_size = logit_S1.size()[0] + logit_U1.size()[0]\n",
    "    \n",
    "    \n",
    "    a = S(logit_S2) * LS(perturbed_logit_S1)\n",
    "    a = torch.sum(a)\n",
    "\n",
    "    b = S(logit_S1) * LS(perturbed_logit_S2)\n",
    "    b = torch.sum(b)\n",
    "\n",
    "    c = S(logit_U2) * LS(perturbed_logit_U1)\n",
    "    c = torch.sum(c)\n",
    "\n",
    "    d = S(logit_U1) * LS(perturbed_logit_U2)\n",
    "    d = torch.sum(d)\n",
    "\n",
    "    return -(a+b+c+d)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lamda(epoch):\n",
    "    epoch = epoch + 1\n",
    "    global lambda_cot\n",
    "    global lambda_diff\n",
    "    if epoch <= 80:\n",
    "        lambda_cot = lambda_cot_max*np.exp(-5*(1-epoch/80)**2)\n",
    "        lambda_diff = lambda_diff_max*np.exp(-5*(1-epoch/80)**2)\n",
    "    else: \n",
    "        lambda_cot = lambda_cot_max\n",
    "        lambda_diff = lambda_diff_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_func = cnn\n",
    "m1 = model_func()\n",
    "m2 = model_func()\n",
    "\n",
    "m1.cuda()\n",
    "m2.cuda()\n",
    "\n",
    "multi_gpu = True\n",
    "if multi_gpu:\n",
    "    m1 = nn.DataParallel(m1)\n",
    "    m2 = nn.DataParallel(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.50s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "audio_root = \"../dataset/audio\"\n",
    "metadata_root = \"../dataset/metadata\"\n",
    "\n",
    "dataset = DatasetManager(metadata_root, audio_root, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion_bce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# optimizer\n",
    "parameters = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = torch.optim.SGD(\n",
    "    parameters,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    "    lr=0.05\n",
    ")\n",
    "\n",
    "# Augmentation to use\n",
    "# ps1 = sa.PitchShift(0.5, DatasetManager.SR, (-2, 3))\n",
    "# n = sa.Noise(0.5, (0.05, 0.2))\n",
    "augments = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_gen_1 = GradientSignAttack( \n",
    "                m1,\n",
    "                loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "                eps=0.02, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    "            )\n",
    "\n",
    "adv_gen_2 = GradientSignAttack( \n",
    "                m2,\n",
    "                loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "                eps=0.02, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_cot_max = 10\n",
    "lambda_diff_max = 0.5\n",
    "lambda_cot = 0.0\n",
    "lambda_diff = 0.0\n",
    "best_acc = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 837/837 [00:04<00:00, 189.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "ratio = 0.1\n",
    "batch_size = 100\n",
    "nb_epoch = 600\n",
    "\n",
    "# prepare the sampler with the specified ratio of supervised fime and a specific batch size\n",
    "nb_train_file = len(dataset.audio[\"train\"])\n",
    "nb_s_file = int(nb_train_file * ratio)   # theorical number of supervised file\n",
    "nb_s_file = nb_s_file - (nb_s_file % DatasetManager.NB_CLASS)  # need to be a multiple of number of class\n",
    "nb_u_file = nb_train_file - nb_s_file\n",
    "sampler = CoTrainingSampler(batch_size, nb_s_file, nb_u_file, nb_view=2, ratio=None, method=\"duplicate\")\n",
    "\n",
    "# create the generator and the loader\n",
    "generator = CoTrainingGenerator(dataset, sampler, augments=augments)\n",
    "train_loader = data.DataLoader(generator, batch_sampler=sampler)\n",
    "\n",
    "# val loader\n",
    "x, y = generator.validation\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)\n",
    "val_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# scheduler\n",
    "lr_lambda = lambda epoch: 0.5 * (np.cos(np.pi * epoch / nb_epoch) + 1)\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "callbacks = [lr_scheduler]\n",
    "# callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"%s_cnn_Cosd-lr_sgd-0.05lr-wd0.0001-m0.9_%de_noaugment\" % ( get_datetime(), nb_epoch )\n",
    "tensorboard = SummaryWriter(log_dir=\"cotraining/%s\" % title, comment=model_func.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 77/78 [00:00<00:00, 126750.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693 693\n",
      "693 693\n",
      "6930 6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "S1, S2, U = [], [], []\n",
    "for batch in tqdm.tqdm(sampler):\n",
    "    s1, s2, u = batch[0][0], batch[0][1], batch[0][2]\n",
    "    S1.extend(s1)\n",
    "    S2.extend(s2)\n",
    "    U.extend(u)\n",
    "    \n",
    "print(len(S1), len(np.unique(S1)))\n",
    "print(len(S2), len(np.unique(S2)))\n",
    "print(len(U), len(np.unique(U)))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1_func = CategoricalAccuracy()\n",
    "acc2_func = CategoricalAccuracy()\n",
    "   \n",
    "def SU_train(epoch, train_loader):\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    adjust_lamda(epoch)\n",
    "    \n",
    "    acc1_func.reset()\n",
    "    acc2_func.reset()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        # output add one extra dimension (1, B, ...) instead of (B, ...)\n",
    "        X = [x.squeeze() for x in X]\n",
    "        y = [y_.squeeze() for y_ in y]\n",
    "        \n",
    "        # separate Supervised (S) and Unsupervised (U) parts\n",
    "        X_S, X_U = X[:-1], X[-1]\n",
    "        y_S, y_U = y[:-1], y[-1]\n",
    "        \n",
    "        # move to GPU\n",
    "        X_S = [xs.cuda().float() for xs in X_S]\n",
    "        y_S = [ys.cuda().long() for ys in y_S]\n",
    "        X_U = X_U.cuda().float()\n",
    "        y_U = y_U.cuda().long()\n",
    "        \n",
    "        # Prediction\n",
    "        logits_S1 = m1(X_S[0])\n",
    "        logits_S2 = m2(X_S[1])\n",
    "        logits_U1 = m1(X_U)\n",
    "        logits_U2 = m2(X_U)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        # TODO pseudo labels ? how many class ?\n",
    "        _, pred_U1 = torch.max(logits_U1, 1)\n",
    "        _, pred_U2 = torch.max(logits_U2, 1)\n",
    "\n",
    "        # fix batchnorm & dropout\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "        \n",
    "        #generate adversarial examples\n",
    "        # Multi-target doesn't work with advertorch. Using the pseudo label of the prediction\n",
    "        perturbed_data_S1 = adv_gen_1.perturb(X_S[0], y_S[0])\n",
    "        perturbed_data_S2 = adv_gen_2.perturb(X_S[1], y_S[1])\n",
    "        \n",
    "        perturbed_data_U1 = adv_gen_1.perturb(X_U, pred_U1)\n",
    "        perturbed_data_U2 = adv_gen_2.perturb(X_U, pred_U2)\n",
    "        \n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict using adversarial samples\n",
    "        perturbed_logits_S1 = m1(perturbed_data_S2)\n",
    "        perturbed_logits_S2 = m2(perturbed_data_S1)\n",
    "\n",
    "        perturbed_logits_U1 = m1(perturbed_data_U2)\n",
    "        perturbed_logits_U2 = m2(perturbed_data_U1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "        \n",
    "        Loss_sup = Lsup(logits_S1, logits_S2, y_S[0], y_S[1])\n",
    "        Loss_cot = Lcot(logits_U1, logits_U2)\n",
    "        Loss_diff = Ldiff(\n",
    "            logits_S1, logits_S2, perturbed_logits_S1, perturbed_logits_S2,\n",
    "            logits_U1, logits_U2, perturbed_logits_U1, perturbed_logits_U2\n",
    "        )\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc the metrics\n",
    "        pred_S1 = torch.softmax(logits_S1, dim=1)\n",
    "        pred_S2 = torch.softmax(logits_S2, dim=1)\n",
    "        _, pred_S1 = torch.max(pred_S1, 1)\n",
    "        _, pred_S2 = torch.max(pred_S2, 1)\n",
    "        acc_1 = acc1_func(pred_S1, y_S[0])\n",
    "        acc_2 = acc2_func(pred_S2, y_S[1])\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        print(\"Epoch {:4}, {:3d}% \\t acc: {:3.4e} {:3.4e} - loss {:3.4e} {:3.4e} {:3.4e} {:3.4e} took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / sampler.nb_batch),\n",
    "            \n",
    "            acc_1, acc_2,\n",
    "            running_loss/(i+1), ls/(i+1), lc/(i+1), ld/(i+1), \n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "            \n",
    "    # using tensorboard to monitor loss and acc\n",
    "    tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_1, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_2, epoch )\n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    \n",
    "    acc1_func.reset()\n",
    "    acc2_func.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.cuda().float()\n",
    "            y = y.cuda().long()\n",
    "\n",
    "            logits_S1 = m1(X)\n",
    "            logits_S2 = m2(X)\n",
    "\n",
    "            # calc the metrics\n",
    "            pred_S1 = torch.softmax(logits_S1, dim=1)\n",
    "            pred_S2 = torch.softmax(logits_S2, dim=1)\n",
    "            _, pred_S1 = torch.max(pred_S1, 1)\n",
    "            _, pred_S2 = torch.max(pred_S2, 1)\n",
    "            acc_1_val = acc1_func(pred_S1, y)\n",
    "            acc_2_val = acc2_func(pred_S2, y)\n",
    "            \n",
    "    tensorboard.add_scalar('val/acc_1', acc_1_val, epoch)\n",
    "    tensorboard.add_scalar('val/acc_2', acc_2_val, epoch)\n",
    "    \n",
    "    for callback in callbacks:\n",
    "        callback.step()\n",
    "\n",
    "    print('\\nnet1 test acc: %.3e | net2 test acc: %.3e' % (acc_1_val, acc_2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1,  98% \t acc: 1.8470e-01 1.7605e-01 - loss 4.5511e+00 4.5268e+00 6.8578e-02 5.0059e+00 took: 51.70s\n",
      "net1 test acc: 1.053e-01 | net2 test acc: 1.109e-01\n",
      "Epoch    2,  98% \t acc: 1.1111e-01 1.2698e-01 - loss 4.6088e+00 4.5861e+00 2.4825e-02 4.7559e+00 took: 45.32s\n",
      "net1 test acc: 1.209e-01 | net2 test acc: 1.804e-01\n",
      "Epoch    3,  98% \t acc: 8.6580e-02 1.3709e-01 - loss 4.5852e+00 4.5592e+00 2.9029e-02 4.7562e+00 took: 44.31s\n",
      "net1 test acc: 6.568e-02 | net2 test acc: 2.189e-01\n",
      "Epoch    4,  98% \t acc: 1.2554e-01 1.5440e-01 - loss 4.5174e+00 4.4871e+00 3.6703e-02 4.7839e+00 took: 44.37s\n",
      "net1 test acc: 1.787e-01 | net2 test acc: 1.052e-01\n",
      "Epoch    5,  98% \t acc: 1.5296e-01 1.8903e-01 - loss 4.4310e+00 4.3940e+00 5.3192e-02 4.9257e+00 took: 44.35s\n",
      "net1 test acc: 1.825e-01 | net2 test acc: 2.411e-01\n",
      "Epoch    6,  98% \t acc: 1.7605e-01 2.0202e-01 - loss 4.3261e+00 4.2866e+00 5.0039e-02 4.6991e+00 took: 47.37s\n",
      "net1 test acc: 1.925e-01 | net2 test acc: 2.990e-01\n",
      "Epoch    7,  98% \t acc: 1.8759e-01 2.2511e-01 - loss 4.3326e+00 4.2861e+00 6.1586e-02 4.7547e+00 took: 45.08s\n",
      "net1 test acc: 2.194e-01 | net2 test acc: 2.121e-01\n",
      "Epoch    8,  98% \t acc: 1.7027e-01 2.4098e-01 - loss 4.2170e+00 4.1665e+00 5.6969e-02 4.6545e+00 took: 47.93s\n",
      "net1 test acc: 1.991e-01 | net2 test acc: 2.813e-01\n",
      "Epoch    9,  98% \t acc: 1.6306e-01 2.4387e-01 - loss 4.2083e+00 4.1499e+00 6.3844e-02 4.7227e+00 took: 48.07s\n",
      "net1 test acc: 1.961e-01 | net2 test acc: 1.826e-01\n",
      "Epoch   10,  98% \t acc: 1.7460e-01 2.3810e-01 - loss 4.2512e+00 4.1874e+00 5.8751e-02 4.6885e+00 took: 44.50s\n",
      "net1 test acc: 1.918e-01 | net2 test acc: 2.332e-01\n",
      "Epoch   11,  98% \t acc: 1.7027e-01 2.4098e-01 - loss 4.3345e+00 4.2571e+00 7.5473e-02 4.8752e+00 took: 45.75s\n",
      "net1 test acc: 2.186e-01 | net2 test acc: 2.423e-01\n",
      "Epoch   12,  98% \t acc: 1.6595e-01 2.5685e-01 - loss 4.2695e+00 4.1866e+00 6.6291e-02 4.8166e+00 took: 44.50s\n",
      "net1 test acc: 1.920e-01 | net2 test acc: 2.165e-01\n",
      "Epoch   13,  98% \t acc: 1.7893e-01 2.7994e-01 - loss 4.2300e+00 4.1438e+00 5.5492e-02 4.6412e+00 took: 47.76s\n",
      "net1 test acc: 1.883e-01 | net2 test acc: 2.058e-01\n",
      "Epoch   14,  98% \t acc: 1.6739e-01 2.3954e-01 - loss 4.2403e+00 4.1426e+00 5.6479e-02 4.7423e+00 took: 48.54s\n",
      "net1 test acc: 1.914e-01 | net2 test acc: 2.208e-01\n",
      "Epoch   15,  98% \t acc: 1.6306e-01 2.5541e-01 - loss 4.2407e+00 4.1319e+00 5.8785e-02 4.7317e+00 took: 44.10s\n",
      "net1 test acc: 2.117e-01 | net2 test acc: 7.934e-02\n",
      "Epoch   16,  98% \t acc: 1.8182e-01 2.8571e-01 - loss 4.1341e+00 4.0128e+00 6.2878e-02 4.6955e+00 took: 47.91s\n",
      "net1 test acc: 1.629e-01 | net2 test acc: 2.835e-01\n",
      "Epoch   17,  98% \t acc: 1.7316e-01 2.4531e-01 - loss 4.3117e+00 4.1621e+00 8.2180e-02 5.0001e+00 took: 44.32s\n",
      "net1 test acc: 2.230e-01 | net2 test acc: 1.317e-01\n",
      "Epoch   18,  98% \t acc: 1.7172e-01 1.9769e-01 - loss 4.3866e+00 4.2219e+00 7.6534e-02 5.1059e+00 took: 44.44s\n",
      "net1 test acc: 2.196e-01 | net2 test acc: 1.320e-01\n",
      "Epoch   19,  98% \t acc: 1.7172e-01 2.0346e-01 - loss 4.3957e+00 4.2114e+00 8.0056e-02 5.1435e+00 took: 44.28s\n",
      "net1 test acc: 2.162e-01 | net2 test acc: 1.235e-01\n",
      "Epoch   20,  98% \t acc: 1.7316e-01 2.4964e-01 - loss 4.3082e+00 4.1050e+00 8.2342e-02 5.1203e+00 took: 44.74s\n",
      "net1 test acc: 2.088e-01 | net2 test acc: 1.795e-01\n",
      "Epoch   21,  98% \t acc: 1.7605e-01 2.7417e-01 - loss 4.3422e+00 4.1212e+00 8.1404e-02 5.0785e+00 took: 47.52s\n",
      "net1 test acc: 1.899e-01 | net2 test acc: 1.280e-01\n",
      "Epoch   22,  98% \t acc: 1.8615e-01 3.0736e-01 - loss 4.2633e+00 4.0137e+00 8.8722e-02 5.1390e+00 took: 48.40s\n",
      "net1 test acc: 2.066e-01 | net2 test acc: 2.029e-01\n",
      "Epoch   23,  98% \t acc: 1.8326e-01 3.0447e-01 - loss 4.2833e+00 4.0202e+00 8.3040e-02 4.9991e+00 took: 47.58s\n",
      "net1 test acc: 1.958e-01 | net2 test acc: 2.074e-01\n",
      "Epoch   24,  98% \t acc: 1.9048e-01 2.8571e-01 - loss 4.3334e+00 4.0374e+00 8.9141e-02 5.0781e+00 took: 47.68s\n",
      "net1 test acc: 1.802e-01 | net2 test acc: 8.012e-02\n",
      "Epoch   25,  98% \t acc: 1.9048e-01 2.5397e-01 - loss 4.3990e+00 4.0821e+00 8.5372e-02 5.0280e+00 took: 44.32s\n",
      "net1 test acc: 1.939e-01 | net2 test acc: 1.566e-01\n",
      "Epoch   26,  98% \t acc: 1.7316e-01 2.2944e-01 - loss 4.4875e+00 4.1586e+00 7.2339e-02 4.9737e+00 took: 47.85s\n",
      "net1 test acc: 2.014e-01 | net2 test acc: 1.612e-01\n",
      "Epoch   27,  98% \t acc: 2.0635e-01 2.4675e-01 - loss 4.4197e+00 4.0484e+00 7.9500e-02 5.0756e+00 took: 44.95s\n",
      "net1 test acc: 2.076e-01 | net2 test acc: 1.480e-01\n",
      "Epoch   28,  98% \t acc: 1.8615e-01 2.9582e-01 - loss 4.3919e+00 3.9927e+00 7.8159e-02 5.0381e+00 took: 48.76s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.881e-01\n",
      "Epoch   29,  98% \t acc: 2.0635e-01 3.0880e-01 - loss 4.3849e+00 3.9449e+00 8.5591e-02 5.0029e+00 took: 47.61s\n",
      "net1 test acc: 2.130e-01 | net2 test acc: 1.888e-01\n",
      "Epoch   30,  98% \t acc: 1.8759e-01 3.2035e-01 - loss 4.4408e+00 3.9643e+00 8.4982e-02 5.0208e+00 took: 46.38s\n",
      "net1 test acc: 1.896e-01 | net2 test acc: 1.787e-01\n",
      "Epoch   31,  98% \t acc: 2.5397e-01 2.9004e-01 - loss 4.4349e+00 3.9318e+00 8.0423e-02 4.9572e+00 took: 47.05s\n",
      "net1 test acc: 2.077e-01 | net2 test acc: 1.168e-01\n",
      "Epoch   32,  98% \t acc: 2.1356e-01 2.5541e-01 - loss 4.5662e+00 4.0428e+00 6.9447e-02 4.9438e+00 took: 51.90s\n",
      "net1 test acc: 2.246e-01 | net2 test acc: 1.780e-01\n",
      "Epoch   33,  98% \t acc: 2.5974e-01 2.5397e-01 - loss 4.5684e+00 4.0287e+00 6.0395e-02 4.8551e+00 took: 45.84s\n",
      "net1 test acc: 1.932e-01 | net2 test acc: 1.836e-01\n",
      "Epoch   34,  98% \t acc: 2.2078e-01 2.4675e-01 - loss 4.6523e+00 4.0697e+00 6.0798e-02 4.8700e+00 took: 46.51s\n",
      "net1 test acc: 1.910e-01 | net2 test acc: 1.214e-01\n",
      "Epoch   35,  98% \t acc: 2.6551e-01 2.4675e-01 - loss 4.6354e+00 4.0400e+00 5.0292e-02 4.7880e+00 took: 46.33s\n",
      "net1 test acc: 2.200e-01 | net2 test acc: 1.909e-01\n",
      "Epoch   36,  98% \t acc: 2.3810e-01 2.4387e-01 - loss 4.6361e+00 3.9912e+00 5.3021e-02 4.7931e+00 took: 44.23s\n",
      "net1 test acc: 2.225e-01 | net2 test acc: 1.844e-01\n",
      "Epoch   37,  98% \t acc: 2.6263e-01 2.8283e-01 - loss 4.5996e+00 3.9179e+00 5.3087e-02 4.7188e+00 took: 44.10s\n",
      "net1 test acc: 2.196e-01 | net2 test acc: 1.680e-01\n",
      "Epoch   38,  98% \t acc: 2.6118e-01 2.5253e-01 - loss 4.7153e+00 3.9722e+00 5.7156e-02 4.7528e+00 took: 44.62s\n",
      "net1 test acc: 1.821e-01 | net2 test acc: 1.447e-01\n",
      "Epoch   39,  98% \t acc: 2.3232e-01 2.4820e-01 - loss 4.7884e+00 4.0173e+00 5.2315e-02 4.6878e+00 took: 43.97s\n",
      "net1 test acc: 1.838e-01 | net2 test acc: 2.131e-01\n",
      "Epoch   40,  98% \t acc: 2.6984e-01 3.0447e-01 - loss 4.4921e+00 3.7421e+00 4.2105e-02 4.3937e+00 took: 44.22s\n",
      "net1 test acc: 2.062e-01 | net2 test acc: 2.085e-01\n",
      "Epoch   41,  98% \t acc: 2.6551e-01 2.7706e-01 - loss 4.6755e+00 3.8369e+00 5.1676e-02 4.4698e+00 took: 44.72s\n",
      "net1 test acc: 2.346e-01 | net2 test acc: 2.519e-01\n",
      "Epoch   42,  98% \t acc: 2.6696e-01 2.9004e-01 - loss 4.6588e+00 3.8133e+00 3.9771e-02 4.4292e+00 took: 44.68s\n",
      "net1 test acc: 1.780e-01 | net2 test acc: 1.859e-01\n",
      "Epoch   43,  98% \t acc: 2.5541e-01 3.0159e-01 - loss 4.6614e+00 3.7843e+00 3.6286e-02 4.3861e+00 took: 46.27s\n",
      "net1 test acc: 2.237e-01 | net2 test acc: 1.647e-01\n",
      "Epoch   44,  98% \t acc: 2.3521e-01 2.7273e-01 - loss 4.9090e+00 3.9716e+00 3.6471e-02 4.4304e+00 took: 44.71s\n",
      "net1 test acc: 2.364e-01 | net2 test acc: 1.774e-01\n",
      "Epoch   45,  98% \t acc: 2.3954e-01 2.9293e-01 - loss 4.8350e+00 3.8656e+00 3.3501e-02 4.3788e+00 took: 45.31s\n",
      "net1 test acc: 2.072e-01 | net2 test acc: 1.972e-01\n",
      "Epoch   46,  98% \t acc: 2.5397e-01 2.8716e-01 - loss 4.8541e+00 3.8196e+00 3.4058e-02 4.4238e+00 took: 46.47s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 1.850e-01\n",
      "Epoch   47,  98% \t acc: 2.6407e-01 2.7417e-01 - loss 4.9401e+00 3.8376e+00 3.8218e-02 4.3989e+00 took: 45.27s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 2.325e-01\n",
      "Epoch   48,  98% \t acc: 2.8716e-01 3.0592e-01 - loss 4.8846e+00 3.7443e+00 3.5670e-02 4.3621e+00 took: 44.31s\n",
      "net1 test acc: 2.383e-01 | net2 test acc: 1.660e-01\n",
      "Epoch   49,  98% \t acc: 2.7706e-01 2.8139e-01 - loss 4.8885e+00 3.7205e+00 3.2136e-02 4.3067e+00 took: 44.20s\n",
      "net1 test acc: 2.564e-01 | net2 test acc: 2.151e-01\n",
      "Epoch   50,  98% \t acc: 2.7994e-01 3.0736e-01 - loss 4.9480e+00 3.7155e+00 3.3528e-02 4.3088e+00 took: 44.12s\n",
      "net1 test acc: 2.296e-01 | net2 test acc: 1.999e-01\n",
      "Epoch   51,  98% \t acc: 2.7561e-01 2.8139e-01 - loss 5.1154e+00 3.8120e+00 3.4056e-02 4.3475e+00 took: 44.44s\n",
      "net1 test acc: 2.218e-01 | net2 test acc: 2.085e-01\n",
      "Epoch   52,  98% \t acc: 2.8716e-01 3.0303e-01 - loss 5.1020e+00 3.7432e+00 3.3236e-02 4.3494e+00 took: 45.79s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 2.362e-01\n",
      "Epoch   53,  98% \t acc: 2.3665e-01 2.6407e-01 - loss 5.2959e+00 3.8997e+00 2.9994e-02 4.3355e+00 took: 48.78s\n",
      "net1 test acc: 2.311e-01 | net2 test acc: 3.198e-01\n",
      "Epoch   54,  98% \t acc: 2.7273e-01 3.0880e-01 - loss 5.1696e+00 3.7634e+00 2.4384e-02 4.2813e+00 took: 45.21s\n",
      "net1 test acc: 2.221e-01 | net2 test acc: 1.771e-01\n",
      "Epoch   55,  98% \t acc: 2.6118e-01 2.7850e-01 - loss 5.2278e+00 3.7801e+00 2.3494e-02 4.2484e+00 took: 44.51s\n",
      "net1 test acc: 2.326e-01 | net2 test acc: 2.004e-01\n",
      "Epoch   56,  98% \t acc: 2.5541e-01 2.7417e-01 - loss 5.3478e+00 3.8426e+00 2.4216e-02 4.2367e+00 took: 44.55s\n",
      "net1 test acc: 2.295e-01 | net2 test acc: 2.083e-01\n",
      "Epoch   57,  98% \t acc: 2.5830e-01 2.8860e-01 - loss 5.3817e+00 3.8010e+00 2.5874e-02 4.2619e+00 took: 47.51s\n",
      "net1 test acc: 2.437e-01 | net2 test acc: 2.006e-01\n",
      "Epoch   58,  98% \t acc: 2.5830e-01 2.4675e-01 - loss 5.5575e+00 3.9133e+00 2.4608e-02 4.3076e+00 took: 47.90s\n",
      "net1 test acc: 2.226e-01 | net2 test acc: 1.469e-01\n",
      "Epoch   59,  98% \t acc: 2.5830e-01 2.9149e-01 - loss 5.5127e+00 3.7668e+00 2.9716e-02 4.3338e+00 took: 48.36s\n",
      "net1 test acc: 2.311e-01 | net2 test acc: 2.857e-01\n",
      "Epoch   60,  98% \t acc: 2.7417e-01 2.5974e-01 - loss 5.5658e+00 3.8138e+00 2.5465e-02 4.2799e+00 took: 45.02s\n",
      "net1 test acc: 2.567e-01 | net2 test acc: 1.948e-01\n",
      "Epoch   61,  98% \t acc: 3.0159e-01 2.8139e-01 - loss 5.4688e+00 3.6575e+00 2.8162e-02 4.2397e+00 took: 47.89s\n",
      "net1 test acc: 2.470e-01 | net2 test acc: 1.869e-01\n",
      "Epoch   62,  98% \t acc: 3.1313e-01 2.8427e-01 - loss 5.5324e+00 3.7172e+00 2.5086e-02 4.1744e+00 took: 48.52s\n",
      "net1 test acc: 2.195e-01 | net2 test acc: 1.839e-01\n",
      "Epoch   63,  98% \t acc: 2.6696e-01 2.8427e-01 - loss 5.6555e+00 3.7777e+00 2.6629e-02 4.1743e+00 took: 50.71s\n",
      "net1 test acc: 1.762e-01 | net2 test acc: 1.804e-01\n",
      "Epoch   64,  98% \t acc: 2.7994e-01 2.7561e-01 - loss 5.6182e+00 3.7366e+00 2.3001e-02 4.1365e+00 took: 45.23s\n",
      "net1 test acc: 2.290e-01 | net2 test acc: 1.908e-01\n",
      "Epoch   65,  98% \t acc: 2.5253e-01 2.9437e-01 - loss 5.7198e+00 3.7974e+00 2.2695e-02 4.1299e+00 took: 48.62s\n",
      "net1 test acc: 2.418e-01 | net2 test acc: 1.818e-01\n",
      "Epoch   66,  98% \t acc: 2.5397e-01 2.5685e-01 - loss 5.8256e+00 3.8326e+00 2.4224e-02 4.1611e+00 took: 45.41s\n",
      "net1 test acc: 2.394e-01 | net2 test acc: 1.758e-01\n",
      "Epoch   67,  98% \t acc: 2.6263e-01 2.7706e-01 - loss 5.7939e+00 3.7720e+00 2.3420e-02 4.1460e+00 took: 44.48s\n",
      "net1 test acc: 2.277e-01 | net2 test acc: 2.169e-01\n",
      "Epoch   68,  98% \t acc: 2.7994e-01 2.7706e-01 - loss 5.8398e+00 3.7590e+00 2.4403e-02 4.1690e+00 took: 45.18s\n",
      "net1 test acc: 2.266e-01 | net2 test acc: 1.769e-01\n",
      "Epoch   69,  98% \t acc: 2.6551e-01 2.5830e-01 - loss 5.9987e+00 3.8485e+00 2.3588e-02 4.2550e+00 took: 45.49s\n",
      "net1 test acc: 2.269e-01 | net2 test acc: 2.302e-01\n",
      "Epoch   70,  98% \t acc: 2.4964e-01 2.5830e-01 - loss 5.9537e+00 3.7969e+00 2.1577e-02 4.2327e+00 took: 47.99s\n",
      "net1 test acc: 2.174e-01 | net2 test acc: 2.229e-01\n",
      "Epoch   71,  98% \t acc: 2.7850e-01 2.5685e-01 - loss 6.0024e+00 3.8285e+00 2.2392e-02 4.1840e+00 took: 48.76s\n",
      "net1 test acc: 2.225e-01 | net2 test acc: 2.025e-01\n",
      "Epoch   72,  98% \t acc: 2.6263e-01 2.7417e-01 - loss 5.9393e+00 3.7912e+00 1.9128e-02 4.1338e+00 took: 48.30s\n",
      "net1 test acc: 2.366e-01 | net2 test acc: 1.939e-01\n",
      "Epoch   73,  98% \t acc: 2.6407e-01 2.9149e-01 - loss 5.9065e+00 3.7188e+00 2.0707e-02 4.1320e+00 took: 47.99s\n",
      "net1 test acc: 2.336e-01 | net2 test acc: 1.709e-01\n",
      "Epoch   74,  98% \t acc: 2.9004e-01 2.4531e-01 - loss 5.9914e+00 3.7797e+00 2.0329e-02 4.1430e+00 took: 48.16s\n",
      "net1 test acc: 2.355e-01 | net2 test acc: 2.370e-01\n",
      "Epoch   75,  98% \t acc: 2.7850e-01 2.4820e-01 - loss 6.0634e+00 3.7881e+00 2.3345e-02 4.1734e+00 took: 47.31s\n",
      "net1 test acc: 2.437e-01 | net2 test acc: 1.855e-01\n",
      "Epoch   76,  98% \t acc: 2.5685e-01 2.4820e-01 - loss 6.0629e+00 3.8084e+00 2.2093e-02 4.1240e+00 took: 48.32s\n",
      "net1 test acc: 1.975e-01 | net2 test acc: 1.792e-01\n",
      "Epoch   77,  98% \t acc: 2.7561e-01 2.7128e-01 - loss 5.9517e+00 3.7361e+00 1.9432e-02 4.0739e+00 took: 48.05s\n",
      "net1 test acc: 2.232e-01 | net2 test acc: 1.779e-01\n",
      "Epoch   78,  98% \t acc: 2.9149e-01 2.7561e-01 - loss 5.9478e+00 3.7138e+00 1.8593e-02 4.1101e+00 took: 48.11s\n",
      "net1 test acc: 2.144e-01 | net2 test acc: 2.121e-01\n",
      "Epoch   79,  98% \t acc: 2.4387e-01 2.5685e-01 - loss 6.1190e+00 3.8527e+00 1.9036e-02 4.1554e+00 took: 47.67s\n",
      "net1 test acc: 2.025e-01 | net2 test acc: 1.661e-01\n",
      "Epoch   80,  98% \t acc: 2.4531e-01 2.7273e-01 - loss 6.0667e+00 3.8296e+00 1.7414e-02 4.1259e+00 took: 48.18s\n",
      "net1 test acc: 2.244e-01 | net2 test acc: 1.606e-01\n",
      "Epoch   81,  98% \t acc: 2.3954e-01 2.5830e-01 - loss 6.0939e+00 3.8653e+00 1.6656e-02 4.1241e+00 took: 48.10s\n",
      "net1 test acc: 2.026e-01 | net2 test acc: 1.918e-01\n",
      "Epoch   82,  98% \t acc: 2.6263e-01 2.9437e-01 - loss 6.0820e+00 3.7893e+00 2.2658e-02 4.1324e+00 took: 48.02s\n",
      "net1 test acc: 2.362e-01 | net2 test acc: 2.004e-01\n",
      "Epoch   83,  98% \t acc: 2.5974e-01 2.7273e-01 - loss 6.1270e+00 3.8321e+00 2.2013e-02 4.1496e+00 took: 48.10s\n",
      "net1 test acc: 2.207e-01 | net2 test acc: 2.211e-01\n",
      "Epoch   84,  98% \t acc: 2.8283e-01 2.7850e-01 - loss 6.0635e+00 3.7759e+00 2.1362e-02 4.1480e+00 took: 47.83s\n",
      "net1 test acc: 2.377e-01 | net2 test acc: 1.861e-01\n",
      "Epoch   85,  98% \t acc: 2.7561e-01 2.6840e-01 - loss 6.0852e+00 3.8314e+00 1.8967e-02 4.1281e+00 took: 48.01s\n",
      "net1 test acc: 2.167e-01 | net2 test acc: 2.126e-01\n",
      "Epoch   86,  98% \t acc: 2.5253e-01 2.4242e-01 - loss 6.2958e+00 3.9970e+00 1.9729e-02 4.2031e+00 took: 47.89s\n",
      "net1 test acc: 2.446e-01 | net2 test acc: 2.259e-01\n",
      "Epoch   87,  98% \t acc: 2.8571e-01 2.7417e-01 - loss 5.9633e+00 3.6897e+00 1.8390e-02 4.1794e+00 took: 47.39s\n",
      "net1 test acc: 2.400e-01 | net2 test acc: 1.785e-01\n",
      "Epoch   88,  98% \t acc: 2.6407e-01 2.6696e-01 - loss 6.0893e+00 3.7639e+00 2.2619e-02 4.1983e+00 took: 47.17s\n",
      "net1 test acc: 2.242e-01 | net2 test acc: 1.787e-01\n",
      "Epoch   89,  98% \t acc: 2.8283e-01 2.7706e-01 - loss 6.0220e+00 3.7552e+00 1.8783e-02 4.1579e+00 took: 48.69s\n",
      "net1 test acc: 2.409e-01 | net2 test acc: 2.227e-01\n",
      "Epoch   90,  98% \t acc: 2.7561e-01 2.5974e-01 - loss 6.1000e+00 3.8107e+00 2.0924e-02 4.1600e+00 took: 48.13s\n",
      "net1 test acc: 2.241e-01 | net2 test acc: 2.230e-01\n",
      "Epoch   91,  98% \t acc: 2.7706e-01 2.6118e-01 - loss 5.9918e+00 3.7599e+00 1.7383e-02 4.1161e+00 took: 47.73s\n",
      "net1 test acc: 2.288e-01 | net2 test acc: 2.229e-01\n",
      "Epoch   92,  98% \t acc: 2.7850e-01 2.5541e-01 - loss 5.9971e+00 3.7193e+00 2.0568e-02 4.1443e+00 took: 47.90s\n",
      "net1 test acc: 2.302e-01 | net2 test acc: 1.672e-01\n",
      "Epoch   93,  98% \t acc: 2.7273e-01 2.7128e-01 - loss 5.9941e+00 3.7802e+00 1.6428e-02 4.0993e+00 took: 48.06s\n",
      "net1 test acc: 1.885e-01 | net2 test acc: 1.774e-01\n",
      "Epoch   94,  98% \t acc: 2.6407e-01 2.6407e-01 - loss 6.0975e+00 3.7803e+00 2.2891e-02 4.1765e+00 took: 48.01s\n",
      "net1 test acc: 2.121e-01 | net2 test acc: 1.783e-01\n",
      "Epoch   95,  98% \t acc: 2.8139e-01 2.8571e-01 - loss 5.9958e+00 3.7266e+00 2.0480e-02 4.1288e+00 took: 48.05s\n",
      "net1 test acc: 2.402e-01 | net2 test acc: 1.806e-01\n",
      "Epoch   96,  98% \t acc: 2.7128e-01 2.6551e-01 - loss 6.2105e+00 3.9128e+00 2.1594e-02 4.1635e+00 took: 48.52s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 2.044e-01\n",
      "Epoch   97,  98% \t acc: 2.4531e-01 2.3088e-01 - loss 6.2567e+00 3.9465e+00 2.1966e-02 4.1810e+00 took: 48.00s\n",
      "net1 test acc: 2.341e-01 | net2 test acc: 1.730e-01\n",
      "Epoch   98,  98% \t acc: 2.6984e-01 2.6984e-01 - loss 6.0484e+00 3.7535e+00 2.1179e-02 4.1663e+00 took: 48.24s\n",
      "net1 test acc: 1.921e-01 | net2 test acc: 1.850e-01\n",
      "Epoch   99,  98% \t acc: 2.9726e-01 2.6551e-01 - loss 5.9514e+00 3.6999e+00 1.9782e-02 4.1074e+00 took: 48.27s\n",
      "net1 test acc: 2.128e-01 | net2 test acc: 1.795e-01\n",
      "Epoch  100,  98% \t acc: 2.6118e-01 2.5397e-01 - loss 6.1197e+00 3.8487e+00 1.9747e-02 4.1471e+00 took: 44.97s\n",
      "net1 test acc: 2.464e-01 | net2 test acc: 2.300e-01\n",
      "Epoch  101,  98% \t acc: 2.5253e-01 2.6840e-01 - loss 6.0827e+00 3.8365e+00 1.7862e-02 4.1351e+00 took: 44.98s\n",
      "net1 test acc: 2.272e-01 | net2 test acc: 2.147e-01\n",
      "Epoch  102,  98% \t acc: 2.6407e-01 2.6551e-01 - loss 6.0086e+00 3.7402e+00 2.0650e-02 4.1237e+00 took: 45.39s\n",
      "net1 test acc: 1.907e-01 | net2 test acc: 1.766e-01\n",
      "Epoch  103,  98% \t acc: 2.7561e-01 2.6263e-01 - loss 5.9550e+00 3.7013e+00 2.0062e-02 4.1061e+00 took: 48.42s\n",
      "net1 test acc: 2.255e-01 | net2 test acc: 2.095e-01\n",
      "Epoch  104,  98% \t acc: 2.9582e-01 2.7706e-01 - loss 5.9657e+00 3.6965e+00 2.2246e-02 4.0935e+00 took: 46.44s\n",
      "net1 test acc: 2.345e-01 | net2 test acc: 1.758e-01\n",
      "Epoch  105,  98% \t acc: 2.5974e-01 2.8571e-01 - loss 6.0784e+00 3.7639e+00 2.3892e-02 4.1510e+00 took: 48.51s\n",
      "net1 test acc: 2.237e-01 | net2 test acc: 2.199e-01\n",
      "Epoch  106,  98% \t acc: 2.7128e-01 2.8139e-01 - loss 5.9375e+00 3.6809e+00 2.1858e-02 4.0760e+00 took: 48.05s\n",
      "net1 test acc: 2.280e-01 | net2 test acc: 2.291e-01\n",
      "Epoch  107,  98% \t acc: 2.7128e-01 2.8283e-01 - loss 6.0088e+00 3.7438e+00 2.0820e-02 4.1136e+00 took: 47.60s\n",
      "net1 test acc: 1.423e-01 | net2 test acc: 1.879e-01\n",
      "Epoch  108,  98% \t acc: 2.9437e-01 2.5830e-01 - loss 5.9609e+00 3.7180e+00 1.8197e-02 4.1219e+00 took: 47.82s\n",
      "net1 test acc: 2.386e-01 | net2 test acc: 1.967e-01\n",
      "Epoch  109,  98% \t acc: 2.6263e-01 2.8427e-01 - loss 5.9647e+00 3.6925e+00 2.1450e-02 4.1155e+00 took: 47.71s\n",
      "net1 test acc: 2.255e-01 | net2 test acc: 1.802e-01\n",
      "Epoch  110,  98% \t acc: 2.8716e-01 2.5253e-01 - loss 5.9766e+00 3.6788e+00 2.2495e-02 4.1457e+00 took: 47.23s\n",
      "net1 test acc: 2.281e-01 | net2 test acc: 2.370e-01\n",
      "Epoch  111,  98% \t acc: 2.9004e-01 2.9437e-01 - loss 5.9141e+00 3.6891e+00 1.8943e-02 4.0711e+00 took: 48.12s\n",
      "net1 test acc: 2.308e-01 | net2 test acc: 2.197e-01\n",
      "Epoch  112,  98% \t acc: 2.7850e-01 2.8427e-01 - loss 5.9420e+00 3.7090e+00 2.0206e-02 4.0617e+00 took: 46.36s\n",
      "net1 test acc: 2.236e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  113,  98% \t acc: 2.9149e-01 2.8571e-01 - loss 5.8387e+00 3.6263e+00 1.9140e-02 4.0420e+00 took: 46.25s\n",
      "net1 test acc: 2.304e-01 | net2 test acc: 1.628e-01\n",
      "Epoch  114,  98% \t acc: 2.9870e-01 2.6263e-01 - loss 5.9226e+00 3.7004e+00 1.9514e-02 4.0542e+00 took: 44.26s\n",
      "net1 test acc: 2.326e-01 | net2 test acc: 1.761e-01\n",
      "Epoch  115,  98% \t acc: 2.8139e-01 2.6118e-01 - loss 5.9557e+00 3.7435e+00 1.7801e-02 4.0684e+00 took: 44.51s\n",
      "net1 test acc: 1.471e-01 | net2 test acc: 1.582e-01\n",
      "Epoch  116,  98% \t acc: 2.9726e-01 2.6696e-01 - loss 5.8058e+00 3.6495e+00 1.4110e-02 4.0303e+00 took: 44.05s\n",
      "net1 test acc: 2.356e-01 | net2 test acc: 1.900e-01\n",
      "Epoch  117,  98% \t acc: 2.9870e-01 2.8716e-01 - loss 5.8751e+00 3.6358e+00 1.9626e-02 4.0861e+00 took: 47.10s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 1.742e-01\n",
      "Epoch  118,  98% \t acc: 3.1025e-01 3.1025e-01 - loss 5.7185e+00 3.5160e+00 1.7386e-02 4.0573e+00 took: 45.65s\n",
      "net1 test acc: 2.315e-01 | net2 test acc: 1.744e-01\n",
      "Epoch  119,  98% \t acc: 2.8427e-01 2.8427e-01 - loss 5.8966e+00 3.7014e+00 1.7525e-02 4.0399e+00 took: 44.31s\n",
      "net1 test acc: 2.161e-01 | net2 test acc: 1.707e-01\n",
      "Epoch  120,  98% \t acc: 2.8139e-01 2.9870e-01 - loss 5.9153e+00 3.6708e+00 2.1508e-02 4.0589e+00 took: 44.59s\n",
      "net1 test acc: 2.177e-01 | net2 test acc: 1.780e-01\n",
      "Epoch  121,  98% \t acc: 2.9293e-01 2.6263e-01 - loss 5.9625e+00 3.6717e+00 2.0637e-02 4.1688e+00 took: 44.60s\n",
      "net1 test acc: 2.210e-01 | net2 test acc: 2.188e-01\n",
      "Epoch  122,  98% \t acc: 2.9149e-01 2.9726e-01 - loss 5.8681e+00 3.6342e+00 1.9481e-02 4.0783e+00 took: 44.37s\n",
      "net1 test acc: 2.316e-01 | net2 test acc: 2.308e-01\n",
      "Epoch  123,  98% \t acc: 3.2468e-01 3.0736e-01 - loss 5.6522e+00 3.4642e+00 1.7634e-02 4.0234e+00 took: 43.85s\n",
      "net1 test acc: 2.017e-01 | net2 test acc: 2.161e-01\n",
      "Epoch  124,  98% \t acc: 3.0303e-01 2.9437e-01 - loss 5.8277e+00 3.6175e+00 1.8476e-02 4.0510e+00 took: 43.66s\n",
      "net1 test acc: 2.307e-01 | net2 test acc: 1.791e-01\n",
      "Epoch  125,  98% \t acc: 3.1746e-01 2.9870e-01 - loss 5.7773e+00 3.5759e+00 1.7812e-02 4.0466e+00 took: 44.26s\n",
      "net1 test acc: 2.207e-01 | net2 test acc: 1.531e-01\n",
      "Epoch  126,  98% \t acc: 3.0303e-01 2.9004e-01 - loss 5.9494e+00 3.6757e+00 2.2447e-02 4.0985e+00 took: 43.73s\n",
      "net1 test acc: 2.232e-01 | net2 test acc: 2.344e-01\n",
      "Epoch  127,  98% \t acc: 3.0014e-01 3.0880e-01 - loss 5.7697e+00 3.5579e+00 1.8527e-02 4.0530e+00 took: 43.77s\n",
      "net1 test acc: 2.281e-01 | net2 test acc: 2.459e-01\n",
      "Epoch  128,  98% \t acc: 2.8716e-01 2.9726e-01 - loss 5.8716e+00 3.5877e+00 2.4540e-02 4.0770e+00 took: 47.87s\n",
      "net1 test acc: 1.720e-01 | net2 test acc: 1.788e-01\n",
      "Epoch  129,  98% \t acc: 3.1313e-01 3.1602e-01 - loss 5.7454e+00 3.5446e+00 1.9418e-02 4.0132e+00 took: 47.63s\n",
      "net1 test acc: 2.274e-01 | net2 test acc: 2.199e-01\n",
      "Epoch  130,  98% \t acc: 3.0880e-01 3.2323e-01 - loss 5.7453e+00 3.5001e+00 2.2009e-02 4.0501e+00 took: 47.42s\n",
      "net1 test acc: 2.204e-01 | net2 test acc: 2.037e-01\n",
      "Epoch  131,  98% \t acc: 2.9726e-01 2.9004e-01 - loss 5.9864e+00 3.7162e+00 2.0542e-02 4.1296e+00 took: 47.85s\n",
      "net1 test acc: 2.286e-01 | net2 test acc: 1.848e-01\n",
      "Epoch  132,  98% \t acc: 3.1025e-01 3.1457e-01 - loss 5.7639e+00 3.5588e+00 1.7380e-02 4.0626e+00 took: 44.72s\n",
      "net1 test acc: 2.181e-01 | net2 test acc: 2.204e-01\n",
      "Epoch  133,  98% \t acc: 3.1890e-01 3.0159e-01 - loss 5.6847e+00 3.4926e+00 1.8037e-02 4.0237e+00 took: 48.93s\n",
      "net1 test acc: 2.200e-01 | net2 test acc: 2.237e-01\n",
      "Epoch  134,  98% \t acc: 3.3766e-01 3.2756e-01 - loss 5.6898e+00 3.4615e+00 2.0592e-02 4.0448e+00 took: 44.77s\n",
      "net1 test acc: 2.246e-01 | net2 test acc: 2.087e-01\n",
      "Epoch  135,  98% \t acc: 3.2323e-01 3.2323e-01 - loss 5.6387e+00 3.4990e+00 1.4253e-02 3.9942e+00 took: 45.42s\n",
      "net1 test acc: 2.264e-01 | net2 test acc: 1.759e-01\n",
      "Epoch  136,  98% \t acc: 3.3189e-01 3.2323e-01 - loss 5.5635e+00 3.3873e+00 1.8271e-02 3.9869e+00 took: 45.44s\n",
      "net1 test acc: 2.283e-01 | net2 test acc: 2.104e-01\n",
      "Epoch  137,  98% \t acc: 3.3622e-01 3.1890e-01 - loss 5.6098e+00 3.4441e+00 1.7601e-02 3.9794e+00 took: 46.47s\n",
      "net1 test acc: 2.255e-01 | net2 test acc: 1.679e-01\n",
      "Epoch  138,  98% \t acc: 3.1169e-01 2.9582e-01 - loss 5.7538e+00 3.5629e+00 1.8254e-02 4.0166e+00 took: 47.28s\n",
      "net1 test acc: 2.126e-01 | net2 test acc: 1.699e-01\n",
      "Epoch  139,  98% \t acc: 3.0303e-01 3.0736e-01 - loss 5.8200e+00 3.6350e+00 1.7827e-02 4.0135e+00 took: 45.67s\n",
      "net1 test acc: 2.262e-01 | net2 test acc: 2.415e-01\n",
      "Epoch  140,  98% \t acc: 3.2612e-01 2.8716e-01 - loss 5.8781e+00 3.6473e+00 2.1410e-02 4.0334e+00 took: 45.89s\n",
      "net1 test acc: 2.262e-01 | net2 test acc: 2.240e-01\n",
      "Epoch  141,  98% \t acc: 3.0736e-01 3.3045e-01 - loss 5.6806e+00 3.4795e+00 1.9426e-02 4.0136e+00 took: 44.99s\n",
      "net1 test acc: 2.285e-01 | net2 test acc: 2.285e-01\n",
      "Epoch  142,  98% \t acc: 3.2035e-01 3.2035e-01 - loss 5.6302e+00 3.4383e+00 1.9332e-02 3.9972e+00 took: 45.56s\n",
      "net1 test acc: 2.505e-01 | net2 test acc: 2.323e-01\n",
      "Epoch  143,  98% \t acc: 3.0592e-01 3.2900e-01 - loss 5.6882e+00 3.4906e+00 1.9772e-02 3.9997e+00 took: 45.49s\n",
      "net1 test acc: 1.501e-01 | net2 test acc: 1.579e-01\n",
      "Epoch  144,  98% \t acc: 3.3478e-01 3.5354e-01 - loss 5.6116e+00 3.4238e+00 2.1144e-02 3.9527e+00 took: 48.46s\n",
      "net1 test acc: 2.155e-01 | net2 test acc: 2.147e-01\n",
      "Epoch  145,  98% \t acc: 3.1025e-01 3.3189e-01 - loss 5.7858e+00 3.5564e+00 2.0907e-02 4.0407e+00 took: 45.64s\n",
      "net1 test acc: 2.541e-01 | net2 test acc: 2.389e-01\n",
      "Epoch  146,  98% \t acc: 2.8283e-01 3.2179e-01 - loss 5.7823e+00 3.5751e+00 1.9772e-02 4.0190e+00 took: 45.06s\n",
      "net1 test acc: 2.095e-01 | net2 test acc: 1.855e-01\n",
      "Epoch  147,  98% \t acc: 3.5354e-01 3.2179e-01 - loss 5.6337e+00 3.4268e+00 2.0560e-02 4.0026e+00 took: 45.14s\n",
      "net1 test acc: 2.289e-01 | net2 test acc: 1.669e-01\n",
      "Epoch  148,  98% \t acc: 3.1746e-01 3.2612e-01 - loss 5.7039e+00 3.4922e+00 2.1535e-02 3.9928e+00 took: 45.75s\n",
      "net1 test acc: 2.370e-01 | net2 test acc: 2.370e-01\n",
      "Epoch  149,  98% \t acc: 3.2900e-01 3.2612e-01 - loss 5.7516e+00 3.5135e+00 2.2144e-02 4.0333e+00 took: 48.11s\n",
      "net1 test acc: 2.460e-01 | net2 test acc: 2.297e-01\n",
      "Epoch  150,  98% \t acc: 3.0447e-01 3.3622e-01 - loss 5.6924e+00 3.4898e+00 2.0282e-02 3.9995e+00 took: 47.40s\n",
      "net1 test acc: 2.319e-01 | net2 test acc: 1.777e-01\n",
      "Epoch  151,  98% \t acc: 3.1313e-01 3.3911e-01 - loss 5.6607e+00 3.4904e+00 1.8189e-02 3.9768e+00 took: 47.76s\n",
      "net1 test acc: 2.136e-01 | net2 test acc: 1.787e-01\n",
      "Epoch  152,  98% \t acc: 3.2756e-01 3.5931e-01 - loss 5.5535e+00 3.4111e+00 1.7522e-02 3.9345e+00 took: 47.97s\n",
      "net1 test acc: 2.221e-01 | net2 test acc: 1.750e-01\n",
      "Epoch  153,  98% \t acc: 3.3622e-01 3.6508e-01 - loss 5.5520e+00 3.3703e+00 2.1147e-02 3.9404e+00 took: 47.92s\n",
      "net1 test acc: 2.237e-01 | net2 test acc: 2.434e-01\n",
      "Epoch  154,  98% \t acc: 3.5065e-01 3.4199e-01 - loss 5.5818e+00 3.4231e+00 1.9216e-02 3.9331e+00 took: 45.82s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 2.299e-01\n",
      "Epoch  155,  98% \t acc: 3.5498e-01 3.4343e-01 - loss 5.5235e+00 3.3641e+00 1.8189e-02 3.9549e+00 took: 48.02s\n",
      "net1 test acc: 2.294e-01 | net2 test acc: 1.775e-01\n",
      "Epoch  156,  98% \t acc: 3.3766e-01 3.4921e-01 - loss 5.5502e+00 3.4064e+00 1.7907e-02 3.9294e+00 took: 47.92s\n",
      "net1 test acc: 2.180e-01 | net2 test acc: 2.280e-01\n",
      "Epoch  157,  98% \t acc: 3.3622e-01 3.5498e-01 - loss 5.5454e+00 3.3841e+00 1.8285e-02 3.9570e+00 took: 47.95s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 2.191e-01\n",
      "Epoch  158,  98% \t acc: 3.3189e-01 3.3045e-01 - loss 5.8037e+00 3.5599e+00 2.3544e-02 4.0167e+00 took: 46.45s\n",
      "net1 test acc: 2.272e-01 | net2 test acc: 1.542e-01\n",
      "Epoch  159,  98% \t acc: 3.2900e-01 3.4343e-01 - loss 5.7841e+00 3.5687e+00 2.0384e-02 4.0230e+00 took: 45.47s\n",
      "net1 test acc: 2.345e-01 | net2 test acc: 2.144e-01\n",
      "Epoch  160,  98% \t acc: 3.3622e-01 3.3911e-01 - loss 5.7253e+00 3.5152e+00 1.9423e-02 4.0317e+00 took: 45.51s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 2.196e-01\n",
      "Epoch  161,  98% \t acc: 3.4343e-01 3.3766e-01 - loss 5.5274e+00 3.4167e+00 1.4816e-02 3.9250e+00 took: 47.64s\n",
      "net1 test acc: 2.128e-01 | net2 test acc: 2.269e-01\n",
      "Epoch  162,  98% \t acc: 3.4343e-01 3.4343e-01 - loss 5.6070e+00 3.4215e+00 2.0885e-02 3.9532e+00 took: 45.75s\n",
      "net1 test acc: 2.375e-01 | net2 test acc: 2.423e-01\n",
      "Epoch  163,  98% \t acc: 3.4199e-01 3.7085e-01 - loss 5.4873e+00 3.3432e+00 1.9369e-02 3.9010e+00 took: 48.08s\n",
      "net1 test acc: 2.371e-01 | net2 test acc: 2.430e-01\n",
      "Epoch  164,  98% \t acc: 3.5209e-01 3.5642e-01 - loss 5.5144e+00 3.3867e+00 1.7785e-02 3.8998e+00 took: 47.21s\n",
      "net1 test acc: 2.125e-01 | net2 test acc: 2.265e-01\n",
      "Epoch  165,  98% \t acc: 3.4776e-01 3.4343e-01 - loss 5.4719e+00 3.3416e+00 1.8487e-02 3.8909e+00 took: 48.05s\n",
      "net1 test acc: 2.125e-01 | net2 test acc: 2.106e-01\n",
      "Epoch  166,  98% \t acc: 3.5354e-01 3.4055e-01 - loss 5.4970e+00 3.3883e+00 1.7633e-02 3.8647e+00 took: 48.02s\n",
      "net1 test acc: 2.548e-01 | net2 test acc: 2.292e-01\n",
      "Epoch  167,  98% \t acc: 3.5065e-01 3.4488e-01 - loss 5.4869e+00 3.3602e+00 1.8371e-02 3.8859e+00 took: 48.56s\n",
      "net1 test acc: 2.250e-01 | net2 test acc: 2.139e-01\n",
      "Epoch  168,  98% \t acc: 3.4632e-01 3.4921e-01 - loss 5.4388e+00 3.3298e+00 1.7711e-02 3.8638e+00 took: 48.12s\n",
      "net1 test acc: 2.207e-01 | net2 test acc: 2.240e-01\n",
      "Epoch  169,  98% \t acc: 3.6075e-01 3.8095e-01 - loss 5.3547e+00 3.2624e+00 1.6793e-02 3.8487e+00 took: 47.78s\n",
      "net1 test acc: 1.777e-01 | net2 test acc: 1.772e-01\n",
      "Epoch  170,  98% \t acc: 3.5931e-01 3.6508e-01 - loss 5.3754e+00 3.2638e+00 1.7833e-02 3.8664e+00 took: 48.25s\n",
      "net1 test acc: 2.360e-01 | net2 test acc: 2.349e-01\n",
      "Epoch  171,  98% \t acc: 3.4343e-01 3.4921e-01 - loss 5.7135e+00 3.4764e+00 2.4036e-02 3.9934e+00 took: 46.12s\n",
      "net1 test acc: 2.535e-01 | net2 test acc: 2.237e-01\n",
      "Epoch  172,  98% \t acc: 3.3478e-01 3.4632e-01 - loss 5.6511e+00 3.4247e+00 2.3130e-02 3.9901e+00 took: 48.19s\n",
      "net1 test acc: 2.553e-01 | net2 test acc: 1.569e-01\n",
      "Epoch  173,  98% \t acc: 3.4921e-01 3.3045e-01 - loss 5.5239e+00 3.3899e+00 1.8515e-02 3.8978e+00 took: 43.84s\n",
      "net1 test acc: 2.306e-01 | net2 test acc: 2.480e-01\n",
      "Epoch  174,  98% \t acc: 3.2323e-01 3.5354e-01 - loss 5.5036e+00 3.3833e+00 1.7315e-02 3.8943e+00 took: 46.62s\n",
      "net1 test acc: 2.335e-01 | net2 test acc: 2.368e-01\n",
      "Epoch  175,  98% \t acc: 3.4632e-01 3.6219e-01 - loss 5.4640e+00 3.3194e+00 1.8910e-02 3.9109e+00 took: 46.18s\n",
      "net1 test acc: 2.527e-01 | net2 test acc: 2.405e-01\n",
      "Epoch  176,  98% \t acc: 3.6508e-01 3.5354e-01 - loss 5.4191e+00 3.2901e+00 1.9626e-02 3.8654e+00 took: 48.71s\n",
      "net1 test acc: 2.214e-01 | net2 test acc: 1.627e-01\n",
      "Epoch  177,  98% \t acc: 3.3189e-01 3.5354e-01 - loss 5.5151e+00 3.3853e+00 1.8154e-02 3.8966e+00 took: 48.46s\n",
      "net1 test acc: 1.993e-01 | net2 test acc: 2.174e-01\n",
      "Epoch  178,  98% \t acc: 3.2323e-01 3.6508e-01 - loss 5.5446e+00 3.3645e+00 2.2191e-02 3.9164e+00 took: 48.16s\n",
      "net1 test acc: 2.264e-01 | net2 test acc: 2.440e-01\n",
      "Epoch  179,  98% \t acc: 3.2612e-01 3.6508e-01 - loss 5.5459e+00 3.3633e+00 2.1300e-02 3.9392e+00 took: 47.79s\n",
      "net1 test acc: 2.144e-01 | net2 test acc: 2.172e-01\n",
      "Epoch  180,  98% \t acc: 3.4343e-01 3.5498e-01 - loss 5.5188e+00 3.3390e+00 2.1262e-02 3.9342e+00 took: 48.24s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 1.680e-01\n",
      "Epoch  181,  98% \t acc: 3.6219e-01 3.7951e-01 - loss 5.3759e+00 3.2762e+00 1.6977e-02 3.8599e+00 took: 46.54s\n",
      "net1 test acc: 2.077e-01 | net2 test acc: 2.207e-01\n",
      "Epoch  182,  98% \t acc: 3.6652e-01 3.7807e-01 - loss 5.4107e+00 3.2607e+00 2.0185e-02 3.8963e+00 took: 46.65s\n",
      "net1 test acc: 2.245e-01 | net2 test acc: 2.445e-01\n",
      "Epoch  183,  98% \t acc: 3.4776e-01 3.4632e-01 - loss 5.4380e+00 3.3641e+00 1.6230e-02 3.8232e+00 took: 48.28s\n",
      "net1 test acc: 2.316e-01 | net2 test acc: 2.283e-01\n",
      "Epoch  184,  98% \t acc: 3.5498e-01 3.8240e-01 - loss 5.3940e+00 3.2892e+00 1.7369e-02 3.8621e+00 took: 49.12s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 1.745e-01\n",
      "Epoch  185,  98% \t acc: 3.4632e-01 3.5931e-01 - loss 5.4761e+00 3.3346e+00 1.8697e-02 3.9091e+00 took: 48.20s\n",
      "net1 test acc: 2.217e-01 | net2 test acc: 2.087e-01\n",
      "Epoch  186,  98% \t acc: 3.4199e-01 3.8961e-01 - loss 5.4421e+00 3.2904e+00 2.0037e-02 3.9028e+00 took: 47.85s\n",
      "net1 test acc: 2.119e-01 | net2 test acc: 2.227e-01\n",
      "Epoch  187,  98% \t acc: 3.5065e-01 3.7951e-01 - loss 5.4772e+00 3.3450e+00 1.9907e-02 3.8663e+00 took: 47.82s\n",
      "net1 test acc: 2.378e-01 | net2 test acc: 2.192e-01\n",
      "Epoch  188,  98% \t acc: 3.3045e-01 3.8240e-01 - loss 5.4600e+00 3.3182e+00 1.9748e-02 3.8886e+00 took: 47.23s\n",
      "net1 test acc: 1.826e-01 | net2 test acc: 1.729e-01\n",
      "Epoch  189,  98% \t acc: 3.7374e-01 3.5498e-01 - loss 5.3700e+00 3.2811e+00 1.7117e-02 3.8353e+00 took: 45.18s\n",
      "net1 test acc: 2.180e-01 | net2 test acc: 1.750e-01\n",
      "Epoch  190,  98% \t acc: 3.7662e-01 3.7662e-01 - loss 5.3316e+00 3.2414e+00 1.6769e-02 3.8450e+00 took: 46.69s\n",
      "net1 test acc: 1.583e-01 | net2 test acc: 1.658e-01\n",
      "Epoch  191,  98% \t acc: 3.7229e-01 4.0260e-01 - loss 5.4160e+00 3.2693e+00 2.0264e-02 3.8882e+00 took: 45.40s\n",
      "net1 test acc: 2.348e-01 | net2 test acc: 2.245e-01\n",
      "Epoch  192,  98% \t acc: 3.8095e-01 4.0693e-01 - loss 5.2912e+00 3.2054e+00 1.9352e-02 3.7846e+00 took: 45.15s\n",
      "net1 test acc: 2.413e-01 | net2 test acc: 2.205e-01\n",
      "Epoch  193,  98% \t acc: 3.6941e-01 4.2136e-01 - loss 5.1719e+00 3.1172e+00 1.7376e-02 3.7619e+00 took: 45.63s\n",
      "net1 test acc: 2.371e-01 | net2 test acc: 2.327e-01\n",
      "Epoch  194,  98% \t acc: 3.8961e-01 4.1270e-01 - loss 5.1677e+00 3.1176e+00 1.8093e-02 3.7384e+00 took: 45.20s\n",
      "net1 test acc: 2.353e-01 | net2 test acc: 1.729e-01\n",
      "Epoch  195,  98% \t acc: 3.8240e-01 3.6941e-01 - loss 5.3491e+00 3.2670e+00 1.8594e-02 3.7923e+00 took: 47.23s\n",
      "net1 test acc: 2.332e-01 | net2 test acc: 2.138e-01\n",
      "Epoch  196,  98% \t acc: 3.8095e-01 4.0115e-01 - loss 5.3144e+00 3.2482e+00 1.8811e-02 3.7561e+00 took: 47.20s\n",
      "net1 test acc: 2.210e-01 | net2 test acc: 2.021e-01\n",
      "Epoch  197,  98% \t acc: 3.7518e-01 3.9250e-01 - loss 5.3339e+00 3.2685e+00 1.7630e-02 3.7783e+00 took: 49.06s\n",
      "net1 test acc: 2.268e-01 | net2 test acc: 2.246e-01\n",
      "Epoch  198,  98% \t acc: 3.9105e-01 3.7374e-01 - loss 5.2436e+00 3.1786e+00 1.7739e-02 3.7752e+00 took: 45.78s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.255e-01\n",
      "Epoch  199,  98% \t acc: 3.6219e-01 3.9250e-01 - loss 5.2867e+00 3.2197e+00 1.8742e-02 3.7591e+00 took: 48.43s\n",
      "net1 test acc: 2.149e-01 | net2 test acc: 2.145e-01\n",
      "Epoch  200,  98% \t acc: 3.9971e-01 4.0404e-01 - loss 5.2131e+00 3.1769e+00 1.6920e-02 3.7340e+00 took: 45.62s\n",
      "net1 test acc: 2.185e-01 | net2 test acc: 1.651e-01\n",
      "Epoch  201,  98% \t acc: 3.8961e-01 3.9538e-01 - loss 5.2357e+00 3.2096e+00 1.6814e-02 3.7158e+00 took: 47.21s\n",
      "net1 test acc: 2.020e-01 | net2 test acc: 2.214e-01\n",
      "Epoch  202,  98% \t acc: 3.9394e-01 3.9827e-01 - loss 5.1803e+00 3.1480e+00 1.7447e-02 3.7158e+00 took: 45.75s\n",
      "net1 test acc: 2.020e-01 | net2 test acc: 2.261e-01\n",
      "Epoch  203,  98% \t acc: 3.9250e-01 3.9827e-01 - loss 5.2911e+00 3.2303e+00 2.0120e-02 3.7191e+00 took: 46.67s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.095e-01\n",
      "Epoch  204,  98% \t acc: 3.8817e-01 4.0260e-01 - loss 5.1663e+00 3.1692e+00 1.7049e-02 3.6533e+00 took: 45.48s\n",
      "net1 test acc: 2.020e-01 | net2 test acc: 2.061e-01\n",
      "Epoch  205,  98% \t acc: 3.9538e-01 4.1270e-01 - loss 5.1319e+00 3.1381e+00 1.7111e-02 3.6454e+00 took: 47.04s\n",
      "net1 test acc: 2.308e-01 | net2 test acc: 2.260e-01\n",
      "Epoch  206,  98% \t acc: 3.8528e-01 4.1126e-01 - loss 5.1889e+00 3.1719e+00 1.8520e-02 3.6635e+00 took: 45.82s\n",
      "net1 test acc: 2.281e-01 | net2 test acc: 2.185e-01\n",
      "Epoch  207,  98% \t acc: 3.6941e-01 4.0548e-01 - loss 5.2084e+00 3.2202e+00 1.7879e-02 3.6188e+00 took: 46.04s\n",
      "net1 test acc: 2.367e-01 | net2 test acc: 2.259e-01\n",
      "Epoch  208,  98% \t acc: 4.0115e-01 3.6941e-01 - loss 5.3680e+00 3.2888e+00 2.2056e-02 3.7173e+00 took: 45.62s\n",
      "net1 test acc: 2.266e-01 | net2 test acc: 2.244e-01\n",
      "Epoch  209,  98% \t acc: 3.9971e-01 3.9250e-01 - loss 5.2640e+00 3.2369e+00 2.0078e-02 3.6527e+00 took: 45.31s\n",
      "net1 test acc: 2.158e-01 | net2 test acc: 2.132e-01\n",
      "Epoch  210,  98% \t acc: 3.7807e-01 4.0693e-01 - loss 5.1977e+00 3.1971e+00 1.7859e-02 3.6440e+00 took: 45.60s\n",
      "net1 test acc: 2.300e-01 | net2 test acc: 2.162e-01\n",
      "Epoch  211,  98% \t acc: 3.6364e-01 4.1847e-01 - loss 5.1900e+00 3.1748e+00 1.8689e-02 3.6566e+00 took: 45.38s\n",
      "net1 test acc: 2.012e-01 | net2 test acc: 1.680e-01\n",
      "Epoch  212,  98% \t acc: 3.8095e-01 4.1703e-01 - loss 5.1909e+00 3.1723e+00 1.8217e-02 3.6728e+00 took: 46.49s\n",
      "net1 test acc: 2.315e-01 | net2 test acc: 2.337e-01\n",
      "Epoch  213,  98% \t acc: 3.9250e-01 4.1126e-01 - loss 5.1690e+00 3.1715e+00 1.8872e-02 3.6176e+00 took: 45.25s\n",
      "net1 test acc: 2.126e-01 | net2 test acc: 2.200e-01\n",
      "Epoch  214,  98% \t acc: 3.7518e-01 4.2713e-01 - loss 5.1080e+00 3.1300e+00 1.7528e-02 3.6054e+00 took: 47.48s\n",
      "net1 test acc: 2.070e-01 | net2 test acc: 1.647e-01\n",
      "Epoch  215,  98% \t acc: 4.0260e-01 4.0260e-01 - loss 5.0492e+00 3.0842e+00 1.5802e-02 3.6139e+00 took: 51.80s\n",
      "net1 test acc: 2.021e-01 | net2 test acc: 2.174e-01\n",
      "Epoch  216,  98% \t acc: 3.8240e-01 4.1414e-01 - loss 5.1816e+00 3.1600e+00 1.9041e-02 3.6624e+00 took: 50.25s\n",
      "net1 test acc: 2.257e-01 | net2 test acc: 1.838e-01\n",
      "Epoch  217,  98% \t acc: 4.0693e-01 4.2280e-01 - loss 5.0907e+00 3.1265e+00 1.7023e-02 3.5879e+00 took: 47.70s\n",
      "net1 test acc: 2.319e-01 | net2 test acc: 2.405e-01\n",
      "Epoch  218,  98% \t acc: 4.0693e-01 4.1558e-01 - loss 5.1544e+00 3.1505e+00 2.0610e-02 3.5956e+00 took: 47.45s\n",
      "net1 test acc: 2.118e-01 | net2 test acc: 2.285e-01\n",
      "Epoch  219,  98% \t acc: 4.2280e-01 4.2713e-01 - loss 4.9934e+00 3.0784e+00 1.6833e-02 3.4934e+00 took: 48.35s\n",
      "net1 test acc: 2.032e-01 | net2 test acc: 2.132e-01\n",
      "Epoch  220,  98% \t acc: 4.3290e-01 4.1414e-01 - loss 4.9579e+00 3.0930e+00 1.6464e-02 3.4006e+00 took: 45.21s\n",
      "net1 test acc: 2.229e-01 | net2 test acc: 2.318e-01\n",
      "Epoch  221,  98% \t acc: 4.2136e-01 3.8961e-01 - loss 5.0737e+00 3.2266e+00 1.7537e-02 3.3434e+00 took: 48.45s\n",
      "net1 test acc: 2.055e-01 | net2 test acc: 2.162e-01\n",
      "Epoch  222,  98% \t acc: 3.6652e-01 4.1414e-01 - loss 5.1395e+00 3.2183e+00 1.9773e-02 3.4469e+00 took: 44.99s\n",
      "net1 test acc: 2.253e-01 | net2 test acc: 2.065e-01\n",
      "Epoch  223,  98% \t acc: 3.9971e-01 3.7807e-01 - loss 5.1628e+00 3.2599e+00 1.7421e-02 3.4574e+00 took: 44.62s\n",
      "net1 test acc: 2.254e-01 | net2 test acc: 2.265e-01\n",
      "Epoch  224,  98% \t acc: 4.0837e-01 4.1270e-01 - loss 5.0550e+00 3.1479e+00 1.7479e-02 3.4646e+00 took: 45.10s\n",
      "net1 test acc: 2.091e-01 | net2 test acc: 2.069e-01\n",
      "Epoch  225,  98% \t acc: 4.2280e-01 4.3001e-01 - loss 5.0223e+00 3.1525e+00 1.6317e-02 3.4134e+00 took: 44.76s\n",
      "net1 test acc: 2.145e-01 | net2 test acc: 2.175e-01\n",
      "Epoch  226,  98% \t acc: 4.3434e-01 4.1126e-01 - loss 4.9771e+00 3.1655e+00 1.5871e-02 3.3058e+00 took: 44.87s\n",
      "net1 test acc: 2.083e-01 | net2 test acc: 2.095e-01\n",
      "Epoch  227,  98% \t acc: 4.3434e-01 4.4589e-01 - loss 4.8864e+00 3.0920e+00 2.0426e-02 3.1802e+00 took: 44.58s\n",
      "net1 test acc: 2.025e-01 | net2 test acc: 2.002e-01\n",
      "Epoch  228,  98% \t acc: 4.2857e-01 4.2569e-01 - loss 4.9995e+00 3.2283e+00 1.9524e-02 3.1518e+00 took: 45.77s\n",
      "net1 test acc: 2.132e-01 | net2 test acc: 2.121e-01\n",
      "Epoch  229,  98% \t acc: 4.1558e-01 4.3001e-01 - loss 5.0395e+00 3.2847e+00 2.0166e-02 3.1062e+00 took: 48.77s\n",
      "net1 test acc: 2.215e-01 | net2 test acc: 2.192e-01\n",
      "Epoch  230,  98% \t acc: 4.1703e-01 4.3723e-01 - loss 5.0097e+00 3.2508e+00 1.8987e-02 3.1380e+00 took: 44.38s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 2.204e-01\n",
      "Epoch  231,  98% \t acc: 4.1126e-01 3.9394e-01 - loss 5.1343e+00 3.3469e+00 1.9492e-02 3.1850e+00 took: 45.46s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.266e-01\n",
      "Epoch  232,  98% \t acc: 4.1270e-01 4.1991e-01 - loss 5.0195e+00 3.2089e+00 1.9770e-02 3.2259e+00 took: 48.63s\n",
      "net1 test acc: 2.255e-01 | net2 test acc: 2.232e-01\n",
      "Epoch  233,  98% \t acc: 4.1703e-01 4.3723e-01 - loss 4.9942e+00 3.2301e+00 1.8876e-02 3.1507e+00 took: 50.90s\n",
      "net1 test acc: 1.704e-01 | net2 test acc: 2.200e-01\n",
      "Epoch  234,  98% \t acc: 4.1991e-01 4.2713e-01 - loss 5.0168e+00 3.2647e+00 1.9243e-02 3.1193e+00 took: 49.65s\n",
      "net1 test acc: 2.221e-01 | net2 test acc: 2.174e-01\n",
      "Epoch  235,  98% \t acc: 4.3867e-01 4.1847e-01 - loss 5.0086e+00 3.2766e+00 1.8987e-02 3.0843e+00 took: 44.87s\n",
      "net1 test acc: 2.183e-01 | net2 test acc: 2.199e-01\n",
      "Epoch  236,  98% \t acc: 4.2136e-01 4.2424e-01 - loss 4.9748e+00 3.2040e+00 2.0337e-02 3.1349e+00 took: 48.69s\n",
      "net1 test acc: 2.313e-01 | net2 test acc: 2.268e-01\n",
      "Epoch  237,  98% \t acc: 4.2280e-01 4.0981e-01 - loss 5.0911e+00 3.2822e+00 2.2504e-02 3.1678e+00 took: 48.38s\n",
      "net1 test acc: 2.166e-01 | net2 test acc: 1.807e-01\n",
      "Epoch  238,  98% \t acc: 4.4012e-01 4.0548e-01 - loss 5.0503e+00 3.2784e+00 2.0269e-02 3.1384e+00 took: 45.21s\n",
      "net1 test acc: 2.180e-01 | net2 test acc: 2.036e-01\n",
      "Epoch  239,  98% \t acc: 4.2857e-01 3.8961e-01 - loss 5.0915e+00 3.3016e+00 2.0949e-02 3.1609e+00 took: 46.41s\n",
      "net1 test acc: 2.161e-01 | net2 test acc: 2.236e-01\n",
      "Epoch  240,  98% \t acc: 4.3001e-01 4.0260e-01 - loss 5.0084e+00 3.2555e+00 1.9684e-02 3.1122e+00 took: 44.12s\n",
      "net1 test acc: 2.106e-01 | net2 test acc: 2.139e-01\n",
      "Epoch  241,  98% \t acc: 4.3290e-01 4.2424e-01 - loss 4.9259e+00 3.2442e+00 1.5755e-02 3.0481e+00 took: 44.56s\n",
      "net1 test acc: 2.167e-01 | net2 test acc: 2.100e-01\n",
      "Epoch  242,  98% \t acc: 4.4589e-01 4.2424e-01 - loss 4.9924e+00 3.2870e+00 1.8704e-02 3.0367e+00 took: 47.80s\n",
      "net1 test acc: 2.181e-01 | net2 test acc: 2.118e-01\n",
      "Epoch  243,  98% \t acc: 4.4012e-01 4.1847e-01 - loss 4.8893e+00 3.2700e+00 1.7203e-02 2.8945e+00 took: 44.96s\n",
      "net1 test acc: 2.186e-01 | net2 test acc: 1.637e-01\n",
      "Epoch  244,  98% \t acc: 4.2857e-01 4.2136e-01 - loss 5.0142e+00 3.2745e+00 1.9460e-02 3.0902e+00 took: 46.37s\n",
      "net1 test acc: 2.226e-01 | net2 test acc: 2.192e-01\n",
      "Epoch  245,  98% \t acc: 4.0260e-01 4.4444e-01 - loss 4.9652e+00 3.2546e+00 1.8484e-02 3.0517e+00 took: 44.06s\n",
      "net1 test acc: 2.329e-01 | net2 test acc: 2.151e-01\n",
      "Epoch  246,  98% \t acc: 4.3579e-01 4.3723e-01 - loss 4.9618e+00 3.2516e+00 1.9369e-02 3.0330e+00 took: 44.43s\n",
      "net1 test acc: 2.070e-01 | net2 test acc: 2.059e-01\n",
      "Epoch  247,  98% \t acc: 4.2136e-01 4.3001e-01 - loss 4.9238e+00 3.2498e+00 1.8115e-02 2.9856e+00 took: 44.69s\n",
      "net1 test acc: 2.289e-01 | net2 test acc: 1.666e-01\n",
      "Epoch  248,  98% \t acc: 4.5022e-01 4.2857e-01 - loss 4.8674e+00 3.2204e+00 1.8044e-02 2.9331e+00 took: 44.44s\n",
      "net1 test acc: 1.990e-01 | net2 test acc: 2.120e-01\n",
      "Epoch  249,  98% \t acc: 4.4300e-01 3.9683e-01 - loss 5.0075e+00 3.3196e+00 1.8491e-02 3.0059e+00 took: 44.25s\n",
      "net1 test acc: 2.227e-01 | net2 test acc: 2.405e-01\n",
      "Epoch  250,  98% \t acc: 4.5599e-01 4.5455e-01 - loss 4.6902e+00 3.0556e+00 1.6285e-02 2.9434e+00 took: 43.75s\n",
      "net1 test acc: 2.167e-01 | net2 test acc: 2.137e-01\n",
      "Epoch  251,  98% \t acc: 4.5455e-01 4.5743e-01 - loss 4.7509e+00 3.1833e+00 1.7628e-02 2.7827e+00 took: 44.20s\n",
      "net1 test acc: 2.081e-01 | net2 test acc: 2.096e-01\n",
      "Epoch  252,  98% \t acc: 4.7042e-01 4.6609e-01 - loss 4.6819e+00 3.1560e+00 1.6424e-02 2.7234e+00 took: 45.96s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.177e-01\n",
      "Epoch  253,  98% \t acc: 4.5022e-01 4.4300e-01 - loss 4.9175e+00 3.3302e+00 1.8682e-02 2.8010e+00 took: 48.25s\n",
      "net1 test acc: 1.999e-01 | net2 test acc: 2.204e-01\n",
      "Epoch  254,  98% \t acc: 4.7042e-01 4.2569e-01 - loss 4.8438e+00 3.2235e+00 1.7694e-02 2.8867e+00 took: 46.78s\n",
      "net1 test acc: 2.177e-01 | net2 test acc: 2.121e-01\n",
      "Epoch  255,  98% \t acc: 4.4012e-01 4.3434e-01 - loss 4.9042e+00 3.3037e+00 2.0414e-02 2.7928e+00 took: 46.02s\n",
      "net1 test acc: 2.025e-01 | net2 test acc: 2.091e-01\n",
      "Epoch  256,  98% \t acc: 4.6320e-01 4.4444e-01 - loss 4.8147e+00 3.2255e+00 1.7997e-02 2.8184e+00 took: 44.38s\n",
      "net1 test acc: 2.028e-01 | net2 test acc: 2.083e-01\n",
      "Epoch  257,  98% \t acc: 4.8196e-01 4.0981e-01 - loss 4.8799e+00 3.2569e+00 1.8344e-02 2.8792e+00 took: 45.01s\n",
      "net1 test acc: 2.092e-01 | net2 test acc: 2.007e-01\n",
      "Epoch  258,  98% \t acc: 4.6176e-01 4.3867e-01 - loss 4.8153e+00 3.2517e+00 1.7431e-02 2.7787e+00 took: 44.68s\n",
      "net1 test acc: 2.055e-01 | net2 test acc: 2.166e-01\n",
      "Epoch  259,  98% \t acc: 4.6465e-01 4.3001e-01 - loss 4.9055e+00 3.2454e+00 1.9490e-02 2.9304e+00 took: 45.12s\n",
      "net1 test acc: 2.089e-01 | net2 test acc: 2.349e-01\n",
      "Epoch  260,  98% \t acc: 4.7619e-01 4.1414e-01 - loss 4.9376e+00 3.3134e+00 1.9205e-02 2.8642e+00 took: 47.78s\n",
      "net1 test acc: 2.178e-01 | net2 test acc: 2.170e-01\n",
      "Epoch  261,  98% \t acc: 4.4877e-01 4.3146e-01 - loss 4.9349e+00 3.2384e+00 2.0706e-02 2.9790e+00 took: 46.12s\n",
      "net1 test acc: 2.155e-01 | net2 test acc: 2.144e-01\n",
      "Epoch  262,  98% \t acc: 5.1082e-01 4.5455e-01 - loss 4.6859e+00 3.0972e+00 1.8498e-02 2.8075e+00 took: 47.52s\n",
      "net1 test acc: 2.009e-01 | net2 test acc: 2.053e-01\n",
      "Epoch  263,  98% \t acc: 4.9784e-01 4.5743e-01 - loss 4.5607e+00 3.0811e+00 1.6337e-02 2.6324e+00 took: 46.01s\n",
      "net1 test acc: 2.305e-01 | net2 test acc: 2.208e-01\n",
      "Epoch  264,  98% \t acc: 4.9351e-01 4.3434e-01 - loss 4.7872e+00 3.2634e+00 1.8726e-02 2.6731e+00 took: 44.55s\n",
      "net1 test acc: 2.162e-01 | net2 test acc: 2.118e-01\n",
      "Epoch  265,  98% \t acc: 4.5310e-01 4.6176e-01 - loss 4.8663e+00 3.2583e+00 2.0425e-02 2.8074e+00 took: 44.88s\n",
      "net1 test acc: 2.087e-01 | net2 test acc: 2.153e-01\n",
      "Epoch  266,  98% \t acc: 4.3867e-01 4.1558e-01 - loss 4.9710e+00 3.3317e+00 1.8301e-02 2.9125e+00 took: 45.91s\n",
      "net1 test acc: 2.210e-01 | net2 test acc: 2.199e-01\n",
      "Epoch  267,  98% \t acc: 4.3723e-01 4.6176e-01 - loss 4.7772e+00 3.1640e+00 1.8550e-02 2.8554e+00 took: 47.20s\n",
      "net1 test acc: 2.211e-01 | net2 test acc: 2.226e-01\n",
      "Epoch  268,  98% \t acc: 4.4589e-01 4.5599e-01 - loss 4.7477e+00 3.1971e+00 1.7803e-02 2.7451e+00 took: 46.44s\n",
      "net1 test acc: 2.234e-01 | net2 test acc: 2.341e-01\n",
      "Epoch  269,  98% \t acc: 4.5310e-01 4.7330e-01 - loss 4.7206e+00 3.1968e+00 1.5965e-02 2.7283e+00 took: 48.26s\n",
      "net1 test acc: 2.123e-01 | net2 test acc: 2.089e-01\n",
      "Epoch  270,  98% \t acc: 4.5310e-01 4.9351e-01 - loss 4.7036e+00 3.1579e+00 2.0710e-02 2.6772e+00 took: 47.59s\n",
      "net1 test acc: 2.219e-01 | net2 test acc: 2.241e-01\n",
      "Epoch  271,  98% \t acc: 4.4877e-01 4.7042e-01 - loss 4.7515e+00 3.1967e+00 2.0582e-02 2.6979e+00 took: 47.98s\n",
      "net1 test acc: 2.077e-01 | net2 test acc: 2.210e-01\n",
      "Epoch  272,  98% \t acc: 4.7619e-01 4.5743e-01 - loss 4.7174e+00 3.1796e+00 1.9197e-02 2.6916e+00 took: 47.35s\n",
      "net1 test acc: 2.121e-01 | net2 test acc: 2.207e-01\n",
      "Epoch  273,  98% \t acc: 4.5166e-01 4.3001e-01 - loss 4.8526e+00 3.2580e+00 1.9490e-02 2.7994e+00 took: 48.44s\n",
      "net1 test acc: 2.194e-01 | net2 test acc: 2.190e-01\n",
      "Epoch  274,  98% \t acc: 4.6032e-01 4.5310e-01 - loss 4.7540e+00 3.1564e+00 1.7015e-02 2.8550e+00 took: 47.73s\n",
      "net1 test acc: 2.338e-01 | net2 test acc: 2.253e-01\n",
      "Epoch  275,  98% \t acc: 4.6753e-01 4.8052e-01 - loss 4.5998e+00 3.0153e+00 1.9413e-02 2.7807e+00 took: 48.19s\n",
      "net1 test acc: 2.114e-01 | net2 test acc: 2.215e-01\n",
      "Epoch  276,  98% \t acc: 4.7330e-01 4.4877e-01 - loss 4.7479e+00 3.2557e+00 1.7084e-02 2.6428e+00 took: 47.70s\n",
      "net1 test acc: 2.076e-01 | net2 test acc: 2.058e-01\n",
      "Epoch  277,  98% \t acc: 4.5599e-01 4.5887e-01 - loss 4.7181e+00 3.1687e+00 1.9048e-02 2.7178e+00 took: 47.70s\n",
      "net1 test acc: 2.115e-01 | net2 test acc: 2.144e-01\n",
      "Epoch  278,  98% \t acc: 4.9495e-01 4.8918e-01 - loss 4.4971e+00 3.0202e+00 1.8172e-02 2.5903e+00 took: 47.87s\n",
      "net1 test acc: 2.087e-01 | net2 test acc: 2.076e-01\n",
      "Epoch  279,  98% \t acc: 4.6032e-01 4.7186e-01 - loss 4.6806e+00 3.1819e+00 2.1194e-02 2.5735e+00 took: 47.90s\n",
      "net1 test acc: 2.046e-01 | net2 test acc: 2.039e-01\n",
      "Epoch  280,  98% \t acc: 4.6898e-01 4.6465e-01 - loss 4.6594e+00 3.1294e+00 2.0736e-02 2.6453e+00 took: 47.75s\n",
      "net1 test acc: 2.178e-01 | net2 test acc: 2.211e-01\n",
      "Epoch  281,  98% \t acc: 5.0361e-01 4.9351e-01 - loss 4.4992e+00 3.0761e+00 1.9550e-02 2.4553e+00 took: 48.13s\n",
      "net1 test acc: 2.114e-01 | net2 test acc: 2.069e-01\n",
      "Epoch  282,  98% \t acc: 4.9495e-01 4.9639e-01 - loss 4.5053e+00 3.0721e+00 1.9467e-02 2.4770e+00 took: 46.36s\n",
      "net1 test acc: 2.050e-01 | net2 test acc: 2.028e-01\n",
      "Epoch  283,  98% \t acc: 4.7186e-01 5.0216e-01 - loss 4.5389e+00 3.1107e+00 1.9390e-02 2.4686e+00 took: 45.07s\n",
      "net1 test acc: 2.040e-01 | net2 test acc: 2.107e-01\n",
      "Epoch  284,  98% \t acc: 4.4877e-01 4.8196e-01 - loss 4.7415e+00 3.2987e+00 1.8603e-02 2.5135e+00 took: 44.84s\n",
      "net1 test acc: 2.137e-01 | net2 test acc: 2.096e-01\n",
      "Epoch  285,  98% \t acc: 4.8629e-01 5.2958e-01 - loss 4.4231e+00 2.9999e+00 1.9962e-02 2.4470e+00 took: 45.06s\n",
      "net1 test acc: 2.218e-01 | net2 test acc: 2.059e-01\n",
      "Epoch  286,  98% \t acc: 4.9784e-01 5.1804e-01 - loss 4.4082e+00 3.0196e+00 2.0424e-02 2.3687e+00 took: 45.63s\n",
      "net1 test acc: 2.189e-01 | net2 test acc: 2.148e-01\n",
      "Epoch  287,  98% \t acc: 4.7475e-01 4.7908e-01 - loss 4.6521e+00 3.1897e+00 2.1608e-02 2.4928e+00 took: 45.59s\n",
      "net1 test acc: 2.048e-01 | net2 test acc: 2.115e-01\n",
      "Epoch  288,  98% \t acc: 4.5310e-01 4.3434e-01 - loss 4.8823e+00 3.3834e+00 2.1317e-02 2.5715e+00 took: 45.72s\n",
      "net1 test acc: 2.058e-01 | net2 test acc: 2.047e-01\n",
      "Epoch  289,  98% \t acc: 4.8918e-01 4.7042e-01 - loss 4.5579e+00 3.0708e+00 1.9428e-02 2.5855e+00 took: 46.01s\n",
      "net1 test acc: 2.012e-01 | net2 test acc: 2.035e-01\n",
      "Epoch  290,  98% \t acc: 4.9495e-01 4.9639e-01 - loss 4.5041e+00 3.0676e+00 1.7817e-02 2.5166e+00 took: 48.44s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.218e-01\n",
      "Epoch  291,  98% \t acc: 5.1082e-01 4.8773e-01 - loss 4.3711e+00 2.9797e+00 1.7710e-02 2.4287e+00 took: 48.22s\n",
      "net1 test acc: 2.189e-01 | net2 test acc: 2.145e-01\n",
      "Epoch  292,  98% \t acc: 4.8918e-01 5.2092e-01 - loss 4.4768e+00 3.0555e+00 2.1282e-02 2.4171e+00 took: 48.18s\n",
      "net1 test acc: 2.167e-01 | net2 test acc: 2.129e-01\n",
      "Epoch  293,  98% \t acc: 4.8341e-01 5.1804e-01 - loss 4.4565e+00 3.0575e+00 1.9404e-02 2.4100e+00 took: 48.17s\n",
      "net1 test acc: 2.211e-01 | net2 test acc: 2.032e-01\n",
      "Epoch  294,  98% \t acc: 4.8341e-01 5.0938e-01 - loss 4.4535e+00 3.0530e+00 2.0718e-02 2.3866e+00 took: 48.15s\n",
      "net1 test acc: 2.059e-01 | net2 test acc: 2.081e-01\n",
      "Epoch  295,  98% \t acc: 4.8918e-01 5.5267e-01 - loss 4.4355e+00 3.0044e+00 2.3062e-02 2.4008e+00 took: 46.05s\n",
      "net1 test acc: 2.185e-01 | net2 test acc: 2.096e-01\n",
      "Epoch  296,  98% \t acc: 5.1659e-01 5.3968e-01 - loss 4.2815e+00 2.9253e+00 2.0299e-02 2.3065e+00 took: 45.06s\n",
      "net1 test acc: 2.058e-01 | net2 test acc: 2.047e-01\n",
      "Epoch  297,  98% \t acc: 5.2958e-01 5.2525e-01 - loss 4.3079e+00 2.9217e+00 2.3387e-02 2.3047e+00 took: 45.75s\n",
      "net1 test acc: 2.134e-01 | net2 test acc: 2.051e-01\n",
      "Epoch  298,  98% \t acc: 4.7475e-01 5.4690e-01 - loss 4.4648e+00 3.0753e+00 2.1958e-02 2.3397e+00 took: 47.89s\n",
      "net1 test acc: 2.080e-01 | net2 test acc: 2.028e-01\n",
      "Epoch  299,  98% \t acc: 5.3535e-01 5.4257e-01 - loss 4.2256e+00 2.8706e+00 2.0685e-02 2.2965e+00 took: 45.35s\n",
      "net1 test acc: 2.136e-01 | net2 test acc: 2.172e-01\n",
      "Epoch  300,  98% \t acc: 5.3680e-01 5.3391e-01 - loss 4.3700e+00 3.0371e+00 2.1675e-02 2.2322e+00 took: 45.47s\n",
      "net1 test acc: 2.191e-01 | net2 test acc: 2.014e-01\n",
      "Epoch  301,  98% \t acc: 4.9206e-01 5.1659e-01 - loss 4.5773e+00 3.1994e+00 2.3676e-02 2.2823e+00 took: 45.67s\n",
      "net1 test acc: 2.038e-01 | net2 test acc: 1.971e-01\n",
      "Epoch  302,  98% \t acc: 5.5267e-01 5.3102e-01 - loss 4.1892e+00 2.8876e+00 1.9928e-02 2.2047e+00 took: 44.86s\n",
      "net1 test acc: 2.069e-01 | net2 test acc: 2.025e-01\n",
      "Epoch  303,  98% \t acc: 5.4113e-01 5.5123e-01 - loss 4.1432e+00 2.9089e+00 1.7315e-02 2.1222e+00 took: 45.20s\n",
      "net1 test acc: 2.186e-01 | net2 test acc: 2.153e-01\n",
      "Epoch  304,  98% \t acc: 5.2814e-01 5.0794e-01 - loss 4.4201e+00 3.0802e+00 2.1073e-02 2.2582e+00 took: 46.04s\n",
      "net1 test acc: 2.023e-01 | net2 test acc: 2.026e-01\n",
      "Epoch  305,  98% \t acc: 5.3824e-01 5.1659e-01 - loss 4.3084e+00 3.0154e+00 1.8735e-02 2.2112e+00 took: 45.28s\n",
      "net1 test acc: 2.150e-01 | net2 test acc: 2.017e-01\n",
      "Epoch  306,  98% \t acc: 5.3968e-01 5.3102e-01 - loss 4.2107e+00 2.8669e+00 2.0757e-02 2.2725e+00 took: 46.81s\n",
      "net1 test acc: 2.077e-01 | net2 test acc: 2.047e-01\n",
      "Epoch  307,  98% \t acc: 5.3680e-01 5.1659e-01 - loss 4.2175e+00 2.9545e+00 1.9827e-02 2.1295e+00 took: 48.56s\n",
      "net1 test acc: 2.190e-01 | net2 test acc: 2.224e-01\n",
      "Epoch  308,  98% \t acc: 5.2814e-01 5.3102e-01 - loss 4.4116e+00 3.1163e+00 2.1885e-02 2.1528e+00 took: 46.29s\n",
      "net1 test acc: 2.047e-01 | net2 test acc: 1.958e-01\n",
      "Epoch  309,  98% \t acc: 5.5123e-01 5.1804e-01 - loss 4.3369e+00 3.0013e+00 2.2561e-02 2.2200e+00 took: 47.60s\n",
      "net1 test acc: 2.096e-01 | net2 test acc: 2.062e-01\n",
      "Epoch  310,  98% \t acc: 5.4113e-01 5.4401e-01 - loss 4.1679e+00 2.9027e+00 2.0797e-02 2.1144e+00 took: 48.23s\n",
      "net1 test acc: 2.072e-01 | net2 test acc: 2.329e-01\n",
      "Epoch  311,  98% \t acc: 5.4401e-01 5.5267e-01 - loss 4.1804e+00 2.9270e+00 1.9517e-02 2.1165e+00 took: 48.20s\n",
      "net1 test acc: 2.051e-01 | net2 test acc: 2.074e-01\n",
      "Epoch  312,  98% \t acc: 5.6854e-01 5.7864e-01 - loss 3.9075e+00 2.6871e+00 2.0474e-02 2.0314e+00 took: 47.21s\n",
      "net1 test acc: 2.040e-01 | net2 test acc: 2.051e-01\n",
      "Epoch  313,  98% \t acc: 5.1371e-01 5.6566e-01 - loss 4.0757e+00 2.9017e+00 1.9485e-02 1.9582e+00 took: 45.14s\n",
      "net1 test acc: 2.002e-01 | net2 test acc: 2.036e-01\n",
      "Epoch  314,  98% \t acc: 5.4834e-01 5.5556e-01 - loss 4.0315e+00 2.8337e+00 1.9795e-02 1.9996e+00 took: 48.21s\n",
      "net1 test acc: 1.995e-01 | net2 test acc: 1.995e-01\n",
      "Epoch  315,  98% \t acc: 5.5988e-01 5.2670e-01 - loss 4.2587e+00 3.0396e+00 2.2152e-02 1.9950e+00 took: 47.40s\n",
      "net1 test acc: 2.085e-01 | net2 test acc: 2.096e-01\n",
      "Epoch  316,  98% \t acc: 5.3968e-01 5.7143e-01 - loss 4.0889e+00 2.8689e+00 1.8744e-02 2.0650e+00 took: 47.41s\n",
      "net1 test acc: 2.129e-01 | net2 test acc: 2.074e-01\n",
      "Epoch  317,  98% \t acc: 5.9596e-01 5.7576e-01 - loss 3.8978e+00 2.6733e+00 2.0572e-02 2.0375e+00 took: 48.11s\n",
      "net1 test acc: 1.976e-01 | net2 test acc: 1.953e-01\n",
      "Epoch  318,  98% \t acc: 5.4690e-01 5.4690e-01 - loss 4.1303e+00 2.9371e+00 2.0879e-02 1.9688e+00 took: 47.80s\n",
      "net1 test acc: 2.153e-01 | net2 test acc: 2.153e-01\n",
      "Epoch  319,  98% \t acc: 5.7576e-01 5.6710e-01 - loss 3.9411e+00 2.6956e+00 2.4601e-02 1.9989e+00 took: 48.42s\n",
      "net1 test acc: 2.162e-01 | net2 test acc: 2.066e-01\n",
      "Epoch  320,  98% \t acc: 5.5267e-01 5.4401e-01 - loss 4.1714e+00 2.9327e+00 2.3142e-02 2.0146e+00 took: 45.98s\n",
      "net1 test acc: 2.064e-01 | net2 test acc: 2.119e-01\n",
      "Epoch  321,  98% \t acc: 5.7864e-01 5.6566e-01 - loss 3.9190e+00 2.7513e+00 1.9067e-02 1.9541e+00 took: 44.98s\n",
      "net1 test acc: 2.319e-01 | net2 test acc: 2.164e-01\n",
      "Epoch  322,  98% \t acc: 5.3968e-01 5.6133e-01 - loss 4.3316e+00 3.0438e+00 2.3633e-02 2.1031e+00 took: 47.69s\n",
      "net1 test acc: 2.104e-01 | net2 test acc: 2.040e-01\n",
      "Epoch  323,  98% \t acc: 5.5700e-01 5.8009e-01 - loss 4.0849e+00 2.8530e+00 2.2526e-02 2.0134e+00 took: 48.07s\n",
      "net1 test acc: 1.972e-01 | net2 test acc: 1.939e-01\n",
      "Epoch  324,  98% \t acc: 5.8297e-01 5.6999e-01 - loss 3.8926e+00 2.7077e+00 2.0405e-02 1.9617e+00 took: 47.65s\n",
      "net1 test acc: 2.175e-01 | net2 test acc: 2.111e-01\n",
      "Epoch  325,  98% \t acc: 5.8442e-01 6.1039e-01 - loss 3.8122e+00 2.6563e+00 2.0402e-02 1.9038e+00 took: 48.25s\n",
      "net1 test acc: 2.153e-01 | net2 test acc: 2.108e-01\n",
      "Epoch  326,  98% \t acc: 5.9452e-01 5.2814e-01 - loss 4.0433e+00 2.8289e+00 2.2006e-02 1.9887e+00 took: 48.03s\n",
      "net1 test acc: 2.246e-01 | net2 test acc: 2.146e-01\n",
      "Epoch  327,  98% \t acc: 6.0462e-01 5.3824e-01 - loss 3.9194e+00 2.7210e+00 1.9835e-02 2.0002e+00 took: 47.94s\n",
      "net1 test acc: 2.047e-01 | net2 test acc: 2.077e-01\n",
      "Epoch  328,  98% \t acc: 6.3059e-01 5.6999e-01 - loss 3.7762e+00 2.6414e+00 1.9728e-02 1.8749e+00 took: 46.03s\n",
      "net1 test acc: 2.017e-01 | net2 test acc: 2.039e-01\n",
      "Epoch  329,  98% \t acc: 6.2771e-01 5.8297e-01 - loss 3.7468e+00 2.6095e+00 1.7839e-02 1.9177e+00 took: 48.63s\n",
      "net1 test acc: 2.092e-01 | net2 test acc: 2.100e-01\n",
      "Epoch  330,  98% \t acc: 6.0317e-01 5.7720e-01 - loss 3.8508e+00 2.7232e+00 1.9565e-02 1.8639e+00 took: 47.88s\n",
      "net1 test acc: 2.036e-01 | net2 test acc: 2.002e-01\n",
      "Epoch  331,  98% \t acc: 6.4935e-01 5.7143e-01 - loss 3.6489e+00 2.5256e+00 2.0515e-02 1.8364e+00 took: 48.02s\n",
      "net1 test acc: 2.134e-01 | net2 test acc: 2.059e-01\n",
      "Epoch  332,  98% \t acc: 6.3781e-01 5.7576e-01 - loss 3.6462e+00 2.5539e+00 1.8264e-02 1.8194e+00 took: 48.15s\n",
      "net1 test acc: 2.070e-01 | net2 test acc: 2.059e-01\n",
      "Epoch  333,  98% \t acc: 6.3781e-01 6.0606e-01 - loss 3.5168e+00 2.4834e+00 1.7318e-02 1.7205e+00 took: 45.57s\n",
      "net1 test acc: 2.074e-01 | net2 test acc: 1.980e-01\n",
      "Epoch  334,  98% \t acc: 6.1328e-01 5.9596e-01 - loss 3.7581e+00 2.6751e+00 1.9369e-02 1.7786e+00 took: 47.02s\n",
      "net1 test acc: 1.998e-01 | net2 test acc: 1.965e-01\n",
      "Epoch  335,  98% \t acc: 6.0173e-01 5.9452e-01 - loss 3.7931e+00 2.6750e+00 2.0796e-02 1.8204e+00 took: 48.12s\n",
      "net1 test acc: 2.188e-01 | net2 test acc: 2.025e-01\n",
      "Epoch  336,  98% \t acc: 6.1761e-01 6.0895e-01 - loss 3.6949e+00 2.5552e+00 2.3238e-02 1.8147e+00 took: 48.05s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.939e-01\n",
      "Epoch  337,  98% \t acc: 5.8153e-01 5.7287e-01 - loss 3.9596e+00 2.8279e+00 2.0681e-02 1.8497e+00 took: 47.91s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.999e-01\n",
      "Epoch  338,  98% \t acc: 6.0029e-01 6.6955e-01 - loss 3.5292e+00 2.4464e+00 2.0721e-02 1.7513e+00 took: 47.80s\n",
      "net1 test acc: 2.138e-01 | net2 test acc: 2.105e-01\n",
      "Epoch  339,  98% \t acc: 5.9885e-01 6.0173e-01 - loss 3.7585e+00 2.6810e+00 2.1154e-02 1.7319e+00 took: 47.70s\n",
      "net1 test acc: 1.995e-01 | net2 test acc: 1.950e-01\n",
      "Epoch  340,  98% \t acc: 6.4069e-01 6.3348e-01 - loss 3.4867e+00 2.3871e+00 2.2690e-02 1.7454e+00 took: 44.95s\n",
      "net1 test acc: 2.159e-01 | net2 test acc: 2.029e-01\n",
      "Epoch  341,  98% \t acc: 6.5079e-01 6.3781e-01 - loss 3.5155e+00 2.4189e+00 2.2794e-02 1.7372e+00 took: 44.98s\n",
      "net1 test acc: 1.977e-01 | net2 test acc: 1.988e-01\n",
      "Epoch  342,  98% \t acc: 6.5224e-01 6.5801e-01 - loss 3.3670e+00 2.3380e+00 2.0349e-02 1.6510e+00 took: 45.22s\n",
      "net1 test acc: 2.127e-01 | net2 test acc: 2.083e-01\n",
      "Epoch  343,  98% \t acc: 6.5224e-01 5.9307e-01 - loss 3.5713e+00 2.5690e+00 1.8720e-02 1.6301e+00 took: 45.16s\n",
      "net1 test acc: 2.146e-01 | net2 test acc: 2.124e-01\n",
      "Epoch  344,  98% \t acc: 6.4069e-01 6.4358e-01 - loss 3.3554e+00 2.3728e+00 1.8327e-02 1.5988e+00 took: 45.60s\n",
      "net1 test acc: 2.070e-01 | net2 test acc: 2.026e-01\n",
      "Epoch  345,  98% \t acc: 6.7533e-01 5.9452e-01 - loss 3.4269e+00 2.4266e+00 1.8270e-02 1.6350e+00 took: 45.44s\n",
      "net1 test acc: 2.006e-01 | net2 test acc: 1.972e-01\n",
      "Epoch  346,  98% \t acc: 6.6667e-01 6.4358e-01 - loss 3.2808e+00 2.3131e+00 1.7949e-02 1.5764e+00 took: 45.53s\n",
      "net1 test acc: 2.085e-01 | net2 test acc: 2.002e-01\n",
      "Epoch  347,  98% \t acc: 6.5657e-01 6.5079e-01 - loss 3.2482e+00 2.3011e+00 1.7855e-02 1.5371e+00 took: 46.94s\n",
      "net1 test acc: 2.110e-01 | net2 test acc: 2.021e-01\n",
      "Epoch  348,  98% \t acc: 6.2193e-01 6.4502e-01 - loss 3.4345e+00 2.4663e+00 1.8924e-02 1.5579e+00 took: 47.45s\n",
      "net1 test acc: 1.961e-01 | net2 test acc: 2.014e-01\n",
      "Epoch  349,  98% \t acc: 6.3203e-01 6.1183e-01 - loss 3.7350e+00 2.6944e+00 1.9603e-02 1.6891e+00 took: 48.17s\n",
      "net1 test acc: 1.972e-01 | net2 test acc: 1.998e-01\n",
      "Epoch  350,  98% \t acc: 6.4214e-01 6.2626e-01 - loss 3.4852e+00 2.4627e+00 1.9370e-02 1.6575e+00 took: 48.14s\n",
      "net1 test acc: 2.044e-01 | net2 test acc: 2.002e-01\n",
      "Epoch  351,  98% \t acc: 6.5657e-01 6.3203e-01 - loss 3.3198e+00 2.3429e+00 1.6572e-02 1.6225e+00 took: 46.05s\n",
      "net1 test acc: 1.969e-01 | net2 test acc: 1.958e-01\n",
      "Epoch  352,  98% \t acc: 6.5657e-01 6.8110e-01 - loss 3.2410e+00 2.2419e+00 1.9998e-02 1.5983e+00 took: 45.61s\n",
      "net1 test acc: 2.104e-01 | net2 test acc: 2.081e-01\n",
      "Epoch  353,  98% \t acc: 6.8831e-01 6.4935e-01 - loss 3.1363e+00 2.1800e+00 1.9789e-02 1.5170e+00 took: 47.83s\n",
      "net1 test acc: 2.074e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  354,  98% \t acc: 7.0274e-01 6.6234e-01 - loss 3.1594e+00 2.1757e+00 2.1098e-02 1.5453e+00 took: 46.63s\n",
      "net1 test acc: 1.931e-01 | net2 test acc: 1.909e-01\n",
      "Epoch  355,  98% \t acc: 6.9264e-01 6.8110e-01 - loss 3.1124e+00 2.2021e+00 1.9571e-02 1.4291e+00 took: 46.58s\n",
      "net1 test acc: 2.078e-01 | net2 test acc: 2.056e-01\n",
      "Epoch  356,  98% \t acc: 6.7532e-01 6.8398e-01 - loss 2.9972e+00 2.0911e+00 1.7202e-02 1.4680e+00 took: 47.98s\n",
      "net1 test acc: 2.059e-01 | net2 test acc: 2.104e-01\n",
      "Epoch  357,  98% \t acc: 6.7677e-01 7.0707e-01 - loss 3.0274e+00 2.0961e+00 2.0551e-02 1.4516e+00 took: 47.73s\n",
      "net1 test acc: 2.034e-01 | net2 test acc: 2.015e-01\n",
      "Epoch  358,  98% \t acc: 6.9408e-01 6.7965e-01 - loss 3.0557e+00 2.1716e+00 1.9262e-02 1.3829e+00 took: 45.58s\n",
      "net1 test acc: 2.018e-01 | net2 test acc: 2.074e-01\n",
      "Epoch  359,  98% \t acc: 7.1140e-01 6.8687e-01 - loss 2.8605e+00 1.9790e+00 1.9280e-02 1.3773e+00 took: 46.90s\n",
      "net1 test acc: 2.017e-01 | net2 test acc: 1.995e-01\n",
      "Epoch  360,  98% \t acc: 6.8975e-01 6.3348e-01 - loss 3.2068e+00 2.3313e+00 1.7639e-02 1.3983e+00 took: 45.33s\n",
      "net1 test acc: 2.002e-01 | net2 test acc: 2.014e-01\n",
      "Epoch  361,  98% \t acc: 7.0996e-01 6.6234e-01 - loss 3.0014e+00 2.1059e+00 1.8498e-02 1.4211e+00 took: 45.61s\n",
      "net1 test acc: 2.081e-01 | net2 test acc: 1.996e-01\n",
      "Epoch  362,  98% \t acc: 6.9697e-01 6.7244e-01 - loss 2.9765e+00 2.1376e+00 1.6503e-02 1.3477e+00 took: 45.41s\n",
      "net1 test acc: 2.028e-01 | net2 test acc: 1.961e-01\n",
      "Epoch  363,  98% \t acc: 7.0707e-01 6.9408e-01 - loss 2.9675e+00 2.0319e+00 2.1273e-02 1.4459e+00 took: 45.14s\n",
      "net1 test acc: 2.091e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  364,  98% \t acc: 6.9120e-01 6.8831e-01 - loss 2.9983e+00 2.0787e+00 2.0016e-02 1.4388e+00 took: 46.83s\n",
      "net1 test acc: 2.070e-01 | net2 test acc: 2.056e-01\n",
      "Epoch  365,  98% \t acc: 6.8110e-01 7.0130e-01 - loss 3.0071e+00 2.0847e+00 2.1064e-02 1.4234e+00 took: 47.66s\n",
      "net1 test acc: 2.097e-01 | net2 test acc: 2.045e-01\n",
      "Epoch  366,  98% \t acc: 6.9841e-01 7.1284e-01 - loss 2.8859e+00 1.9971e+00 2.0131e-02 1.3751e+00 took: 48.31s\n",
      "net1 test acc: 2.007e-01 | net2 test acc: 2.018e-01\n",
      "Epoch  367,  98% \t acc: 7.1717e-01 6.9120e-01 - loss 2.8998e+00 2.0141e+00 2.1473e-02 1.3420e+00 took: 45.43s\n",
      "net1 test acc: 2.002e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  368,  98% \t acc: 7.1573e-01 7.2006e-01 - loss 2.7577e+00 1.9340e+00 1.8307e-02 1.2813e+00 took: 45.65s\n",
      "net1 test acc: 2.059e-01 | net2 test acc: 1.962e-01\n",
      "Epoch  369,  98% \t acc: 7.2727e-01 6.8831e-01 - loss 2.8218e+00 2.0206e+00 1.8018e-02 1.2421e+00 took: 45.19s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.988e-01\n",
      "Epoch  370,  98% \t acc: 7.6335e-01 6.9120e-01 - loss 2.5941e+00 1.8060e+00 1.8145e-02 1.2133e+00 took: 45.37s\n",
      "net1 test acc: 1.977e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  371,  98% \t acc: 7.5902e-01 6.8543e-01 - loss 2.7039e+00 1.9121e+00 1.7467e-02 1.2343e+00 took: 47.17s\n",
      "net1 test acc: 2.045e-01 | net2 test acc: 1.978e-01\n",
      "Epoch  372,  98% \t acc: 7.4170e-01 7.0563e-01 - loss 2.6745e+00 1.8851e+00 1.6973e-02 1.2392e+00 took: 48.45s\n",
      "net1 test acc: 1.996e-01 | net2 test acc: 2.026e-01\n",
      "Epoch  373,  98% \t acc: 7.4170e-01 7.1429e-01 - loss 2.5319e+00 1.8065e+00 1.5473e-02 1.1414e+00 took: 45.48s\n",
      "net1 test acc: 2.037e-01 | net2 test acc: 1.970e-01\n",
      "Epoch  374,  98% \t acc: 7.6623e-01 7.0419e-01 - loss 2.6546e+00 1.8570e+00 1.8297e-02 1.2293e+00 took: 45.21s\n",
      "net1 test acc: 1.991e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  375,  98% \t acc: 7.4170e-01 6.9841e-01 - loss 2.6764e+00 1.9370e+00 1.6074e-02 1.1573e+00 took: 45.96s\n",
      "net1 test acc: 1.962e-01 | net2 test acc: 1.929e-01\n",
      "Epoch  376,  98% \t acc: 7.8499e-01 7.0418e-01 - loss 2.5439e+00 1.7749e+00 1.7157e-02 1.1949e+00 took: 48.01s\n",
      "net1 test acc: 2.056e-01 | net2 test acc: 2.015e-01\n",
      "Epoch  377,  98% \t acc: 7.5180e-01 7.1140e-01 - loss 2.5958e+00 1.8209e+00 1.7350e-02 1.2027e+00 took: 47.98s\n",
      "net1 test acc: 1.996e-01 | net2 test acc: 1.951e-01\n",
      "Epoch  378,  98% \t acc: 8.1674e-01 7.4748e-01 - loss 2.2485e+00 1.5323e+00 1.6146e-02 1.1095e+00 took: 46.03s\n",
      "net1 test acc: 1.996e-01 | net2 test acc: 1.951e-01\n",
      "Epoch  379,  98% \t acc: 8.0808e-01 7.6191e-01 - loss 2.1448e+00 1.4786e+00 1.4181e-02 1.0487e+00 took: 47.91s\n",
      "net1 test acc: 2.029e-01 | net2 test acc: 1.962e-01\n",
      "Epoch  380,  98% \t acc: 7.5758e-01 7.1717e-01 - loss 2.5420e+00 1.7861e+00 1.8533e-02 1.1412e+00 took: 47.71s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  381,  98% \t acc: 7.6191e-01 7.3016e-01 - loss 2.5332e+00 1.7598e+00 1.7805e-02 1.1906e+00 took: 47.65s\n",
      "net1 test acc: 1.909e-01 | net2 test acc: 1.901e-01\n",
      "Epoch  382,  98% \t acc: 7.7489e-01 7.2294e-01 - loss 2.3675e+00 1.6210e+00 1.7815e-02 1.1369e+00 took: 47.64s\n",
      "net1 test acc: 2.015e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  383,  98% \t acc: 8.0952e-01 7.4892e-01 - loss 2.2035e+00 1.4855e+00 1.6351e-02 1.1091e+00 took: 47.68s\n",
      "net1 test acc: 2.040e-01 | net2 test acc: 1.918e-01\n",
      "Epoch  384,  98% \t acc: 7.8932e-01 7.7489e-01 - loss 2.1869e+00 1.5022e+00 1.5916e-02 1.0511e+00 took: 47.54s\n",
      "net1 test acc: 2.007e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  385,  98% \t acc: 7.9221e-01 7.5469e-01 - loss 2.2927e+00 1.5729e+00 1.7747e-02 1.0846e+00 took: 47.81s\n",
      "net1 test acc: 2.018e-01 | net2 test acc: 1.951e-01\n",
      "Epoch  386,  98% \t acc: 7.9654e-01 7.4748e-01 - loss 2.3079e+00 1.5599e+00 1.8936e-02 1.1172e+00 took: 47.84s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.944e-01\n",
      "Epoch  387,  98% \t acc: 7.8355e-01 7.4892e-01 - loss 2.2999e+00 1.6136e+00 1.5309e-02 1.0664e+00 took: 47.82s\n",
      "net1 test acc: 2.007e-01 | net2 test acc: 1.944e-01\n",
      "Epoch  388,  98% \t acc: 8.1385e-01 7.6046e-01 - loss 2.1157e+00 1.4792e+00 1.4184e-02 9.8940e-01 took: 45.46s\n",
      "net1 test acc: 1.931e-01 | net2 test acc: 1.920e-01\n",
      "Epoch  389,  98% \t acc: 8.0520e-01 7.7778e-01 - loss 2.1311e+00 1.4702e+00 1.5282e-02 1.0160e+00 took: 48.36s\n",
      "net1 test acc: 1.939e-01 | net2 test acc: 1.865e-01\n",
      "Epoch  390,  98% \t acc: 8.0952e-01 7.6046e-01 - loss 2.1609e+00 1.4977e+00 1.5060e-02 1.0252e+00 took: 46.29s\n",
      "net1 test acc: 2.075e-01 | net2 test acc: 2.019e-01\n",
      "Epoch  391,  98% \t acc: 8.1818e-01 7.7056e-01 - loss 1.9896e+00 1.3312e+00 1.5792e-02 1.0009e+00 took: 45.05s\n",
      "net1 test acc: 2.089e-01 | net2 test acc: 1.970e-01\n",
      "Epoch  392,  98% \t acc: 8.0520e-01 7.8355e-01 - loss 2.0998e+00 1.4193e+00 1.6362e-02 1.0337e+00 took: 46.53s\n",
      "net1 test acc: 2.021e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  393,  98% \t acc: 8.3406e-01 7.7778e-01 - loss 1.9783e+00 1.3373e+00 1.5370e-02 9.7452e-01 took: 46.07s\n",
      "net1 test acc: 2.010e-01 | net2 test acc: 1.944e-01\n",
      "Epoch  394,  98% \t acc: 8.3550e-01 7.8211e-01 - loss 1.9215e+00 1.3243e+00 1.2630e-02 9.4196e-01 took: 47.55s\n",
      "net1 test acc: 2.007e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  395,  98% \t acc: 8.0375e-01 7.6768e-01 - loss 2.2323e+00 1.5481e+00 1.6737e-02 1.0335e+00 took: 46.90s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.977e-01\n",
      "Epoch  396,  98% \t acc: 8.0664e-01 7.5180e-01 - loss 2.2324e+00 1.5710e+00 1.5021e-02 1.0223e+00 took: 47.11s\n",
      "net1 test acc: 1.925e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  397,  98% \t acc: 8.1097e-01 7.5469e-01 - loss 2.1313e+00 1.4723e+00 1.5487e-02 1.0082e+00 took: 47.84s\n",
      "net1 test acc: 2.002e-01 | net2 test acc: 1.914e-01\n",
      "Epoch  398,  98% \t acc: 8.2973e-01 7.6191e-01 - loss 2.0723e+00 1.3909e+00 1.6029e-02 1.0422e+00 took: 47.26s\n",
      "net1 test acc: 2.021e-01 | net2 test acc: 1.944e-01\n",
      "Epoch  399,  98% \t acc: 8.4849e-01 7.8788e-01 - loss 1.8617e+00 1.2588e+00 1.3436e-02 9.3716e-01 took: 47.99s\n",
      "net1 test acc: 1.958e-01 | net2 test acc: 1.925e-01\n",
      "Epoch  400,  98% \t acc: 8.4993e-01 7.9077e-01 - loss 1.8892e+00 1.2641e+00 1.5466e-02 9.4088e-01 took: 47.98s\n",
      "net1 test acc: 2.045e-01 | net2 test acc: 1.989e-01\n",
      "Epoch  401,  98% \t acc: 8.0520e-01 8.0664e-01 - loss 1.9453e+00 1.3224e+00 1.5779e-02 9.3032e-01 took: 47.92s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.980e-01\n",
      "Epoch  402,  98% \t acc: 8.1385e-01 7.7489e-01 - loss 2.0532e+00 1.4304e+00 1.4834e-02 9.4896e-01 took: 47.96s\n",
      "net1 test acc: 1.996e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  403,  98% \t acc: 8.4560e-01 8.1963e-01 - loss 1.7625e+00 1.1894e+00 1.2842e-02 8.8923e-01 took: 48.17s\n",
      "net1 test acc: 1.985e-01 | net2 test acc: 1.970e-01\n",
      "Epoch  404,  98% \t acc: 8.5281e-01 8.0520e-01 - loss 1.7319e+00 1.1835e+00 1.2660e-02 8.4351e-01 took: 46.79s\n",
      "net1 test acc: 1.939e-01 | net2 test acc: 1.895e-01\n",
      "Epoch  405,  98% \t acc: 8.4416e-01 8.3550e-01 - loss 1.6357e+00 1.1145e+00 1.3212e-02 7.7819e-01 took: 47.93s\n",
      "net1 test acc: 2.143e-01 | net2 test acc: 1.979e-01\n",
      "Epoch  406,  98% \t acc: 8.6724e-01 8.1097e-01 - loss 1.6741e+00 1.1233e+00 1.3785e-02 8.2594e-01 took: 46.95s\n",
      "net1 test acc: 1.944e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  407,  98% \t acc: 8.5137e-01 8.2684e-01 - loss 1.5719e+00 1.0373e+00 1.3409e-02 8.0107e-01 took: 47.81s\n",
      "net1 test acc: 2.075e-01 | net2 test acc: 2.011e-01\n",
      "Epoch  408,  98% \t acc: 8.5714e-01 8.3117e-01 - loss 1.5436e+00 1.0089e+00 1.3369e-02 8.0208e-01 took: 48.05s\n",
      "net1 test acc: 2.017e-01 | net2 test acc: 1.842e-01\n",
      "Epoch  409,  98% \t acc: 8.5426e-01 8.1097e-01 - loss 1.6491e+00 1.1065e+00 1.3276e-02 8.1967e-01 took: 47.85s\n",
      "net1 test acc: 2.021e-01 | net2 test acc: 1.988e-01\n",
      "Epoch  410,  98% \t acc: 8.6436e-01 8.3117e-01 - loss 1.6022e+00 1.0518e+00 1.4209e-02 8.1656e-01 took: 47.69s\n",
      "net1 test acc: 1.950e-01 | net2 test acc: 1.906e-01\n",
      "Epoch  411,  98% \t acc: 8.7446e-01 8.0952e-01 - loss 1.6069e+00 1.0957e+00 1.2400e-02 7.7446e-01 took: 47.60s\n",
      "net1 test acc: 1.985e-01 | net2 test acc: 1.910e-01\n",
      "Epoch  412,  98% \t acc: 8.8167e-01 8.1097e-01 - loss 1.5797e+00 1.0408e+00 1.4380e-02 7.9002e-01 took: 46.05s\n",
      "net1 test acc: 1.988e-01 | net2 test acc: 1.914e-01\n",
      "Epoch  413,  98% \t acc: 8.5714e-01 8.4849e-01 - loss 1.4598e+00 9.5545e-01 1.3378e-02 7.4113e-01 took: 45.06s\n",
      "net1 test acc: 2.010e-01 | net2 test acc: 1.951e-01\n",
      "Epoch  414,  98% \t acc: 8.5426e-01 8.4704e-01 - loss 1.5034e+00 1.0232e+00 1.1661e-02 7.2720e-01 took: 45.45s\n",
      "net1 test acc: 1.988e-01 | net2 test acc: 1.891e-01\n",
      "Epoch  415,  98% \t acc: 8.6724e-01 8.5570e-01 - loss 1.3734e+00 9.0021e-01 1.2583e-02 6.9478e-01 took: 47.94s\n",
      "net1 test acc: 1.980e-01 | net2 test acc: 1.850e-01\n",
      "Epoch  416,  98% \t acc: 8.7013e-01 8.4849e-01 - loss 1.4640e+00 9.5133e-01 1.3702e-02 7.5121e-01 took: 48.01s\n",
      "net1 test acc: 1.950e-01 | net2 test acc: 1.883e-01\n",
      "Epoch  417,  98% \t acc: 8.7157e-01 8.7013e-01 - loss 1.3677e+00 9.1682e-01 1.1164e-02 6.7849e-01 took: 48.19s\n",
      "net1 test acc: 1.863e-01 | net2 test acc: 1.830e-01\n",
      "Epoch  418,  98% \t acc: 8.9610e-01 8.4849e-01 - loss 1.2754e+00 8.1299e-01 1.2072e-02 6.8348e-01 took: 47.29s\n",
      "net1 test acc: 1.985e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  419,  98% \t acc: 8.8600e-01 8.6580e-01 - loss 1.3115e+00 8.7063e-01 1.0806e-02 6.6559e-01 took: 48.36s\n",
      "net1 test acc: 2.023e-01 | net2 test acc: 1.978e-01\n",
      "Epoch  420,  98% \t acc: 8.7879e-01 8.4849e-01 - loss 1.3622e+00 9.1702e-01 1.0534e-02 6.7971e-01 took: 47.84s\n",
      "net1 test acc: 1.961e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  421,  98% \t acc: 8.6724e-01 8.5426e-01 - loss 1.3460e+00 8.8678e-01 1.1753e-02 6.8330e-01 took: 45.69s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  422,  98% \t acc: 8.8745e-01 8.7157e-01 - loss 1.2948e+00 8.6901e-01 9.9344e-03 6.5283e-01 took: 45.05s\n",
      "net1 test acc: 2.004e-01 | net2 test acc: 2.026e-01\n",
      "Epoch  423,  98% \t acc: 8.8312e-01 8.4849e-01 - loss 1.3703e+00 9.3629e-01 1.0867e-02 6.5068e-01 took: 45.49s\n",
      "net1 test acc: 1.944e-01 | net2 test acc: 1.977e-01\n",
      "Epoch  424,  98% \t acc: 8.7735e-01 8.8745e-01 - loss 1.2519e+00 7.9432e-01 1.0977e-02 6.9570e-01 took: 46.20s\n",
      "net1 test acc: 1.977e-01 | net2 test acc: 1.932e-01\n",
      "Epoch  425,  98% \t acc: 9.0043e-01 8.6436e-01 - loss 1.1889e+00 7.7108e-01 9.6796e-03 6.4213e-01 took: 45.25s\n",
      "net1 test acc: 2.011e-01 | net2 test acc: 1.981e-01\n",
      "Epoch  426,  98% \t acc: 9.1486e-01 8.9610e-01 - loss 1.0785e+00 6.7016e-01 9.3888e-03 6.2896e-01 took: 44.76s\n",
      "net1 test acc: 2.021e-01 | net2 test acc: 1.910e-01\n",
      "Epoch  427,  98% \t acc: 8.9033e-01 8.7157e-01 - loss 1.2046e+00 8.0336e-01 9.8050e-03 6.0640e-01 took: 45.18s\n",
      "net1 test acc: 1.981e-01 | net2 test acc: 1.907e-01\n",
      "Epoch  428,  98% \t acc: 9.1198e-01 8.6003e-01 - loss 1.1211e+00 7.2254e-01 9.7211e-03 6.0278e-01 took: 47.27s\n",
      "net1 test acc: 2.037e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  429,  98% \t acc: 9.2352e-01 8.7157e-01 - loss 1.0955e+00 6.8924e-01 1.0410e-02 6.0434e-01 took: 48.41s\n",
      "net1 test acc: 1.944e-01 | net2 test acc: 1.891e-01\n",
      "Epoch  430,  98% \t acc: 9.2352e-01 8.7446e-01 - loss 1.0742e+00 6.4551e-01 1.1101e-02 6.3541e-01 took: 47.97s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 1.981e-01\n",
      "Epoch  431,  98% \t acc: 8.8889e-01 8.9610e-01 - loss 1.1053e+00 6.9491e-01 9.6890e-03 6.2696e-01 took: 47.91s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.910e-01\n",
      "Epoch  432,  98% \t acc: 9.1486e-01 8.8889e-01 - loss 1.0816e+00 6.7206e-01 9.4965e-03 6.2918e-01 took: 47.98s\n",
      "net1 test acc: 1.944e-01 | net2 test acc: 1.921e-01\n",
      "Epoch  433,  98% \t acc: 9.1775e-01 8.8600e-01 - loss 1.0326e+00 6.4543e-01 9.3248e-03 5.8782e-01 took: 48.01s\n",
      "net1 test acc: 1.879e-01 | net2 test acc: 1.846e-01\n",
      "Epoch  434,  98% \t acc: 9.2929e-01 8.8600e-01 - loss 9.8548e-01 5.9450e-01 8.8852e-03 6.0427e-01 took: 47.79s\n",
      "net1 test acc: 2.023e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  435,  98% \t acc: 9.0621e-01 8.9178e-01 - loss 1.0022e+00 6.2357e-01 9.1216e-03 5.7485e-01 took: 48.03s\n",
      "net1 test acc: 2.011e-01 | net2 test acc: 1.989e-01\n",
      "Epoch  436,  98% \t acc: 9.0765e-01 9.1342e-01 - loss 9.8475e-01 6.2308e-01 7.6084e-03 5.7117e-01 took: 45.21s\n",
      "net1 test acc: 2.030e-01 | net2 test acc: 2.008e-01\n",
      "Epoch  437,  98% \t acc: 9.1053e-01 9.0620e-01 - loss 9.5028e-01 5.9305e-01 8.2097e-03 5.5027e-01 took: 47.58s\n",
      "net1 test acc: 1.944e-01 | net2 test acc: 1.932e-01\n",
      "Epoch  438,  98% \t acc: 9.1919e-01 9.0765e-01 - loss 9.3851e-01 5.6911e-01 8.0401e-03 5.7800e-01 took: 45.47s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.895e-01\n",
      "Epoch  439,  98% \t acc: 9.2063e-01 8.9177e-01 - loss 9.9827e-01 6.2113e-01 9.0269e-03 5.7375e-01 took: 47.28s\n",
      "net1 test acc: 1.909e-01 | net2 test acc: 1.876e-01\n",
      "Epoch  440,  98% \t acc: 8.9610e-01 9.1775e-01 - loss 9.1944e-01 5.8228e-01 8.0737e-03 5.1284e-01 took: 46.43s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.906e-01\n",
      "Epoch  441,  98% \t acc: 9.1775e-01 9.1775e-01 - loss 8.7164e-01 5.4993e-01 6.9309e-03 5.0479e-01 took: 46.00s\n",
      "net1 test acc: 1.939e-01 | net2 test acc: 1.883e-01\n",
      "Epoch  442,  98% \t acc: 9.0476e-01 8.9322e-01 - loss 1.0587e+00 6.6005e-01 9.8636e-03 6.0006e-01 took: 48.04s\n",
      "net1 test acc: 2.007e-01 | net2 test acc: 1.974e-01\n",
      "Epoch  443,  98% \t acc: 9.0476e-01 9.1053e-01 - loss 9.7888e-01 6.2450e-01 8.1820e-03 5.4511e-01 took: 51.67s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.891e-01\n",
      "Epoch  444,  98% \t acc: 9.1053e-01 9.1342e-01 - loss 9.6877e-01 6.0198e-01 8.2839e-03 5.6789e-01 took: 48.83s\n",
      "net1 test acc: 1.962e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  445,  98% \t acc: 9.1486e-01 9.1919e-01 - loss 8.4803e-01 5.1498e-01 7.5517e-03 5.1507e-01 took: 47.83s\n",
      "net1 test acc: 1.940e-01 | net2 test acc: 1.929e-01\n",
      "Epoch  446,  98% \t acc: 9.3362e-01 9.0476e-01 - loss 8.2862e-01 5.1911e-01 6.5799e-03 4.8742e-01 took: 47.75s\n",
      "net1 test acc: 1.902e-01 | net2 test acc: 1.925e-01\n",
      "Epoch  447,  98% \t acc: 9.3651e-01 9.1053e-01 - loss 8.2701e-01 4.7943e-01 8.1934e-03 5.3130e-01 took: 45.24s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.853e-01\n",
      "Epoch  448,  98% \t acc: 9.2785e-01 9.1053e-01 - loss 8.7033e-01 5.2273e-01 7.8564e-03 5.3807e-01 took: 46.87s\n",
      "net1 test acc: 1.962e-01 | net2 test acc: 1.929e-01\n",
      "Epoch  449,  98% \t acc: 9.2785e-01 9.1198e-01 - loss 9.1115e-01 5.7088e-01 7.5037e-03 5.3047e-01 took: 45.97s\n",
      "net1 test acc: 1.962e-01 | net2 test acc: 1.910e-01\n",
      "Epoch  450,  98% \t acc: 9.4084e-01 9.3074e-01 - loss 7.4975e-01 4.4394e-01 6.5286e-03 4.8105e-01 took: 48.25s\n",
      "net1 test acc: 1.902e-01 | net2 test acc: 1.925e-01\n",
      "Epoch  451,  98% \t acc: 9.2641e-01 9.1775e-01 - loss 8.6459e-01 5.1060e-01 8.0751e-03 5.4649e-01 took: 49.10s\n",
      "net1 test acc: 1.932e-01 | net2 test acc: 1.974e-01\n",
      "Epoch  452,  98% \t acc: 9.3651e-01 9.3218e-01 - loss 7.8641e-01 4.6969e-01 7.0006e-03 4.9344e-01 took: 45.26s\n",
      "net1 test acc: 1.887e-01 | net2 test acc: 1.898e-01\n",
      "Epoch  453,  98% \t acc: 9.2785e-01 9.1775e-01 - loss 7.7748e-01 4.7109e-01 6.1775e-03 4.8925e-01 took: 48.54s\n",
      "net1 test acc: 1.914e-01 | net2 test acc: 1.944e-01\n",
      "Epoch  454,  98% \t acc: 9.2929e-01 9.1919e-01 - loss 7.6256e-01 4.5005e-01 6.5093e-03 4.9484e-01 took: 47.81s\n",
      "net1 test acc: 1.977e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  455,  98% \t acc: 9.3218e-01 9.1631e-01 - loss 8.4417e-01 4.9443e-01 8.0889e-03 5.3770e-01 took: 48.01s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.914e-01\n",
      "Epoch  456,  98% \t acc: 9.2785e-01 9.1342e-01 - loss 8.1846e-01 5.1763e-01 6.2231e-03 4.7720e-01 took: 48.38s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.906e-01\n",
      "Epoch  457,  98% \t acc: 9.4084e-01 9.1486e-01 - loss 7.7248e-01 4.6527e-01 6.9281e-03 4.7585e-01 took: 47.94s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.876e-01\n",
      "Epoch  458,  98% \t acc: 9.3362e-01 9.2496e-01 - loss 7.3176e-01 4.2339e-01 6.3467e-03 4.8981e-01 took: 47.15s\n",
      "net1 test acc: 1.925e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  459,  98% \t acc: 9.4805e-01 9.4661e-01 - loss 6.3448e-01 3.4396e-01 6.4758e-03 4.5153e-01 took: 44.84s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.925e-01\n",
      "Epoch  460,  98% \t acc: 9.4517e-01 9.3939e-01 - loss 6.5569e-01 3.7348e-01 5.6170e-03 4.5208e-01 took: 48.44s\n",
      "net1 test acc: 1.996e-01 | net2 test acc: 1.974e-01\n",
      "Epoch  461,  98% \t acc: 9.4517e-01 9.3074e-01 - loss 6.8324e-01 3.9193e-01 6.2259e-03 4.5810e-01 took: 48.01s\n",
      "net1 test acc: 2.019e-01 | net2 test acc: 2.019e-01\n",
      "Epoch  462,  98% \t acc: 9.4517e-01 9.2785e-01 - loss 6.3110e-01 3.4925e-01 6.0886e-03 4.4192e-01 took: 46.15s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.932e-01\n",
      "Epoch  463,  98% \t acc: 9.2641e-01 9.4805e-01 - loss 7.3110e-01 4.2560e-01 6.6737e-03 4.7754e-01 took: 48.26s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.920e-01\n",
      "Epoch  464,  98% \t acc: 9.5671e-01 9.3795e-01 - loss 6.4445e-01 3.6278e-01 6.2692e-03 4.3797e-01 took: 46.44s\n",
      "net1 test acc: 1.887e-01 | net2 test acc: 1.909e-01\n",
      "Epoch  465,  98% \t acc: 9.4084e-01 9.5094e-01 - loss 6.1087e-01 3.3484e-01 5.7851e-03 4.3635e-01 took: 47.18s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.917e-01\n",
      "Epoch  466,  98% \t acc: 9.3074e-01 9.3651e-01 - loss 7.6661e-01 4.5345e-01 7.0281e-03 4.8576e-01 took: 47.87s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.940e-01\n",
      "Epoch  467,  98% \t acc: 9.4661e-01 9.3362e-01 - loss 6.8701e-01 3.9101e-01 6.1161e-03 4.6967e-01 took: 45.48s\n",
      "net1 test acc: 1.988e-01 | net2 test acc: 1.944e-01\n",
      "Epoch  468,  98% \t acc: 9.3218e-01 9.4805e-01 - loss 6.6535e-01 3.7754e-01 5.9994e-03 4.5563e-01 took: 45.56s\n",
      "net1 test acc: 1.958e-01 | net2 test acc: 1.958e-01\n",
      "Epoch  469,  98% \t acc: 9.4805e-01 9.4805e-01 - loss 6.2733e-01 3.4925e-01 5.6445e-03 4.4328e-01 took: 45.99s\n",
      "net1 test acc: 1.996e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  470,  98% \t acc: 9.5815e-01 9.5382e-01 - loss 5.6004e-01 3.1093e-01 4.4873e-03 4.0849e-01 took: 45.47s\n",
      "net1 test acc: 1.977e-01 | net2 test acc: 1.977e-01\n",
      "Epoch  471,  98% \t acc: 9.5815e-01 9.3074e-01 - loss 6.6986e-01 3.8795e-01 5.3140e-03 4.5754e-01 took: 45.36s\n",
      "net1 test acc: 2.004e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  472,  98% \t acc: 9.5527e-01 9.5238e-01 - loss 5.5836e-01 2.9585e-01 5.8242e-03 4.0852e-01 took: 45.45s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  473,  98% \t acc: 9.5094e-01 9.6393e-01 - loss 5.6616e-01 2.9467e-01 5.9578e-03 4.2382e-01 took: 48.12s\n",
      "net1 test acc: 1.887e-01 | net2 test acc: 1.876e-01\n",
      "Epoch  474,  98% \t acc: 9.5671e-01 9.4661e-01 - loss 5.4107e-01 2.8682e-01 4.9832e-03 4.0883e-01 took: 47.93s\n",
      "net1 test acc: 1.981e-01 | net2 test acc: 1.992e-01\n",
      "Epoch  475,  98% \t acc: 9.3795e-01 9.3795e-01 - loss 6.1649e-01 3.6772e-01 4.6095e-03 4.0536e-01 took: 47.96s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  476,  98% \t acc: 9.5671e-01 9.3218e-01 - loss 6.1124e-01 3.2963e-01 5.9517e-03 4.4417e-01 took: 48.18s\n",
      "net1 test acc: 2.102e-01 | net2 test acc: 2.049e-01\n",
      "Epoch  477,  98% \t acc: 9.6104e-01 9.3651e-01 - loss 5.3521e-01 2.7724e-01 5.4197e-03 4.0754e-01 took: 48.40s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 1.992e-01\n",
      "Epoch  478,  98% \t acc: 9.5815e-01 9.6248e-01 - loss 5.3762e-01 2.8485e-01 5.1290e-03 4.0296e-01 took: 43.59s\n",
      "net1 test acc: 1.985e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  479,  98% \t acc: 9.5671e-01 9.4517e-01 - loss 5.2734e-01 2.8692e-01 4.5982e-03 3.8886e-01 took: 47.70s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 2.023e-01\n",
      "Epoch  480,  98% \t acc: 9.5815e-01 9.5238e-01 - loss 5.4214e-01 2.9297e-01 4.7771e-03 4.0280e-01 took: 45.05s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.898e-01\n",
      "Epoch  481,  98% \t acc: 9.7403e-01 9.5960e-01 - loss 4.2881e-01 1.9786e-01 4.5549e-03 3.7080e-01 took: 48.77s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.906e-01\n",
      "Epoch  482,  98% \t acc: 9.5527e-01 9.5815e-01 - loss 4.8982e-01 2.5632e-01 4.3255e-03 3.8049e-01 took: 48.40s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  483,  98% \t acc: 9.7258e-01 9.5094e-01 - loss 5.0533e-01 2.6963e-01 4.5833e-03 3.7973e-01 took: 45.45s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 1.992e-01\n",
      "Epoch  484,  98% \t acc: 9.5671e-01 9.5671e-01 - loss 5.1774e-01 2.8952e-01 4.4370e-03 3.6771e-01 took: 47.56s\n",
      "net1 test acc: 1.822e-01 | net2 test acc: 1.844e-01\n",
      "Epoch  485,  98% \t acc: 9.5094e-01 9.6104e-01 - loss 5.1324e-01 2.8345e-01 4.8255e-03 3.6307e-01 took: 45.78s\n",
      "net1 test acc: 2.000e-01 | net2 test acc: 2.023e-01\n",
      "Epoch  486,  98% \t acc: 9.5094e-01 9.5094e-01 - loss 5.3263e-01 2.9733e-01 4.4713e-03 3.8118e-01 took: 45.15s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  487,  98% \t acc: 9.6537e-01 9.3218e-01 - loss 5.0994e-01 2.7280e-01 4.5924e-03 3.8242e-01 took: 46.29s\n",
      "net1 test acc: 1.909e-01 | net2 test acc: 1.920e-01\n",
      "Epoch  488,  98% \t acc: 9.6970e-01 9.6248e-01 - loss 4.4833e-01 2.3840e-01 4.1503e-03 3.3685e-01 took: 47.77s\n",
      "net1 test acc: 2.011e-01 | net2 test acc: 1.992e-01\n",
      "Epoch  489,  98% \t acc: 9.6392e-01 9.5960e-01 - loss 5.0387e-01 2.7700e-01 4.6428e-03 3.6088e-01 took: 47.96s\n",
      "net1 test acc: 1.890e-01 | net2 test acc: 1.890e-01\n",
      "Epoch  490,  98% \t acc: 9.6104e-01 9.5815e-01 - loss 4.6371e-01 2.4869e-01 4.1929e-03 3.4619e-01 took: 47.93s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  491,  98% \t acc: 9.6104e-01 9.5094e-01 - loss 4.7367e-01 2.5823e-01 4.1275e-03 3.4833e-01 took: 47.43s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  492,  98% \t acc: 9.5382e-01 9.6970e-01 - loss 5.4311e-01 3.1198e-01 4.1055e-03 3.8016e-01 took: 45.10s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.962e-01\n",
      "Epoch  493,  98% \t acc: 9.7403e-01 9.6537e-01 - loss 4.2998e-01 2.0230e-01 4.6353e-03 3.6264e-01 took: 45.40s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.974e-01\n",
      "Epoch  494,  98% \t acc: 9.7258e-01 9.5815e-01 - loss 4.4132e-01 2.2616e-01 4.2985e-03 3.4434e-01 took: 48.14s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.944e-01\n",
      "Epoch  495,  98% \t acc: 9.7691e-01 9.6537e-01 - loss 4.0563e-01 2.0005e-01 3.7454e-03 3.3626e-01 took: 48.15s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 1.992e-01\n",
      "Epoch  496,  98% \t acc: 9.7114e-01 9.5815e-01 - loss 4.4082e-01 2.2507e-01 3.8202e-03 3.5510e-01 took: 48.27s\n",
      "net1 test acc: 1.890e-01 | net2 test acc: 1.879e-01\n",
      "Epoch  497,  98% \t acc: 9.6825e-01 9.6104e-01 - loss 4.6442e-01 2.4680e-01 4.2985e-03 3.4927e-01 took: 47.64s\n",
      "net1 test acc: 1.887e-01 | net2 test acc: 1.887e-01\n",
      "Epoch  498,  98% \t acc: 9.5815e-01 9.5527e-01 - loss 5.1638e-01 2.9547e-01 4.1044e-03 3.5974e-01 took: 47.15s\n",
      "net1 test acc: 1.958e-01 | net2 test acc: 1.925e-01\n",
      "Epoch  499,  98% \t acc: 9.5094e-01 9.6681e-01 - loss 5.0359e-01 2.9140e-01 4.3170e-03 3.3804e-01 took: 45.55s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.958e-01\n",
      "Epoch  500,  98% \t acc: 9.6537e-01 9.5960e-01 - loss 4.5734e-01 2.3626e-01 4.0814e-03 3.6053e-01 took: 46.47s\n",
      "net1 test acc: 1.962e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  501,  98% \t acc: 9.6248e-01 9.6825e-01 - loss 4.6352e-01 2.4291e-01 4.2610e-03 3.5599e-01 took: 46.26s\n",
      "net1 test acc: 2.030e-01 | net2 test acc: 2.060e-01\n",
      "Epoch  502,  98% \t acc: 9.7403e-01 9.4950e-01 - loss 4.7463e-01 2.5519e-01 3.8398e-03 3.6208e-01 took: 46.70s\n",
      "net1 test acc: 1.901e-01 | net2 test acc: 1.909e-01\n",
      "Epoch  503,  98% \t acc: 9.6681e-01 9.6104e-01 - loss 4.8380e-01 2.4713e-01 4.3874e-03 3.8559e-01 took: 47.81s\n",
      "net1 test acc: 1.977e-01 | net2 test acc: 1.977e-01\n",
      "Epoch  504,  98% \t acc: 9.5960e-01 9.6248e-01 - loss 4.6560e-01 2.4693e-01 3.6998e-03 3.6335e-01 took: 45.91s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  505,  98% \t acc: 9.7114e-01 9.7114e-01 - loss 4.3219e-01 2.1266e-01 4.4934e-03 3.4919e-01 took: 47.25s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  506,  98% \t acc: 9.6392e-01 9.6248e-01 - loss 4.3839e-01 2.3357e-01 4.1072e-03 3.2749e-01 took: 48.11s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.917e-01\n",
      "Epoch  507,  98% \t acc: 9.6104e-01 9.6681e-01 - loss 4.6427e-01 2.5586e-01 4.1598e-03 3.3363e-01 took: 45.99s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  508,  98% \t acc: 9.7835e-01 9.6825e-01 - loss 3.6282e-01 1.6562e-01 3.5354e-03 3.2369e-01 took: 47.05s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.909e-01\n",
      "Epoch  509,  98% \t acc: 9.7258e-01 9.6393e-01 - loss 4.4086e-01 2.3151e-01 3.7150e-03 3.4440e-01 took: 47.96s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  510,  98% \t acc: 9.6248e-01 9.6825e-01 - loss 4.3327e-01 2.2434e-01 3.9252e-03 3.3936e-01 took: 44.64s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  511,  98% \t acc: 9.7403e-01 9.6681e-01 - loss 4.0585e-01 1.9882e-01 3.7699e-03 3.3866e-01 took: 45.23s\n",
      "net1 test acc: 1.925e-01 | net2 test acc: 1.925e-01\n",
      "Epoch  512,  98% \t acc: 9.5527e-01 9.6392e-01 - loss 4.6175e-01 2.4573e-01 4.3589e-03 3.4486e-01 took: 44.97s\n",
      "net1 test acc: 1.985e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  513,  98% \t acc: 9.6537e-01 9.6537e-01 - loss 4.1990e-01 2.1737e-01 3.8302e-03 3.2847e-01 took: 45.17s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.909e-01\n",
      "Epoch  514,  98% \t acc: 9.5960e-01 9.6681e-01 - loss 4.7987e-01 2.6575e-01 3.6795e-03 3.5466e-01 took: 45.19s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.887e-01\n",
      "Epoch  515,  98% \t acc: 9.7114e-01 9.7114e-01 - loss 4.2737e-01 2.1556e-01 3.7802e-03 3.4801e-01 took: 45.14s\n",
      "net1 test acc: 2.004e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  516,  98% \t acc: 9.6681e-01 9.6970e-01 - loss 4.2116e-01 2.2193e-01 3.0898e-03 3.3665e-01 took: 45.55s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.920e-01\n",
      "Epoch  517,  98% \t acc: 9.6970e-01 9.6970e-01 - loss 3.8951e-01 1.8972e-01 3.6831e-03 3.2592e-01 took: 45.38s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  518,  98% \t acc: 9.7258e-01 9.7547e-01 - loss 3.7942e-01 1.8850e-01 3.6396e-03 3.0906e-01 took: 45.81s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  519,  98% \t acc: 9.6248e-01 9.6970e-01 - loss 4.0769e-01 2.0458e-01 3.6525e-03 3.3318e-01 took: 45.46s\n",
      "net1 test acc: 1.860e-01 | net2 test acc: 1.871e-01\n",
      "Epoch  520,  98% \t acc: 9.6392e-01 9.7114e-01 - loss 4.1008e-01 2.0725e-01 3.8282e-03 3.2909e-01 took: 45.87s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.936e-01\n",
      "Epoch  521,  98% \t acc: 9.7114e-01 9.7547e-01 - loss 3.7933e-01 1.8900e-01 3.4981e-03 3.1071e-01 took: 46.07s\n",
      "net1 test acc: 2.026e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  522,  98% \t acc: 9.6392e-01 9.5960e-01 - loss 4.2723e-01 2.2829e-01 3.7922e-03 3.2203e-01 took: 48.41s\n",
      "net1 test acc: 2.007e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  523,  98% \t acc: 9.8268e-01 9.6825e-01 - loss 3.7590e-01 1.8934e-01 3.4961e-03 3.0320e-01 took: 46.33s\n",
      "net1 test acc: 1.988e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  524,  98% \t acc: 9.7547e-01 9.7258e-01 - loss 3.8936e-01 1.8611e-01 3.6634e-03 3.3323e-01 took: 45.61s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  525,  98% \t acc: 9.6681e-01 9.7403e-01 - loss 3.6993e-01 1.8591e-01 3.1166e-03 3.0569e-01 took: 45.68s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 1.992e-01\n",
      "Epoch  526,  98% \t acc: 9.7114e-01 9.6970e-01 - loss 3.9108e-01 2.0189e-01 3.3494e-03 3.1140e-01 took: 46.59s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  527,  98% \t acc: 9.7547e-01 9.6392e-01 - loss 3.6654e-01 1.8099e-01 3.5989e-03 2.9913e-01 took: 45.47s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  528,  98% \t acc: 9.8413e-01 9.7691e-01 - loss 3.2813e-01 1.4690e-01 3.1937e-03 2.9859e-01 took: 45.17s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.939e-01\n",
      "Epoch  529,  98% \t acc: 9.8124e-01 9.6104e-01 - loss 3.8152e-01 1.8663e-01 3.8770e-03 3.1225e-01 took: 45.52s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.917e-01\n",
      "Epoch  530,  98% \t acc: 9.6970e-01 9.7547e-01 - loss 3.8323e-01 1.9498e-01 3.2655e-03 3.1120e-01 took: 45.37s\n",
      "net1 test acc: 2.015e-01 | net2 test acc: 2.007e-01\n",
      "Epoch  531,  98% \t acc: 9.7835e-01 9.7980e-01 - loss 3.3630e-01 1.4500e-01 3.9323e-03 3.0396e-01 took: 45.60s\n",
      "net1 test acc: 2.007e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  532,  98% \t acc: 9.7691e-01 9.7258e-01 - loss 3.6302e-01 1.7053e-01 3.0727e-03 3.2352e-01 took: 45.71s\n",
      "net1 test acc: 1.996e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  533,  98% \t acc: 9.7547e-01 9.7980e-01 - loss 3.4617e-01 1.6543e-01 3.5401e-03 2.9068e-01 took: 45.57s\n",
      "net1 test acc: 2.079e-01 | net2 test acc: 2.068e-01\n",
      "Epoch  534,  98% \t acc: 9.6392e-01 9.6970e-01 - loss 3.8393e-01 2.0101e-01 3.3615e-03 2.9862e-01 took: 45.97s\n",
      "net1 test acc: 1.977e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  535,  98% \t acc: 9.7547e-01 9.7691e-01 - loss 3.4513e-01 1.6445e-01 3.1890e-03 2.9757e-01 took: 46.42s\n",
      "net1 test acc: 1.925e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  536,  98% \t acc: 9.7547e-01 9.6825e-01 - loss 3.5394e-01 1.6816e-01 3.3735e-03 3.0409e-01 took: 47.11s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.974e-01\n",
      "Epoch  537,  98% \t acc: 9.7403e-01 9.5238e-01 - loss 3.9750e-01 2.0583e-01 3.4596e-03 3.1414e-01 took: 47.28s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  538,  98% \t acc: 9.7835e-01 9.7691e-01 - loss 3.6026e-01 1.7546e-01 3.5499e-03 2.9860e-01 took: 45.63s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.909e-01\n",
      "Epoch  539,  98% \t acc: 9.7258e-01 9.7114e-01 - loss 3.4479e-01 1.7212e-01 2.9183e-03 2.8697e-01 took: 45.63s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.917e-01\n",
      "Epoch  540,  98% \t acc: 9.7980e-01 9.7258e-01 - loss 3.2825e-01 1.4679e-01 3.3875e-03 2.9517e-01 took: 45.78s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.958e-01\n",
      "Epoch  541,  98% \t acc: 9.7691e-01 9.7835e-01 - loss 3.2578e-01 1.4647e-01 2.9901e-03 2.9881e-01 took: 45.09s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.974e-01\n",
      "Epoch  542,  98% \t acc: 9.8124e-01 9.6392e-01 - loss 3.8746e-01 2.0077e-01 3.1888e-03 3.0961e-01 took: 46.66s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  543,  98% \t acc: 9.7403e-01 9.7258e-01 - loss 3.4265e-01 1.6552e-01 3.3554e-03 2.8714e-01 took: 48.01s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  544,  98% \t acc: 9.7258e-01 9.7547e-01 - loss 3.3443e-01 1.5725e-01 3.0972e-03 2.9242e-01 took: 48.09s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  545,  98% \t acc: 9.7114e-01 9.6970e-01 - loss 3.7724e-01 1.8871e-01 3.3604e-03 3.0984e-01 took: 45.01s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  546,  98% \t acc: 9.7980e-01 9.6970e-01 - loss 3.3855e-01 1.6162e-01 3.2722e-03 2.8841e-01 took: 47.93s\n",
      "net1 test acc: 1.860e-01 | net2 test acc: 1.860e-01\n",
      "Epoch  547,  98% \t acc: 9.7835e-01 9.7258e-01 - loss 3.4794e-01 1.6955e-01 2.8962e-03 2.9886e-01 took: 48.16s\n",
      "net1 test acc: 2.041e-01 | net2 test acc: 2.041e-01\n",
      "Epoch  548,  98% \t acc: 9.7547e-01 9.7835e-01 - loss 3.5827e-01 1.8342e-01 3.2208e-03 2.8528e-01 took: 47.18s\n",
      "net1 test acc: 1.928e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  549,  98% \t acc: 9.7547e-01 9.7835e-01 - loss 3.2077e-01 1.3990e-01 3.5195e-03 2.9135e-01 took: 48.11s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  550,  98% \t acc: 9.8413e-01 9.7114e-01 - loss 3.3322e-01 1.5396e-01 3.3203e-03 2.9212e-01 took: 48.24s\n",
      "net1 test acc: 1.966e-01 | net2 test acc: 1.955e-01\n",
      "Epoch  551,  98% \t acc: 9.8124e-01 9.8990e-01 - loss 2.8166e-01 1.2173e-01 3.0947e-03 2.5796e-01 took: 47.95s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  552,  98% \t acc: 9.7258e-01 9.7403e-01 - loss 3.7524e-01 1.9768e-01 3.0687e-03 2.9376e-01 took: 46.33s\n",
      "net1 test acc: 2.023e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  553,  98% \t acc: 9.7114e-01 9.7114e-01 - loss 3.6214e-01 1.8075e-01 2.8931e-03 3.0491e-01 took: 48.29s\n",
      "net1 test acc: 1.985e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  554,  98% \t acc: 9.7691e-01 9.7258e-01 - loss 3.3546e-01 1.6704e-01 3.1895e-03 2.7306e-01 took: 47.89s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.920e-01\n",
      "Epoch  555,  98% \t acc: 9.8268e-01 9.7258e-01 - loss 3.3664e-01 1.5865e-01 3.4172e-03 2.8764e-01 took: 47.76s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  556,  98% \t acc: 9.6970e-01 9.6537e-01 - loss 3.4869e-01 1.8675e-01 2.7459e-03 2.6897e-01 took: 46.91s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  557,  98% \t acc: 9.8124e-01 9.7980e-01 - loss 3.0874e-01 1.4104e-01 3.0341e-03 2.7471e-01 took: 45.50s\n",
      "net1 test acc: 2.049e-01 | net2 test acc: 2.060e-01\n",
      "Epoch  558,  98% \t acc: 9.7258e-01 9.7980e-01 - loss 3.5413e-01 1.7770e-01 2.9635e-03 2.9359e-01 took: 47.22s\n",
      "net1 test acc: 1.947e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  559,  98% \t acc: 9.7835e-01 9.7114e-01 - loss 3.4678e-01 1.6995e-01 2.8005e-03 2.9764e-01 took: 47.87s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  560,  98% \t acc: 9.7403e-01 9.6825e-01 - loss 3.4270e-01 1.6449e-01 3.4015e-03 2.8838e-01 took: 47.82s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  561,  98% \t acc: 9.8124e-01 9.7691e-01 - loss 3.5092e-01 1.8085e-01 2.8544e-03 2.8305e-01 took: 45.79s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  562,  98% \t acc: 9.6825e-01 9.6970e-01 - loss 3.7214e-01 1.9009e-01 2.9882e-03 3.0433e-01 took: 45.55s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  563,  98% \t acc: 9.6970e-01 9.7258e-01 - loss 3.4871e-01 1.7265e-01 3.3863e-03 2.8439e-01 took: 45.19s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  564,  98% \t acc: 9.7980e-01 9.8124e-01 - loss 3.0048e-01 1.3682e-01 2.8006e-03 2.7130e-01 took: 45.23s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  565,  98% \t acc: 9.7691e-01 9.8268e-01 - loss 3.2432e-01 1.5177e-01 3.2033e-03 2.8104e-01 took: 45.45s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  566,  98% \t acc: 9.7547e-01 9.7258e-01 - loss 3.3574e-01 1.6782e-01 3.2088e-03 2.7167e-01 took: 45.73s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  567,  98% \t acc: 9.7114e-01 9.7403e-01 - loss 3.5779e-01 1.8117e-01 3.3764e-03 2.8572e-01 took: 45.42s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  568,  98% \t acc: 9.8124e-01 9.7835e-01 - loss 2.9114e-01 1.3033e-01 2.4936e-03 2.7174e-01 took: 46.19s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  569,  98% \t acc: 9.7691e-01 9.7980e-01 - loss 3.2276e-01 1.5343e-01 3.1177e-03 2.7631e-01 took: 46.13s\n",
      "net1 test acc: 2.011e-01 | net2 test acc: 2.023e-01\n",
      "Epoch  570,  98% \t acc: 9.8413e-01 9.7403e-01 - loss 3.0070e-01 1.3353e-01 2.8151e-03 2.7804e-01 took: 48.29s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  571,  98% \t acc: 9.8124e-01 9.8124e-01 - loss 3.0294e-01 1.3094e-01 3.4063e-03 2.7587e-01 took: 48.34s\n",
      "net1 test acc: 1.822e-01 | net2 test acc: 1.852e-01\n",
      "Epoch  572,  98% \t acc: 9.8268e-01 9.7691e-01 - loss 3.3441e-01 1.6428e-01 3.1446e-03 2.7738e-01 took: 45.48s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.909e-01\n",
      "Epoch  573,  98% \t acc: 9.7835e-01 9.7258e-01 - loss 3.4573e-01 1.7340e-01 2.9721e-03 2.8520e-01 took: 47.13s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  574,  98% \t acc: 9.7691e-01 9.7114e-01 - loss 3.4416e-01 1.7147e-01 2.7128e-03 2.9113e-01 took: 47.92s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  575,  98% \t acc: 9.8557e-01 9.7691e-01 - loss 3.2519e-01 1.4969e-01 3.2380e-03 2.8623e-01 took: 48.13s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  576,  98% \t acc: 9.7835e-01 9.7691e-01 - loss 3.5953e-01 1.8339e-01 3.3653e-03 2.8497e-01 took: 48.29s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  577,  98% \t acc: 9.7114e-01 9.7980e-01 - loss 3.3912e-01 1.7187e-01 2.7898e-03 2.7870e-01 took: 48.18s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  578,  98% \t acc: 9.7835e-01 9.7691e-01 - loss 3.5008e-01 1.7799e-01 2.9833e-03 2.8452e-01 took: 47.59s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  579,  98% \t acc: 9.8701e-01 9.7691e-01 - loss 2.8835e-01 1.2124e-01 2.9739e-03 2.7475e-01 took: 46.68s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  580,  98% \t acc: 9.6970e-01 9.7403e-01 - loss 3.2887e-01 1.5688e-01 2.9363e-03 2.8524e-01 took: 47.37s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  581,  98% \t acc: 9.7547e-01 9.7980e-01 - loss 3.1872e-01 1.4881e-01 2.8208e-03 2.8342e-01 took: 45.61s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  582,  98% \t acc: 9.8268e-01 9.7114e-01 - loss 3.0676e-01 1.4191e-01 3.1282e-03 2.6714e-01 took: 47.58s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  583,  98% \t acc: 9.7691e-01 9.7258e-01 - loss 3.4203e-01 1.7145e-01 2.7625e-03 2.8592e-01 took: 44.79s\n",
      "net1 test acc: 2.030e-01 | net2 test acc: 2.060e-01\n",
      "Epoch  584,  98% \t acc: 9.8557e-01 9.8413e-01 - loss 2.8593e-01 1.2591e-01 2.5831e-03 2.6839e-01 took: 48.34s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.928e-01\n",
      "Epoch  585,  98% \t acc: 9.7835e-01 9.8124e-01 - loss 3.0245e-01 1.3960e-01 2.6751e-03 2.7220e-01 took: 45.34s\n",
      "net1 test acc: 1.917e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  586,  98% \t acc: 9.7835e-01 9.7980e-01 - loss 3.2643e-01 1.5397e-01 2.9389e-03 2.8616e-01 took: 48.65s\n",
      "net1 test acc: 1.879e-01 | net2 test acc: 1.890e-01\n",
      "Epoch  587,  98% \t acc: 9.7691e-01 9.6825e-01 - loss 3.4936e-01 1.8049e-01 2.8790e-03 2.8016e-01 took: 47.42s\n",
      "net1 test acc: 2.030e-01 | net2 test acc: 2.023e-01\n",
      "Epoch  588,  98% \t acc: 9.7403e-01 9.7403e-01 - loss 3.3695e-01 1.7100e-01 2.8497e-03 2.7492e-01 took: 45.56s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  589,  98% \t acc: 9.7114e-01 9.7691e-01 - loss 3.2818e-01 1.5330e-01 2.6398e-03 2.9697e-01 took: 47.75s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  590,  98% \t acc: 9.7403e-01 9.7835e-01 - loss 3.2641e-01 1.5572e-01 3.5051e-03 2.7127e-01 took: 48.04s\n",
      "net1 test acc: 2.011e-01 | net2 test acc: 2.023e-01\n",
      "Epoch  591,  98% \t acc: 9.7691e-01 9.7980e-01 - loss 3.2546e-01 1.5069e-01 2.8617e-03 2.9231e-01 took: 48.03s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  592,  98% \t acc: 9.8413e-01 9.7835e-01 - loss 3.2990e-01 1.6302e-01 2.9543e-03 2.7468e-01 took: 48.17s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  593,  98% \t acc: 9.8701e-01 9.7114e-01 - loss 3.3477e-01 1.5877e-01 2.9501e-03 2.9302e-01 took: 47.75s\n",
      "net1 test acc: 2.011e-01 | net2 test acc: 2.041e-01\n",
      "Epoch  594,  98% \t acc: 9.8413e-01 9.7403e-01 - loss 2.9643e-01 1.2772e-01 3.4010e-03 2.6939e-01 took: 47.09s\n",
      "net1 test acc: 1.936e-01 | net2 test acc: 1.947e-01\n",
      "Epoch  595,  98% \t acc: 9.7547e-01 9.7691e-01 - loss 3.2083e-01 1.4458e-01 3.2368e-03 2.8776e-01 took: 48.41s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  596,  98% \t acc: 9.8124e-01 9.7835e-01 - loss 3.0854e-01 1.4455e-01 2.9596e-03 2.6880e-01 took: 46.07s\n",
      "net1 test acc: 2.011e-01 | net2 test acc: 2.023e-01\n",
      "Epoch  597,  98% \t acc: 9.7691e-01 9.8268e-01 - loss 3.2811e-01 1.5121e-01 3.1416e-03 2.9098e-01 took: 45.27s\n",
      "net1 test acc: 1.955e-01 | net2 test acc: 1.966e-01\n",
      "Epoch  598,  98% \t acc: 9.8124e-01 9.7258e-01 - loss 3.2366e-01 1.5370e-01 2.7639e-03 2.8464e-01 took: 47.29s\n",
      "net1 test acc: 1.974e-01 | net2 test acc: 1.985e-01\n",
      "Epoch  599,  98% \t acc: 9.7835e-01 9.7403e-01 - loss 3.3641e-01 1.6543e-01 3.1179e-03 2.7959e-01 took: 46.04s\n",
      "net1 test acc: 1.992e-01 | net2 test acc: 2.004e-01\n",
      "Epoch  600,  98% \t acc: 9.8124e-01 9.8413e-01 - loss 3.1419e-01 1.5087e-01 2.7863e-03 2.7092e-01 took: 47.86s\n",
      "net1 test acc: 1.898e-01 | net2 test acc: 1.928e-01\n"
     ]
    }
   ],
   "source": [
    "# Full Co-training\n",
    "nb_epoch = 600\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    SU_train(epoch, train_loader)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
