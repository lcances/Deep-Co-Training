{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/PyTorch/audio/torchaudio/extension/extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from DCT.util.utils import reset_seed, get_datetime, ZipCycle, track_maximum\n",
    "from DCT.util.model_loader import load_model\n",
    "from DCT.util.dataset_loader import load_dataset\n",
    "from DCT.util.optimizer_loader import load_optimizer\n",
    "from DCT.util.preprocess_loader import load_preprocesser\n",
    "from DCT.util.callbacks_loader import load_callbacks\n",
    "\n",
    "from DCT.ramps import Warmup, sigmoid_rampup\n",
    "from DCT.losses import loss_cot, loss_diff, loss_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets/\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"esc50\", type=str)\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"--model\", default=\"wideresnet28_8\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.1, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=600, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.003, type=int)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "group_m = parser.add_argument_group(\"Model parameters\")\n",
    "group_m.add_argument(\"--num_classes\", default=50, type=int)\n",
    "\n",
    "\n",
    "group_u = parser.add_argument_group(\"UrbanSound8k parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4,], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[5], type=int)\n",
    "\n",
    "group_h = parser.add_argument_group('hyperparameters')\n",
    "group_h.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "group_h.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "group_h.add_argument(\"--warmup_length\", default=80, type=int)\n",
    "group_h.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "group_a = parser.add_argument_group(\"Augmentation\")\n",
    "group_a.add_argument(\"--augment\", action=\"append\", help=\"augmentation. use as if python script\")\n",
    "group_a.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "group_a.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"deep-co-training\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"deep-co-training\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, val_transform = load_preprocesser(args.dataset, \"dct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "Dataset already downloaded and verified.\n",
      "Dataset already downloaded and verified.\n"
     ]
    }
   ],
   "source": [
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"dct\",\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    \n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = load_model(args.dataset, args.model)\n",
    "\n",
    "commun_args = dict(\n",
    "    manager=manager,\n",
    "    num_classes=args.num_classes\n",
    ")\n",
    "\n",
    "m1, m2 = model_func(**commun_args), model_func(**commun_args)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/esc50/deep-co-training/wideresnet28_8/0.1S/2020-09-25_10:23:30_wideresnet28_8_0.1S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = f\"{args.model}/{args.supervised_ratio}S/\" \\\n",
    "                    f\"{get_datetime()}_{model_func.__name__}_{args.supervised_ratio}S\"\n",
    "checkpoint_title = f\"{args.model}/{args.supervised_ratio}S/\" \\\n",
    "                   f\"{args.model}_{args.supervised_ratio}S\"\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=f\"{tensorboard_path}/{tensorboard_title}\",comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_params = {}\n",
    "for key, value in args.__dict__.items():\n",
    "    tensorboard_params[key] = str(value)\n",
    "tensorboard.add_hparams(tensorboard_params, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = load_optimizer(args.dataset, \"dct\", model1=m1, model2=m2, learning_rate=args.learning_rate)\n",
    "\n",
    "callbacks = load_callbacks(args.dataset, \"dct\", optimizer=optimizer, nb_epoch=args.nb_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# define the warmups & add them to the callbacks (for update)\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "callbacks += [lambda_cot, lambda_diff]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint = CheckPoint([m1, m2], optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "maximum_tracker = track_maximum()\n",
    "\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        x_s1, x_s2, x_u = x_s1.cuda(), x_s2.cuda(), x_u.cuda()\n",
    "        y_s1, y_s2, y_u = y_s1.cuda(), y_s2.cuda(), y_u.cuda()\n",
    "\n",
    "        with autocast():\n",
    "            logits_s1 = m1(x_s1)\n",
    "            logits_s2 = m2(x_s2)\n",
    "            logits_u1 = m1(x_u)\n",
    "            logits_u2 = m2(x_u)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "\n",
    "        # ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "\n",
    "        #generate adversarial examples ----\n",
    "        adv_data_s1 = adv_generator_1.perturb(x_s1, y_s1)\n",
    "        adv_data_u1 = adv_generator_1.perturb(x_u, pred_u1)\n",
    "\n",
    "        adv_data_s2 = adv_generator_2.perturb(x_s2, y_s2)\n",
    "        adv_data_u2 = adv_generator_2.perturb(x_u, pred_u2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        with autocast():\n",
    "            adv_logits_s1 = m1(adv_data_s2)\n",
    "            adv_logits_s2 = m2(adv_data_s1)\n",
    "\n",
    "            adv_logits_u1 = m1(adv_data_u2)\n",
    "            adv_logits_u2 = m2(adv_data_u1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "\n",
    "        # losses ----\n",
    "        with autocast():\n",
    "            l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "            l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "            l_diff = loss_diff(\n",
    "                logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
    "                logits_u1, logits_u2, adv_logits_u1, adv_logits_u2\n",
    "            )\n",
    "\n",
    "            total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "            # ratios  ----\n",
    "            adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "            adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "            adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "            adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "            ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "            ratio_s2 = metrics_fn[\"ratio_s\"][0](adv_pred_s2, y_s2)\n",
    "            ratio_u1 = metrics_fn[\"ratio_s\"][0](adv_pred_u1, y_u)\n",
    "            ratio_u2 = metrics_fn[\"ratio_s\"][0](adv_pred_u2, y_u)\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, avg_cot.mean, avg_diff.mean, avg_total.mean,\n",
    "                \"\", acc_s1.mean, acc_u1.mean,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits_1 = m1(x)\n",
    "            logits_2 = m2(x)\n",
    "\n",
    "            # losses ----\n",
    "            l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "        \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_tracker(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_tracker(\"acc_2\", acc_2.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint.step(acc_1.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
      "\n",
      "Training 1      - 100    -          9.6850 | 0.1515 | 9.1817 | 9.7479 -           0.0235    | 0.0255   - 28.5715253\n",
      "\u001b[1;4mValidati 1      - 23     -          16.4743 | 0.0000 | 0.0000 | 16.4743 -           0.0425    | 0.0000   - 5.6489\u001b[0m\n",
      "Training 2      - 100    -          7.8891 | 0.0577 | 8.0343 | 7.9241 -           0.0235    | 0.0314   - 28.4556\n",
      "\u001b[1;4mValidati 2      - 23     -          9.9658 | 0.0000 | 0.0000 | 9.9658 -           0.0275    | 0.0000   - 1.1009\u001b[0m0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 3      - 100    -          7.6046 | 0.0580 | 7.9160 | 7.6438 -           0.0471    | 0.0392   - 28.4170\n",
      "\u001b[1;4mValidati 3      - 23     -          8.0084 | 0.0000 | 0.0000 | 8.0084 -           0.0650    | 0.0000   - 1.0793\u001b[0m\n",
      "Training 4      - 100    -          7.2338 | 0.0694 | 7.7222 | 7.2781 -           0.0529    | 0.0588   - 28.3998\n",
      "\u001b[1;4mValidati 4      - 23     -          7.8434 | 0.0000 | 0.0000 | 7.8434 -           0.0675    | 0.0000   - 1.0772\u001b[0m\n",
      "Training 5      - 100    -          7.0395 | 0.0955 | 7.7092 | 7.0923 -           0.0353    | 0.0569   - 28.4258\n",
      "\u001b[1;4mValidati 5      - 23     -          7.8631 | 0.0000 | 0.0000 | 7.8631 -           0.0625    | 0.0000   - 1.0828\u001b[0m\n",
      "Training 6      - 100    -          6.7281 | 0.0831 | 7.3411 | 6.7836 -           0.0824    | 0.0817   - 28.4326\n",
      "\u001b[1;4mValidati 6      - 23     -          7.7457 | 0.0000 | 0.0000 | 7.7457 -           0.0975    | 0.0000   - 1.0727\u001b[0m\n",
      "Training 7      - 100    -          6.6215 | 0.0934 | 7.3510 | 6.6854 -           0.1000    | 0.0758   - 28.4506\n",
      "\u001b[1;4mValidati 7      - 23     -          7.5392 | 0.0000 | 0.0000 | 7.5392 -           0.0825    | 0.0000   - 1.0830\u001b[0m\n",
      "Training 8      - 100    -          6.8410 | 0.1117 | 7.3743 | 6.9158 -           0.0824    | 0.0752   - 28.4668\n",
      "\u001b[1;4mValidati 8      - 23     -          28.3380 | 0.0000 | 0.0000 | 28.3380 -           0.0925    | 0.0000   - 1.0752\u001b[0m\n",
      "Training 9      - 100    -          6.9222 | 0.1138 | 7.6442 | 7.0086 -           0.0824    | 0.0804   - 28.4720\n",
      "\u001b[1;4mValidati 9      - 23     -          10.3296 | 0.0000 | 0.0000 | 10.3296 -           0.1075    | 0.0000   - 1.0848\u001b[0m\n",
      "Training 10     - 100    -          6.5883 | 0.1214 | 7.3134 | 6.6832 -           0.1529    | 0.0824   - 28.4793\n",
      "\u001b[1;4mValidati 10     - 23     -          8.2045 | 0.0000 | 0.0000 | 8.2045 -           0.0800    | 0.0000   - 1.0753\u001b[0m\n",
      "Training 11     - 100    -          6.4939 | 0.1481 | 7.6075 | 6.6089 -           0.1000    | 0.0797   - 28.4697\n",
      "\u001b[1;4mValidati 11     - 23     -          8.1884 | 0.0000 | 0.0000 | 8.1884 -           0.0725    | 0.0000   - 1.0768\u001b[0m\n",
      "Training 12     - 100    -          6.4446 | 0.1107 | 7.2469 | 6.5593 -           0.0941    | 0.0954   - 28.4548\n",
      "\u001b[1;4mValidati 12     - 23     -          7.9927 | 0.0000 | 0.0000 | 7.9927 -           0.1000    | 0.0000   - 1.0851\u001b[0m\n",
      "Training 13     - 100    -          6.1345 | 0.1127 | 7.2597 | 6.2629 -           0.1353    | 0.0882   - 28.4566\n",
      "\u001b[1;4mValidati 13     - 23     -          7.4006 | 0.0000 | 0.0000 | 7.4006 -           0.1075    | 0.0000   - 1.0749\u001b[0m\n",
      "Training 14     - 100    -          5.7123 | 0.1208 | 7.1277 | 5.8554 -           0.1176    | 0.1183   - 28.4445\n",
      "\u001b[1;4mValidati 14     - 23     -          7.7666 | 0.0000 | 0.0000 | 7.7666 -           0.1050    | 0.0000   - 1.0749\u001b[0m\n",
      "Training 15     - 100    -          5.9900 | 0.1210 | 6.9863 | 6.1465 -           0.1471    | 0.1294   - 28.4562\n",
      "\u001b[1;4mValidati 15     - 23     -          7.5701 | 0.0000 | 0.0000 | 7.5701 -           0.0825    | 0.0000   - 1.0859\u001b[0m\n",
      "Training 16     - 100    -          5.6891 | 0.1329 | 7.1238 | 5.8694 -           0.1529    | 0.1248   - 28.4603\n",
      "\u001b[1;4mValidati 16     - 23     -          7.7115 | 0.0000 | 0.0000 | 7.7115 -           0.1275    | 0.0000   - 1.0764\u001b[0m\n",
      "Training 17     - 100    -          5.7183 | 0.1329 | 6.8621 | 5.9123 -           0.1529    | 0.1294   - 28.4532\n",
      "\u001b[1;4mValidati 17     - 23     -          7.6323 | 0.0000 | 0.0000 | 7.6323 -           0.1400    | 0.0000   - 1.0766\u001b[0m\n",
      "Training 18     - 100    -          5.5474 | 0.1393 | 6.9542 | 5.7666 -           0.1882    | 0.1314   - 28.4550\n",
      "\u001b[1;4mValidati 18     - 23     -          7.3575 | 0.0000 | 0.0000 | 7.3575 -           0.1275    | 0.0000   - 1.0833\u001b[0m\n",
      "Training 19     - 100    -          5.0739 | 0.1387 | 6.7776 | 5.3109 -           0.1765    | 0.1490   - 28.4567\n",
      "\u001b[1;4mValidati 19     - 23     -          7.8638 | 0.0000 | 0.0000 | 7.8638 -           0.1500    | 0.0000   - 1.0750\u001b[0m\n",
      "Training 20     - 100    -          5.0599 | 0.1414 | 6.5750 | 5.3168 -           0.2118    | 0.1739   - 28.4584\n",
      "\u001b[1;4mValidati 20     - 23     -          7.5305 | 0.0000 | 0.0000 | 7.5305 -           0.1325    | 0.0000   - 1.0758\u001b[0m\n",
      "Training 21     - 100    -          4.9960 | 0.1546 | 6.5309 | 5.2850 -           0.2059    | 0.1477   - 28.4466\n",
      "\u001b[1;4mValidati 21     - 23     -          7.4125 | 0.0000 | 0.0000 | 7.4125 -           0.1275    | 0.0000   - 1.0845\u001b[0m\n",
      "Training 22     - 100    -          4.9003 | 0.1622 | 6.5563 | 5.2233 -           0.2294    | 0.1739   - 28.4581\n",
      "\u001b[1;4mValidati 22     - 23     -          7.5853 | 0.0000 | 0.0000 | 7.5853 -           0.2000    | 0.0000   - 1.0747\u001b[0m\n",
      "Training 23     - 100    -          4.7923 | 0.1543 | 6.4426 | 5.1363 -           0.2412    | 0.1634   - 28.4499\n",
      "\u001b[1;4mValidati 23     - 23     -          7.7353 | 0.0000 | 0.0000 | 7.7353 -           0.1450    | 0.0000   - 1.0752\u001b[0m\n",
      "Training 24     - 100    -          4.7302 | 0.1433 | 6.2886 | 5.0918 -           0.2706    | 0.1784   - 28.4520\n",
      "\u001b[1;4mValidati 24     - 23     -          7.2406 | 0.0000 | 0.0000 | 7.2406 -           0.1925    | 0.0000   - 1.0839\u001b[0m\n",
      "Training 25     - 100    -          4.6071 | 0.1783 | 6.6689 | 5.0487 -           0.2647    | 0.1817   - 28.4463\n",
      "\u001b[1;4mValidati 25     - 23     -          8.7746 | 0.0000 | 0.0000 | 8.7746 -           0.1175    | 0.0000   - 1.0776\u001b[0m\n",
      "Training 26     - 100    -          4.3535 | 0.1657 | 6.4570 | 4.8133 -           0.3176    | 0.1582   - 28.4432\n",
      "\u001b[1;4mValidati 26     - 23     -          7.1522 | 0.0000 | 0.0000 | 7.1522 -           0.1975    | 0.0000   - 1.0739\u001b[0m\n",
      "Training 27     - 100    -          4.2532 | 0.1352 | 5.9469 | 4.6964 -           0.2706    | 0.1732   - 28.4348\n",
      "\u001b[1;4mValidati 27     - 23     -          7.2703 | 0.0000 | 0.0000 | 7.2703 -           0.1450    | 0.0000   - 1.0898\u001b[0m\n",
      "Training 28     - 100    -          4.1692 | 0.1534 | 5.9325 | 4.6705 -           0.2941    | 0.1810   - 28.5588\n",
      "\u001b[1;4mValidati 28     - 23     -          7.9606 | 0.0000 | 0.0000 | 7.9606 -           0.1275    | 0.0000   - 1.0737\u001b[0m\n",
      "Training 29     - 100    -          4.1352 | 0.1722 | 6.1517 | 4.7155 -           0.3118    | 0.1837   - 28.4362\n",
      "\u001b[1;4mValidati 29     - 23     -          7.8617 | 0.0000 | 0.0000 | 7.8617 -           0.1700    | 0.0000   - 1.0736\u001b[0m\n",
      "Training 30     - 88     -          4.2582 | 0.1951 | 6.3828 | 4.9322 -           0.2533    | 0.1630   - 26.2988\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "    tensorboard.flush()\n",
    "    \n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
