{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "from ubs8k.datasetManager import DatasetManager\n",
    "from ubs8k.datasets import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from metric_utils.metrics import CategoricalAccuracy, FScore, ContinueAverage, Ratio\n",
    "from DCT.util.checkpoint import CheckPoint\n",
    "from DCT.util.utils import reset_seed, get_datetime, ZipCycle\n",
    "from DCT.util.model_loader import get_model_from_name\n",
    "from DCT.util.dataset_loader import load_dataset\n",
    "\n",
    "from DCT.ramps import Warmup, sigmoid_rampup\n",
    "from DCT.losses import loss_cot, loss_diff, loss_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--from_config\", default=\"\", type=str)\n",
    "parser.add_argument(\"-d\", \"--dataset_root\", default=\"../datasets\", type=str)\n",
    "parser.add_argument(\"-D\", \"--dataset\", default=\"cifar10\", type=str, help=\"available [ubs8k | cifar10]\")\n",
    "\n",
    "group_t = parser.add_argument_group(\"Commun parameters\")\n",
    "group_t.add_argument(\"--model\", default=\"wideresnet28_2\", type=str)\n",
    "group_t.add_argument(\"--supervised_ratio\", default=0.08, type=float)\n",
    "# parser.add_argument(\"--supervised_mult\", default=1.0, type=float)\n",
    "group_t.add_argument(\"--batch_size\", default=100, type=int)\n",
    "group_t.add_argument(\"--nb_epoch\", default=100, type=int)\n",
    "group_t.add_argument(\"--learning_rate\", default=0.003, type=int)\n",
    "group_t.add_argument(\"--resume\", action=\"store_true\", default=False)\n",
    "group_t.add_argument(\"--seed\", default=1234, type=int)\n",
    "\n",
    "\n",
    "group_u = parser.add_argument_group(\"UrbanSound8k parameters\")\n",
    "group_u.add_argument(\"-t\", \"--train_folds\", nargs=\"+\", default=[1, 2, 3, 4, 5, 6, 7, 8, 9], type=int)\n",
    "group_u.add_argument(\"-v\", \"--val_folds\", nargs=\"+\", default=[10], type=int)\n",
    "\n",
    "group_h = parser.add_argument_group('hyperparameters')\n",
    "group_h.add_argument(\"--lambda_cot_max\", default=10, type=float)\n",
    "group_h.add_argument(\"--lambda_diff_max\", default=0.5, type=float)\n",
    "group_h.add_argument(\"--warmup_length\", default=80, type=int)\n",
    "group_h.add_argument(\"--epsilon\", default=0.02, type=float)\n",
    "\n",
    "group_a = parser.add_argument_group(\"Augmentation\")\n",
    "group_a.add_argument(\"--augment\", action=\"append\", help=\"augmentation. use as if python script\")\n",
    "group_a.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "group_a.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "\n",
    "group_l = parser.add_argument_group(\"Logs\")\n",
    "group_l.add_argument(\"--checkpoint_root\", default=\"../model_save/\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_root\", default=\"../tensorboard/\", type=str)\n",
    "group_l.add_argument(\"--checkpoint_path\", default=\"deep-co-training\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_path\", default=\"deep-co-trainin\", type=str)\n",
    "group_l.add_argument(\"--tensorboard_sufix\", default=\"\", type=str)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "tensorboard_path = os.path.join(args.tensorboard_root, args.dataset, args.tensorboard_path)\n",
    "checkpoint_path = os.path.join(args.checkpoint_root, args.dataset, args.checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "we pre-processed the images using ZCA and augmented the dataset using horizontal flips and random translations. The translations\n",
    "were drawn from [âˆ’2, 2] pixels,\n",
    "\"\"\"\n",
    "extra_train_transforms = [\n",
    "    transforms.Pad(4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "]\n",
    "\n",
    "manager, train_loader, val_loader = load_dataset(\n",
    "    args.dataset,\n",
    "    \"dct\",\n",
    "    \n",
    "    extra_train_transform = extra_train_transforms,\n",
    "    \n",
    "    dataset_root = args.dataset_root,\n",
    "    supervised_ratio = args.supervised_ratio,\n",
    "    batch_size = args.batch_size,\n",
    "    train_folds = args.train_folds,\n",
    "    val_folds = args.val_folds,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_func = get_model_from_name(args.model)\n",
    "\n",
    "m1, m2 = model_func(manager=manager), model_func(manager=manager)\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard/cifar10/deep-co-trainin/2020-08-27_12:01:17_wideresnet28_2_0.1S\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "tensorboard_title = \"%s_%s_%.1fS\" % (get_datetime(), model_func.__name__, args.supervised_ratio)\n",
    "checkpoint_title = \"%s_%.1fS\" % (model_func.__name__, args.supervised_ratio)\n",
    "tensorboard = SummaryWriter(log_dir=\"%s/%s\" % (tensorboard_path, tensorboard_title), comment=model_func.__name__)\n",
    "print(os.path.join(tensorboard_path, tensorboard_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar10 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    params = list(m1.parameters()) + list(m2.parameters())\n",
    "    optimizer = torch.optim.SGD(params, lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    def lr_lambda(e):\n",
    "        if e < 60:\n",
    "            return 1\n",
    "\n",
    "        elif 60 <= e < 120:\n",
    "            return 0.2\n",
    "\n",
    "        elif 120 <= e < 160:\n",
    "            return 0.04\n",
    "\n",
    "        else:\n",
    "            return 0.008\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ubs8k optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "if args.dataset == \"ubs8k\":\n",
    "    params = list(m1.parameters()) + list(m2.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
    "    lr_lambda = lambda epoch: (1.0 + numpy.cos((epoch-1)*numpy.pi/args.nb_epoch)) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    m1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=0, clip_max=1, targeted=True\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    m2, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=0, clip_max=1, targeted=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "# Losses\n",
    "# see losses.py\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warmup_length, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warmup_length, sigmoid_rampup)\n",
    "\n",
    "# callback\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "callbacks = [lr_scheduler, lambda_cot, lambda_diff]\n",
    "\n",
    "# checkpoints\n",
    "checkpoint_m1 = CheckPoint(m1, optimizer, mode=\"max\", name=\"%s/%s_m1.torch\" % (checkpoint_path, checkpoint_title))\n",
    "\n",
    "# metrics\n",
    "metrics_fn = dict(\n",
    "    ratio_s=[Ratio(), Ratio()],\n",
    "    ratio_u=[Ratio(), Ratio()],\n",
    "    acc_s=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    acc_u=[CategoricalAccuracy(), CategoricalAccuracy()],\n",
    "    f1_s=[FScore(), FScore()],\n",
    "    f1_u=[FScore(), FScore()],\n",
    "    \n",
    "    avg_total=ContinueAverage(),\n",
    "    avg_sup=ContinueAverage(),\n",
    "    avg_cot=ContinueAverage(),\n",
    "    avg_diff=ContinueAverage(),\n",
    ")\n",
    "\n",
    "def reset_metrics():\n",
    "    for item in metrics_fn.values():\n",
    "        if isinstance(item, list):\n",
    "            for f in item:\n",
    "                f.reset()\n",
    "        else:\n",
    "            item.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can resume previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint_m1.load_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def maximum():\n",
    "    def func(key, value):\n",
    "        if key not in func.max:\n",
    "            func.max[key] = value\n",
    "        else:\n",
    "            if func.max[key] < value:\n",
    "                func.max[key] = value\n",
    "        return func.max[key]\n",
    "\n",
    "    func.max = dict()\n",
    "    return func\n",
    "maximum_fn = maximum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} | {:<6.6} | {:<6.6} | {:<6.6} - {:<9.9} {:<9.9} | {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} | {:<6.4f} | {:<6.4f} | {:<6.4f} - {:<9.9} {:<9.4f} | {:<9.4f}- {:<6.4f}\"\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Lsup\", \"Lcot\", \"Ldiff\", \"total\", \"metrics: \", \"acc_s1\", \"acc_u1\",\"Time\"\n",
    ")\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "\n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        x_s1, y_s1 = S1\n",
    "        x_s2, y_s2 = S2\n",
    "        x_u, y_u = U\n",
    "\n",
    "        x_s1, x_s2, x_u = x_s1.cuda(), x_s2.cuda(), x_u.cuda()\n",
    "        y_s1, y_s2, y_u = y_s1.cuda(), y_s2.cuda(), y_u.cuda()\n",
    "\n",
    "        logits_s1 = m1(x_s1)\n",
    "        logits_s2 = m2(x_s2)\n",
    "        logits_u1 = m1(x_u)\n",
    "        logits_u2 = m2(x_u)\n",
    "\n",
    "        # pseudo labels of U\n",
    "        pred_u1 = torch.argmax(logits_u1, 1)\n",
    "        pred_u2 = torch.argmax(logits_u2, 1)\n",
    "\n",
    "        # ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        m1.eval()\n",
    "        m2.eval()\n",
    "\n",
    "        #generate adversarial examples ----\n",
    "        adv_data_s1 = adv_generator_1.perturb(x_s1, y_s1)\n",
    "        adv_data_u1 = adv_generator_1.perturb(x_u, pred_u1)\n",
    "\n",
    "        adv_data_s2 = adv_generator_2.perturb(x_s2, y_s2)\n",
    "        adv_data_u2 = adv_generator_2.perturb(x_u, pred_u2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        adv_logits_s1 = m1(adv_data_s2)\n",
    "        adv_logits_s2 = m2(adv_data_s1)\n",
    "\n",
    "        adv_logits_u1 = m1(adv_data_u2)\n",
    "        adv_logits_u2 = m2(adv_data_u1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "\n",
    "        # losses ----\n",
    "        l_sup = loss_sup(logits_s1, logits_s2, y_s1, y_s2)\n",
    "\n",
    "        l_cot = loss_cot(logits_u1, logits_u2)\n",
    "\n",
    "        l_diff = loss_diff(\n",
    "            logits_s1, logits_s2, adv_logits_s1, adv_logits_s2,\n",
    "            logits_u1, logits_u2, adv_logits_u1, adv_logits_u2\n",
    "        )\n",
    "\n",
    "        total_loss = l_sup + lambda_cot() * l_cot + lambda_diff() * l_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # accuracies ----\n",
    "            pred_s1 = torch.argmax(logits_s1, dim=1)\n",
    "            pred_s2 = torch.argmax(logits_s2, dim=1)\n",
    "\n",
    "            acc_s1 = metrics_fn[\"acc_s\"][0](pred_s1, y_s1)\n",
    "            acc_s2 = metrics_fn[\"acc_s\"][1](pred_s2, y_s2)\n",
    "            acc_u1 = metrics_fn[\"acc_u\"][0](pred_u1, y_u)\n",
    "            acc_u2 = metrics_fn[\"acc_u\"][1](pred_u2, y_u)\n",
    "\n",
    "            # ratios  ----\n",
    "            adv_pred_s1 = torch.argmax(adv_logits_s1, 1)\n",
    "            adv_pred_s2 = torch.argmax(adv_logits_s2, 1)\n",
    "            adv_pred_u1 = torch.argmax(adv_logits_u1, 1)\n",
    "            adv_pred_u2 = torch.argmax(adv_logits_u2, 1)\n",
    "\n",
    "            ratio_s1 = metrics_fn[\"ratio_s\"][0](adv_pred_s1, y_s1)\n",
    "            ratio_s2 = metrics_fn[\"ratio_s\"][0](adv_pred_s2, y_s2)\n",
    "            ratio_u1 = metrics_fn[\"ratio_s\"][0](adv_pred_u1, y_u)\n",
    "            ratio_u2 = metrics_fn[\"ratio_s\"][0](adv_pred_u2, y_u)\n",
    "            # ========\n",
    "\n",
    "            avg_total = metrics_fn[\"avg_total\"](total_loss.item())\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "            avg_diff = metrics_fn[\"avg_diff\"](l_diff.item())\n",
    "            avg_cot = metrics_fn[\"avg_cot\"](l_cot.item())\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, avg_cot.mean, avg_diff.mean, avg_total.mean,\n",
    "                \"\", acc_s1.mean, acc_u1.mean,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', avg_total.mean, epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', avg_sup.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', avg_cot.mean, epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', avg_diff.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_1\", acc_s1.mean, epoch )\n",
    "    tensorboard.add_scalar(\"train/acc_2\", acc_s2.mean, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s1\", acc_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_s2\", acc_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u1\", acc_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/acc_u2\", acc_u2.mean, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s1\", ratio_s1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_s2\", ratio_s2.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u1\", ratio_u1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_u2\", ratio_u2.mean, epoch)\n",
    "\n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [],
   "source": [
    "def test(epoch, msg = \"\"):\n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    reset_metrics()\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            x = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            logits_1 = m1(x)\n",
    "            logits_2 = m2(x)\n",
    "\n",
    "            # losses ----\n",
    "            l_sup = loss_sup(logits_1, logits_2, y, y)\n",
    "\n",
    "            # ======== Calc the metrics ========\n",
    "            # accuracies ----\n",
    "            pred_1 = torch.argmax(logits_1, dim=1)\n",
    "            pred_2 = torch.argmax(logits_2, dim=1)\n",
    "\n",
    "            acc_1 = metrics_fn[\"acc_s\"][0](pred_1, y)\n",
    "            acc_2 = metrics_fn[\"acc_s\"][1](pred_2, y)\n",
    "\n",
    "            avg_sup = metrics_fn[\"avg_sup\"](l_sup.item())\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (batch + 1) / len(train_loader)),\n",
    "                \"\", avg_sup.mean, 0.0, 0.0, avg_sup.mean,\n",
    "                \"\", acc_1.mean, 0.0,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    tensorboard.add_scalar(\"val/acc_1\", acc_1.mean, epoch)\n",
    "    tensorboard.add_scalar(\"val/acc_2\", acc_2.mean, epoch)\n",
    "        \n",
    "    tensorboard.add_scalar(\"max/acc_1\", maximum_fn(\"acc_1\", acc_1.mean), epoch )\n",
    "    tensorboard.add_scalar(\"max/acc_2\", maximum_fn(\"acc_2\", acc_2.mean), epoch )\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n",
    "\n",
    "    # call checkpoint\n",
    "    checkpoint_m1.step(acc_1.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "re_run"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Lsup   | Lcot   | Ldiff  | total  - metrics:  acc_s1    | acc_u1   - Time  \n",
      "\n",
      "Training 1      - 100    -          4.6605 | 0.0019 | 4.5726 | 6.9659 -           0.1018    | 0.1002   - 134.0019\n",
      "\u001b[1;4mValidati 1      - 20     -          6.7738 | 0.0000 | 0.0000 | 6.7738 -           0.0963    | 0.0000   - 2.8441\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 2      - 100    -          4.6281 | 0.0234 | 4.7172 | 4.6479 -           0.0923    | 0.1022   - 113.4253\n",
      "\u001b[1;4mValidati 2      - 20     -          5.3247 | 0.0000 | 0.0000 | 5.3247 -           0.0955    | 0.0000   - 2.7414\u001b[0m\n",
      "Training 3      - 100    -          4.4994 | 0.0582 | 4.8840 | 4.5255 -           0.1043    | 0.1016   - 104.0670\n",
      "\u001b[1;4mValidati 3      - 20     -          4.7966 | 0.0000 | 0.0000 | 4.7966 -           0.1042    | 0.0000   - 2.7343\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 4      - 100    -          4.4510 | 0.0697 | 4.9432 | 4.4818 -           0.0958    | 0.1017   - 103.7856\n",
      "\u001b[1;4mValidati 4      - 20     -          4.7322 | 0.0000 | 0.0000 | 4.7322 -           0.1108    | 0.0000   - 2.7268\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 5      - 100    -          4.4145 | 0.0803 | 5.0196 | 4.4508 -           0.1075    | 0.0999   - 109.2855\n",
      "\u001b[1;4mValidati 5      - 20     -          5.7869 | 0.0000 | 0.0000 | 5.7869 -           0.1231    | 0.0000   - 3.3447\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 6      - 100    -          4.3613 | 0.0902 | 5.0457 | 4.4036 -           0.1165    | 0.1150   - 124.5874\n",
      "\u001b[1;4mValidati 6      - 20     -          4.1382 | 0.0000 | 0.0000 | 4.1382 -           0.1345    | 0.0000   - 2.8455\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 7      - 100    -          4.1393 | 0.0908 | 4.8575 | 4.1855 -           0.1720    | 0.1742   - 131.7068\n",
      "\u001b[1;4mValidati 7      - 20     -          4.3658 | 0.0000 | 0.0000 | 4.3658 -           0.1332    | 0.0000   - 2.8516\u001b[0m\n",
      "Training 8      - 100    -          4.0099 | 0.0879 | 4.7175 | 4.0602 -           0.2148    | 0.2139   - 130.9706\n",
      "\u001b[1;4mValidati 8      - 20     -          4.1051 | 0.0000 | 0.0000 | 4.1051 -           0.1940    | 0.0000   - 3.3490\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 9      - 100    -          3.8816 | 0.0885 | 4.6307 | 3.9374 -           0.2423    | 0.2486   - 125.1594\n",
      "\u001b[1;4mValidati 9      - 20     -          4.3100 | 0.0000 | 0.0000 | 4.3100 -           0.2371    | 0.0000   - 2.8486\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 10     - 100    -          3.7812 | 0.0878 | 4.5999 | 3.8431 -           0.2585    | 0.2727   - 116.7563\n",
      "\u001b[1;4mValidati 10     - 20     -          3.9290 | 0.0000 | 0.0000 | 3.9290 -           0.2635    | 0.0000   - 2.7595\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 11     - 100    -          3.7252 | 0.0947 | 4.6197 | 3.7961 -           0.2780    | 0.2881   - 140.1591\n",
      "\u001b[1;4mValidati 11     - 20     -          3.7559 | 0.0000 | 0.0000 | 3.7559 -           0.3139    | 0.0000   - 3.3332\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 12     - 100    -          3.6338 | 0.0942 | 4.5414 | 3.7117 -           0.3108    | 0.3147   - 136.7532\n",
      "\u001b[1;4mValidati 12     - 20     -          3.8827 | 0.0000 | 0.0000 | 3.8827 -           0.2770    | 0.0000   - 3.3536\u001b[0m\n",
      "Training 13     - 100    -          3.5559 | 0.0910 | 4.4746 | 3.6408 -           0.3165    | 0.3330   - 131.3918\n",
      "\u001b[1;4mValidati 13     - 20     -          5.5464 | 0.0000 | 0.0000 | 5.5464 -           0.2426    | 0.0000   - 3.2083\u001b[0m\n",
      "Training 14     - 100    -          3.4791 | 0.0921 | 4.4243 | 3.5731 -           0.3430    | 0.3490   - 140.6275\n",
      "\u001b[1;4mValidati 14     - 20     -          3.9097 | 0.0000 | 0.0000 | 3.9097 -           0.3352    | 0.0000   - 3.3461\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 15     - 100    -          3.4119 | 0.0990 | 4.4282 | 3.5185 -           0.3553    | 0.3666   - 136.7999\n",
      "\u001b[1;4mValidati 15     - 20     -          3.8853 | 0.0000 | 0.0000 | 3.8853 -           0.3597    | 0.0000   - 3.3498\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 16     - 100    -          3.2831 | 0.0991 | 4.3572 | 3.3999 -           0.3775    | 0.3917   - 118.7730\n",
      "\u001b[1;4mValidati 16     - 20     -          3.8670 | 0.0000 | 0.0000 | 3.8670 -           0.3702    | 0.0000   - 2.8909\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 17     - 100    -          3.2183 | 0.1028 | 4.3162 | 3.3482 -           0.4018    | 0.4073   - 132.9480\n",
      "\u001b[1;4mValidati 17     - 20     -          3.3360 | 0.0000 | 0.0000 | 3.3360 -           0.3899    | 0.0000   - 3.3539\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 18     - 100    -          3.1542 | 0.1000 | 4.2592 | 3.2951 -           0.4133    | 0.4164   - 119.0267\n",
      "\u001b[1;4mValidati 18     - 20     -          3.2680 | 0.0000 | 0.0000 | 3.2680 -           0.3822    | 0.0000   - 2.8539\u001b[0m\n",
      "Training 19     - 100    -          3.0326 | 0.1063 | 4.2533 | 3.1908 -           0.4295    | 0.4285   - 116.3183\n",
      "\u001b[1;4mValidati 19     - 20     -          3.9128 | 0.0000 | 0.0000 | 3.9128 -           0.3734    | 0.0000   - 2.7511\u001b[0m\n",
      "Training 20     - 100    -          2.9937 | 0.1010 | 4.1535 | 3.1624 -           0.4455    | 0.4431   - 134.5207\n",
      "\u001b[1;4mValidati 20     - 20     -          3.4704 | 0.0000 | 0.0000 | 3.4704 -           0.4481    | 0.0000   - 2.8627\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 21     - 100    -          2.9036 | 0.1026 | 4.1320 | 3.0893 -           0.4625    | 0.4639   - 133.3663\n",
      "\u001b[1;4mValidati 21     - 20     -          3.7049 | 0.0000 | 0.0000 | 3.7049 -           0.4005    | 0.0000   - 2.8639\u001b[0m\n",
      "Training 22     - 100    -          2.8532 | 0.1071 | 4.1423 | 3.0603 -           0.4840    | 0.4725   - 121.4017\n",
      "\u001b[1;4mValidati 22     - 20     -          3.4524 | 0.0000 | 0.0000 | 3.4524 -           0.4569    | 0.0000   - 2.7384\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 23     - 100    -          2.7435 | 0.1060 | 4.0475 | 2.9662 -           0.4900    | 0.4841   - 134.6132\n",
      "\u001b[1;4mValidati 23     - 20     -          3.3410 | 0.0000 | 0.0000 | 3.3410 -           0.4685    | 0.0000   - 2.8590\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 24     - 100    -          2.6694 | 0.1073 | 4.0367 | 2.9137 -           0.5173    | 0.4863   - 125.5674\n",
      "\u001b[1;4mValidati 24     - 20     -          3.5875 | 0.0000 | 0.0000 | 3.5875 -           0.4378    | 0.0000   - 2.8495\u001b[0m\n",
      "Training 25     - 100    -          2.6486 | 0.1036 | 3.9607 | 2.9089 -           0.5145    | 0.4943   - 121.1874\n",
      "\u001b[1;4mValidati 25     - 20     -          3.6272 | 0.0000 | 0.0000 | 3.6272 -           0.4583    | 0.0000   - 2.8551\u001b[0m\n",
      "Training 26     - 100    -          2.5253 | 0.1083 | 3.9312 | 2.8123 -           0.5363    | 0.5050   - 140.4983\n",
      "\u001b[1;4mValidati 26     - 20     -          3.2868 | 0.0000 | 0.0000 | 3.2868 -           0.4605    | 0.0000   - 3.3460\u001b[0m\n",
      "Training 27     - 100    -          2.4807 | 0.1068 | 3.9151 | 2.7907 -           0.5483    | 0.5079   - 113.7231\n",
      "\u001b[1;4mValidati 27     - 20     -          3.3477 | 0.0000 | 0.0000 | 3.3477 -           0.5127    | 0.0000   - 2.7240\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 28     - 100    -          2.4175 | 0.1060 | 3.8422 | 2.7496 -           0.5645    | 0.5250   - 116.4060\n",
      "\u001b[1;4mValidati 28     - 20     -          3.1103 | 0.0000 | 0.0000 | 3.1103 -           0.4937    | 0.0000   - 2.9183\u001b[0m\n",
      "Training 29     - 100    -          2.3254 | 0.1068 | 3.8138 | 2.6851 -           0.5745    | 0.5228   - 128.4565\n",
      "\u001b[1;4mValidati 29     - 20     -          3.0181 | 0.0000 | 0.0000 | 3.0181 -           0.4707    | 0.0000   - 3.3446\u001b[0m\n",
      "Training 30     - 100    -          2.2825 | 0.1051 | 3.7708 | 2.6673 -           0.5893    | 0.5286   - 139.1497\n",
      "\u001b[1;4mValidati 30     - 20     -          3.3193 | 0.0000 | 0.0000 | 3.3193 -           0.5174    | 0.0000   - 2.9130\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 31     - 100    -          2.2313 | 0.1017 | 3.7079 | 2.6384 -           0.6013    | 0.5333   - 127.2766\n",
      "\u001b[1;4mValidati 31     - 20     -          3.0114 | 0.0000 | 0.0000 | 3.0114 -           0.4912    | 0.0000   - 2.8665\u001b[0m\n",
      "Training 32     - 100    -          2.1231 | 0.1005 | 3.6578 | 2.5574 -           0.6108    | 0.5390   - 125.2943\n",
      "\u001b[1;4mValidati 32     - 20     -          3.1253 | 0.0000 | 0.0000 | 3.1253 -           0.4973    | 0.0000   - 2.8613\u001b[0m\n",
      "Training 33     - 100    -          2.1144 | 0.1038 | 3.6732 | 2.5895 -           0.6173    | 0.5459   - 113.4774\n",
      "\u001b[1;4mValidati 33     - 20     -          2.9204 | 0.0000 | 0.0000 | 2.9204 -           0.5186    | 0.0000   - 2.7869\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 34     - 100    -          2.0885 | 0.1031 | 3.6158 | 2.5940 -           0.6170    | 0.5435   - 104.4305\n",
      "\u001b[1;4mValidati 34     - 20     -          2.9433 | 0.0000 | 0.0000 | 2.9433 -           0.5602    | 0.0000   - 2.8485\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 35     - 100    -          1.9756 | 0.1025 | 3.5691 | 2.5134 -           0.6560    | 0.5532   - 131.9849\n",
      "\u001b[1;4mValidati 35     - 20     -          3.1279 | 0.0000 | 0.0000 | 3.1279 -           0.5109    | 0.0000   - 2.8563\u001b[0m\n",
      "Training 36     - 100    -          1.9303 | 0.0989 | 3.4899 | 2.4922 -           0.6488    | 0.5512   - 106.7506\n",
      "\u001b[1;4mValidati 36     - 20     -          3.5939 | 0.0000 | 0.0000 | 3.5939 -           0.4532    | 0.0000   - 2.7548\u001b[0m\n",
      "Training 37     - 100    -          1.8617 | 0.0987 | 3.4617 | 2.4606 -           0.6580    | 0.5588   - 118.1442\n",
      "\u001b[1;4mValidati 37     - 20     -          2.8839 | 0.0000 | 0.0000 | 2.8839 -           0.5440    | 0.0000   - 3.3445\u001b[0m\n",
      "Training 38     - 100    -          1.8002 | 0.0961 | 3.3959 | 2.4272 -           0.6725    | 0.5616   - 104.7086\n",
      "\u001b[1;4mValidati 38     - 20     -          3.2361 | 0.0000 | 0.0000 | 3.2361 -           0.5263    | 0.0000   - 2.7768\u001b[0m\n",
      "Training 39     - 100    -          1.7682 | 0.0937 | 3.3567 | 2.4273 -           0.6918    | 0.5631   - 114.0740\n",
      "\u001b[1;4mValidati 39     - 20     -          2.8294 | 0.0000 | 0.0000 | 2.8294 -           0.5319    | 0.0000   - 3.3323\u001b[0m\n",
      "Training 40     - 100    -          1.6842 | 0.0943 | 3.3134 | 2.3834 -           0.6938    | 0.5683   - 132.6034\n",
      "\u001b[1;4mValidati 40     - 20     -          2.7272 | 0.0000 | 0.0000 | 2.7272 -           0.5595    | 0.0000   - 2.8594\u001b[0m\n",
      "Training 41     - 100    -          1.6450 | 0.0917 | 3.2444 | 2.3724 -           0.7063    | 0.5751   - 131.9176\n",
      "\u001b[1;4mValidati 41     - 20     -          3.4658 | 0.0000 | 0.0000 | 3.4658 -           0.4908    | 0.0000   - 2.8594\u001b[0m\n",
      "Training 42     - 100    -          1.5673 | 0.0916 | 3.2005 | 2.3342 -           0.7225    | 0.5794   - 121.0938\n",
      "\u001b[1;4mValidati 42     - 20     -          3.3385 | 0.0000 | 0.0000 | 3.3385 -           0.4977    | 0.0000   - 2.8596\u001b[0m\n",
      "Training 43     - 100    -          1.5425 | 0.0884 | 3.1440 | 2.3372 -           0.7268    | 0.5762   - 124.1442\n",
      "\u001b[1;4mValidati 43     - 20     -          3.0982 | 0.0000 | 0.0000 | 3.0982 -           0.5759    | 0.0000   - 2.8409\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 44     - 100    -          1.5154 | 0.0900 | 3.1541 | 2.3654 -           0.7275    | 0.5796   - 132.6107\n",
      "\u001b[1;4mValidati 44     - 20     -          2.8121 | 0.0000 | 0.0000 | 2.8121 -           0.5656    | 0.0000   - 3.4595\u001b[0m\n",
      "Training 45     - 100    -          1.4899 | 0.0869 | 3.0694 | 2.3633 -           0.7403    | 0.5890   - 136.7076\n",
      "\u001b[1;4mValidati 45     - 20     -          2.6054 | 0.0000 | 0.0000 | 2.6054 -           0.5744    | 0.0000   - 3.3612\u001b[0m\n",
      "Training 46     - 100    -          1.4279 | 0.0869 | 3.0592 | 2.3492 -           0.7510    | 0.5911   - 130.9713\n",
      "\u001b[1;4mValidati 46     - 20     -          2.7718 | 0.0000 | 0.0000 | 2.7718 -           0.5641    | 0.0000   - 2.8725\u001b[0m\n",
      "Training 47     - 100    -          1.4120 | 0.0853 | 3.0121 | 2.3679 -           0.7445    | 0.5889   - 125.1334\n",
      "\u001b[1;4mValidati 47     - 20     -          2.7029 | 0.0000 | 0.0000 | 2.7029 -           0.5694    | 0.0000   - 3.3649\u001b[0m\n",
      "Training 48     - 100    -          1.3407 | 0.0817 | 2.9331 | 2.3159 -           0.7630    | 0.5965   - 117.1139\n",
      "\u001b[1;4mValidati 48     - 20     -          2.7573 | 0.0000 | 0.0000 | 2.7573 -           0.5612    | 0.0000   - 2.8735\u001b[0m\n",
      "Training 49     - 100    -          1.3143 | 0.0802 | 2.8882 | 2.3237 -           0.7570    | 0.5973   - 137.6735\n",
      "\u001b[1;4mValidati 49     - 20     -          2.9914 | 0.0000 | 0.0000 | 2.9914 -           0.5635    | 0.0000   - 2.8668\u001b[0m\n",
      "Training 50     - 100    -          1.3177 | 0.0799 | 2.8770 | 2.3738 -           0.7785    | 0.6032   - 110.0528\n",
      "\u001b[1;4mValidati 50     - 20     -          2.6393 | 0.0000 | 0.0000 | 2.6393 -           0.5735    | 0.0000   - 2.7445\u001b[0m\n",
      "Training 51     - 100    -          1.2506 | 0.0774 | 2.8106 | 2.3293 -           0.7850    | 0.6043   - 134.0743\n",
      "\u001b[1;4mValidati 51     - 20     -          2.7910 | 0.0000 | 0.0000 | 2.7910 -           0.6048    | 0.0000   - 3.3530\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 52     - 100    -          1.2299 | 0.0776 | 2.7846 | 2.3538 -           0.7785    | 0.6036   - 138.7328\n",
      "\u001b[1;4mValidati 52     - 20     -          2.9299 | 0.0000 | 0.0000 | 2.9299 -           0.5531    | 0.0000   - 2.8450\u001b[0m\n",
      "Training 53     - 100    -          1.2031 | 0.0743 | 2.7287 | 2.3450 -           0.7855    | 0.6122   - 138.7259\n",
      "\u001b[1;4mValidati 53     - 20     -          2.9364 | 0.0000 | 0.0000 | 2.9364 -           0.5426    | 0.0000   - 2.9921\u001b[0m\n",
      "Training 54     - 100    -          1.1302 | 0.0762 | 2.7115 | 2.3283 -           0.8070    | 0.6175   - 103.8284\n",
      "\u001b[1;4mValidati 54     - 20     -          3.1410 | 0.0000 | 0.0000 | 3.1410 -           0.5610    | 0.0000   - 2.8594\u001b[0m\n",
      "Training 55     - 100    -          1.1586 | 0.0740 | 2.6657 | 2.3809 -           0.7958    | 0.6158   - 127.7622\n",
      "\u001b[1;4mValidati 55     - 20     -          2.6892 | 0.0000 | 0.0000 | 2.6892 -           0.5792    | 0.0000   - 2.8533\u001b[0m\n",
      "Training 56     - 100    -          1.0921 | 0.0726 | 2.6027 | 2.3363 -           0.8003    | 0.6159   - 137.8717\n",
      "\u001b[1;4mValidati 56     - 20     -          2.9620 | 0.0000 | 0.0000 | 2.9620 -           0.5476    | 0.0000   - 2.8392\u001b[0m\n",
      "Training 57     - 100    -          1.1035 | 0.0716 | 2.6054 | 2.3905 -           0.8175    | 0.6192   - 103.8703\n",
      "\u001b[1;4mValidati 57     - 20     -          2.7127 | 0.0000 | 0.0000 | 2.7127 -           0.5742    | 0.0000   - 2.7694\u001b[0m\n",
      "Training 58     - 100    -          1.0888 | 0.0702 | 2.5689 | 2.4029 -           0.8118    | 0.6238   - 106.8500\n",
      "\u001b[1;4mValidati 58     - 20     -          2.6885 | 0.0000 | 0.0000 | 2.6885 -           0.5946    | 0.0000   - 3.3249\u001b[0m\n",
      "Training 59     - 100    -          1.0674 | 0.0696 | 2.5383 | 2.4139 -           0.8100    | 0.6283   - 130.0290\n",
      "\u001b[1;4mValidati 59     - 20     -          2.6643 | 0.0000 | 0.0000 | 2.6643 -           0.6028    | 0.0000   - 3.3375\u001b[0m\n",
      "Training 60     - 100    -          1.0359 | 0.0677 | 2.4970 | 2.4003 -           0.8220    | 0.6305   - 111.2729\n",
      "\u001b[1;4mValidati 60     - 20     -          2.7655 | 0.0000 | 0.0000 | 2.7655 -           0.5953    | 0.0000   - 2.8569\u001b[0m\n",
      "Training 61     - 100    -          0.4312 | 0.0325 | 1.8464 | 1.3442 -           0.9358    | 0.6744   - 108.9186\n",
      "\u001b[1;4mValidati 61     - 20     -          2.4595 | 0.0000 | 0.0000 | 2.4595 -           0.6598    | 0.0000   - 2.7269\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 62     - 100    -          0.2187 | 0.0256 | 1.5885 | 1.0108 -           0.9675    | 0.6830   - 114.4558\n",
      "\u001b[1;4mValidati 62     - 20     -          2.5974 | 0.0000 | 0.0000 | 2.5974 -           0.6632    | 0.0000   - 3.0725\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 63     - 100    -          0.1443 | 0.0221 | 1.4437 | 0.8760 -           0.9808    | 0.6852   - 127.3365\n",
      "\u001b[1;4mValidati 63     - 20     -          2.6772 | 0.0000 | 0.0000 | 2.6772 -           0.6666    | 0.0000   - 2.8416\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 64     - 100    -          0.1239 | 0.0204 | 1.3512 | 0.8255 -           0.9865    | 0.6877   - 125.2397\n",
      "\u001b[1;4mValidati 64     - 20     -          2.8173 | 0.0000 | 0.0000 | 2.8173 -           0.6726    | 0.0000   - 2.8418\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 65     - 100    -          0.1155 | 0.0212 | 1.2962 | 0.8197 -           0.9880    | 0.6881   - 120.5690\n",
      "\u001b[1;4mValidati 65     - 20     -          2.8029 | 0.0000 | 0.0000 | 2.8029 -           0.6676    | 0.0000   - 3.3403\u001b[0m\n",
      "Training 66     - 100    -          0.1044 | 0.0217 | 1.2473 | 0.8097 -           0.9875    | 0.6877   - 116.1564\n",
      "\u001b[1;4mValidati 66     - 20     -          2.9476 | 0.0000 | 0.0000 | 2.9476 -           0.6594    | 0.0000   - 2.8634\u001b[0m\n",
      "Training 67     - 100    -          0.0992 | 0.0239 | 1.2117 | 0.8237 -           0.9890    | 0.6851   - 131.0840\n",
      "\u001b[1;4mValidati 67     - 20     -          2.8246 | 0.0000 | 0.0000 | 2.8246 -           0.6621    | 0.0000   - 3.3234\u001b[0m\n",
      "Training 68     - 100    -          0.1111 | 0.0267 | 1.2020 | 0.8720 -           0.9865    | 0.6841   - 115.2094\n",
      "\u001b[1;4mValidati 68     - 20     -          2.8097 | 0.0000 | 0.0000 | 2.8097 -           0.6592    | 0.0000   - 2.7280\u001b[0m\n",
      "Training 69     - 100    -          0.1150 | 0.0294 | 1.2141 | 0.9201 -           0.9838    | 0.6864   - 114.8234\n",
      "\u001b[1;4mValidati 69     - 20     -          2.8404 | 0.0000 | 0.0000 | 2.8404 -           0.6630    | 0.0000   - 2.7275\u001b[0m\n",
      "Training 70     - 100    -          0.1381 | 0.0321 | 1.2352 | 0.9922 -           0.9818    | 0.6862   - 138.8072\n",
      "\u001b[1;4mValidati 70     - 20     -          2.7488 | 0.0000 | 0.0000 | 2.7488 -           0.6631    | 0.0000   - 3.3574\u001b[0m\n",
      "Training 71     - 100    -          0.1283 | 0.0323 | 1.2083 | 0.9858 -           0.9820    | 0.6833   - 94.2569\n",
      "\u001b[1;4mValidati 71     - 20     -          2.9900 | 0.0000 | 0.0000 | 2.9900 -           0.6558    | 0.0000   - 3.3532\u001b[0m\n",
      "Training 72     - 100    -          0.1565 | 0.0361 | 1.2404 | 1.0773 -           0.9740    | 0.6817   - 119.0510\n",
      "\u001b[1;4mValidati 72     - 20     -          2.7752 | 0.0000 | 0.0000 | 2.7752 -           0.6584    | 0.0000   - 2.8554\u001b[0m\n",
      "Training 73     - 100    -          0.1716 | 0.0370 | 1.2619 | 1.1239 -           0.9745    | 0.6833   - 127.8548\n",
      "\u001b[1;4mValidati 73     - 20     -          2.7880 | 0.0000 | 0.0000 | 2.7880 -           0.6610    | 0.0000   - 2.8555\u001b[0m\n",
      "Training 74     - 100    -          0.1697 | 0.0374 | 1.2490 | 1.1304 -           0.9743    | 0.6825   - 130.8899\n",
      "\u001b[1;4mValidati 74     - 20     -          2.6121 | 0.0000 | 0.0000 | 2.6121 -           0.6615    | 0.0000   - 2.8498\u001b[0m\n",
      "Training 75     - 100    -          0.1745 | 0.0381 | 1.2448 | 1.1504 -           0.9745    | 0.6843   - 122.6420\n",
      "\u001b[1;4mValidati 75     - 20     -          2.6896 | 0.0000 | 0.0000 | 2.6896 -           0.6637    | 0.0000   - 2.8571\u001b[0m\n",
      "Training 76     - 100    -          0.1607 | 0.0377 | 1.2129 | 1.1250 -           0.9740    | 0.6823   - 136.0662\n",
      "\u001b[1;4mValidati 76     - 20     -          2.9003 | 0.0000 | 0.0000 | 2.9003 -           0.6517    | 0.0000   - 2.8640\u001b[0m\n",
      "Training 77     - 100    -          0.1961 | 0.0404 | 1.2681 | 1.2215 -           0.9675    | 0.6830   - 119.7332\n",
      "\u001b[1;4mValidati 77     - 20     -          2.8191 | 0.0000 | 0.0000 | 2.8191 -           0.6393    | 0.0000   - 3.3593\u001b[0m\n",
      "Training 78     - 100    -          0.1737 | 0.0382 | 1.2167 | 1.1575 -           0.9793    | 0.6843   - 118.8341\n",
      "\u001b[1;4mValidati 78     - 20     -          2.7681 | 0.0000 | 0.0000 | 2.7681 -           0.6560    | 0.0000   - 3.2790\u001b[0m\n",
      "Training 79     - 100    -          0.1670 | 0.0379 | 1.1919 | 1.1386 -           0.9758    | 0.6854   - 131.3560\n",
      "\u001b[1;4mValidati 79     - 20     -          2.6855 | 0.0000 | 0.0000 | 2.6855 -           0.6735    | 0.0000   - 3.3423\u001b[0m\n",
      " better performance: saving ...\n",
      "\n",
      "Training 80     - 100    -          0.1754 | 0.0393 | 1.2034 | 1.1693 -           0.9725    | 0.6832   - 106.1729\n",
      "\u001b[1;4mValidati 80     - 20     -          2.5596 | 0.0000 | 0.0000 | 2.5596 -           0.6636    | 0.0000   - 2.7586\u001b[0m\n",
      "Training 81     - 100    -          0.1549 | 0.0385 | 1.1715 | 1.1255 -           0.9750    | 0.6825   - 118.6676\n",
      "\u001b[1;4mValidati 81     - 20     -          2.8010 | 0.0000 | 0.0000 | 2.8010 -           0.6392    | 0.0000   - 2.8442\u001b[0m\n",
      "Training 82     - 100    -          0.1705 | 0.0397 | 1.1966 | 1.1661 -           0.9740    | 0.6830   - 108.4632\n",
      "\u001b[1;4mValidati 82     - 20     -          2.8245 | 0.0000 | 0.0000 | 2.8245 -           0.6560    | 0.0000   - 2.7536\u001b[0m\n",
      "Training 83     - 100    -          0.1766 | 0.0397 | 1.1787 | 1.1630 -           0.9675    | 0.6834   - 121.6185\n",
      "\u001b[1;4mValidati 83     - 20     -          2.7787 | 0.0000 | 0.0000 | 2.7787 -           0.6537    | 0.0000   - 2.8539\u001b[0m\n",
      "Training 84     - 100    -          0.1672 | 0.0391 | 1.1568 | 1.1366 -           0.9758    | 0.6836   - 131.3756\n",
      "\u001b[1;4mValidati 84     - 20     -          2.8546 | 0.0000 | 0.0000 | 2.8546 -           0.6556    | 0.0000   - 3.3341\u001b[0m\n",
      "Training 85     - 100    -          0.1586 | 0.0393 | 1.1388 | 1.1208 -           0.9728    | 0.6847   - 120.9311\n",
      "\u001b[1;4mValidati 85     - 20     -          2.8674 | 0.0000 | 0.0000 | 2.8674 -           0.6643    | 0.0000   - 3.3486\u001b[0m\n",
      "Training 86     - 100    -          0.1761 | 0.0411 | 1.1668 | 1.1702 -           0.9675    | 0.6825   - 127.4996\n",
      "\u001b[1;4mValidati 86     - 20     -          2.7641 | 0.0000 | 0.0000 | 2.7641 -           0.6560    | 0.0000   - 2.8711\u001b[0m\n",
      "Training 87     - 100    -          0.1541 | 0.0386 | 1.1241 | 1.1022 -           0.9775    | 0.6857   - 132.4191\n",
      "\u001b[1;4mValidati 87     - 20     -          2.8588 | 0.0000 | 0.0000 | 2.8588 -           0.6690    | 0.0000   - 3.3426\u001b[0m\n",
      "Training 88     - 100    -          0.1569 | 0.0394 | 1.1180 | 1.1094 -           0.9765    | 0.6847   - 118.6182\n",
      "\u001b[1;4mValidati 88     - 20     -          2.7586 | 0.0000 | 0.0000 | 2.7586 -           0.6530    | 0.0000   - 2.8537\u001b[0m\n",
      "Training 89     - 100    -          0.1644 | 0.0399 | 1.1364 | 1.1314 -           0.9778    | 0.6850   - 129.5418\n",
      "\u001b[1;4mValidati 89     - 20     -          2.8919 | 0.0000 | 0.0000 | 2.8919 -           0.6598    | 0.0000   - 2.8505\u001b[0m\n",
      "Training 90     - 100    -          0.1732 | 0.0412 | 1.1513 | 1.1609 -           0.9718    | 0.6845   - 124.7853\n",
      "\u001b[1;4mValidati 90     - 20     -          2.7885 | 0.0000 | 0.0000 | 2.7885 -           0.6561    | 0.0000   - 3.3418\u001b[0m\n",
      "Training 91     - 100    -          0.1518 | 0.0382 | 1.1008 | 1.0843 -           0.9765    | 0.6863   - 120.8920\n",
      "\u001b[1;4mValidati 91     - 20     -          2.7939 | 0.0000 | 0.0000 | 2.7939 -           0.6706    | 0.0000   - 2.8480\u001b[0m\n",
      "Training 92     - 100    -          0.1786 | 0.0421 | 1.1523 | 1.1756 -           0.9700    | 0.6836   - 127.8414\n",
      "\u001b[1;4mValidati 92     - 20     -          2.7765 | 0.0000 | 0.0000 | 2.7765 -           0.6684    | 0.0000   - 3.3345\u001b[0m\n",
      "Training 93     - 67     -          0.1223 | 0.0365 | 1.0538 | 1.0142 -           0.9811    | 0.6873   - 86.8533\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "\n",
    "for epoch in range(0, args.nb_epoch):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "    tensorboard.flush()\n",
    "    \n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dct",
   "language": "python",
   "name": "dct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
