{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#Utils\" data-toc-modified-id=\"Utils-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Utils</a></span></li><li><span><a href=\"#Load-and-prepare-the-data\" data-toc-modified-id=\"Load-and-prepare-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load and prepare the data</a></span></li><li><span><a href=\"#Create-model\" data-toc-modified-id=\"Create-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create model</a></span></li><li><span><a href=\"#Loss-function-&amp;-warmup\" data-toc-modified-id=\"Loss-function-&amp;-warmup-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Loss function &amp; warmup</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normal-train\" data-toc-modified-id=\"Normal-train-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Normal train</a></span></li><li><span><a href=\"#custum-acc-train\" data-toc-modified-id=\"custum-acc-train-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>custum acc train</a></span></li><li><span><a href=\"#add-ratio-p(m1)-<>-p(g(m2))\" data-toc-modified-id=\"add-ratio-p(m1)-<>-p(g(m2))-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>add ratio p(m1) &lt;&gt; p(g(m2))</a></span></li><li><span><a href=\"#add-ratio\" data-toc-modified-id=\"add-ratio-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>add ratio</a></span></li></ul></li><li><span><a href=\"#End\" data-toc-modified-id=\"End-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>End</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "from advertorch.attacks import GradientSignAttack"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:11:24.157152Z",
     "start_time": "2019-10-31T10:11:24.137398Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.value = 0\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def reset(self):\n",
    "        self.accumulate_value = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.count += 1\n",
    "\n",
    "        \n",
    "class BinaryAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = (y_pred>0.5).float()\n",
    "            correct = (y_pred == y_true).float().sum()\n",
    "            self.value = correct / (y_true.shape[0] * y_true.shape[1])\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "        \n",
    "class CategoricalAccuracy(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        super().__call__()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.value = torch.mean((y_true == y_pred).float())\n",
    "            \n",
    "            self.accumulate_value += self.value\n",
    "\n",
    "            return self.accumulate_value / self.count\n",
    "        \n",
    "class Ratio(Metrics):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        Metrics.__init__(self, epsilon)\n",
    "        \n",
    "    def __call__(self, y_pred, y_adv_pred):\n",
    "        super().__call__()\n",
    "        \n",
    "        results = zip(y_pred, y_adv_pred)\n",
    "        results_bool = [int(r[0] != r[1]) for r in results]\n",
    "        self.value = sum(results_bool) / len(results_bool) * 100\n",
    "        self.accumulate_value += self.value\n",
    "        \n",
    "        return self.accumulate_value / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:08.726871Z",
     "start_time": "2019-10-31T10:06:08.719339Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:09.238568Z",
     "start_time": "2019-10-31T10:06:09.232552Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_datetime():\n",
    "    now = datetime.datetime.now()\n",
    "    return str(now)[:10] + \"_\" + str(now)[11:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:10.661050Z",
     "start_time": "2019-10-31T10:06:10.654933Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:15.210961Z",
     "start_time": "2019-10-31T10:06:13.425444Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c45f938db469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdataset_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files already downloaded and verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmakedir_exist_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# downloads file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mmakedir_exist_ok\u001b[0;34m(dirpath)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEXIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/dl/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '../dataset'"
     ],
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../dataset'",
     "output_type": "error"
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomAffine(0, translate=(1/16, 1/16)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_root = os.path.join(\"..\", \"dataset\")\n",
    "train_set = torchvision.datasets.CIFAR10(dataset_root, train=True, download=True, transform=transform_train)\n",
    "val_set = torchvision.datasets.CIFAR10(dataset_root, train=False, download=True, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:23.962261Z",
     "start_time": "2019-10-31T10:06:15.213544Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "S_idx, U_idx = [], []\n",
    "classes = [[] for _ in range(10)]\n",
    "\n",
    "for i in tqdm.tqdm(range(len(train_set))):\n",
    "    data, label = train_set[i]\n",
    "    classes[label].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:23.970549Z",
     "start_time": "2019-10-31T10:06:23.964360Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for indexes in classes:\n",
    "    np.random.shuffle(indexes)\n",
    "    S_idx += indexes[:400]\n",
    "    U_idx += indexes[400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:24.006315Z",
     "start_time": "2019-10-31T10:06:23.972348Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class repro(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        \n",
    "#         self.gaussian = GaussianNoise(sigma=0.15)\n",
    "        self.features = torch.nn.Sequential(\n",
    "            weight_norm(nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(128, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            weight_norm(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            weight_norm(nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=0)),\n",
    "            nn.BatchNorm2d(512, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)),\n",
    "            nn.BatchNorm2d(256, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            weight_norm(nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0)),\n",
    "            nn.BatchNorm2d(128, momentum=0.999),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.AvgPool2d(6, stride=2, padding=0),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            weight_norm(nn.Linear(128, 10))\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "#         x = self.gaussian(x)\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 128)\n",
    "#         x = nn.functional.avg_pool2d(x, kernel_size=(6,6))\n",
    "#         x = x.view(-1, 128)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function & warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:24.018576Z",
     "start_time": "2019-10-31T10:06:24.008145Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def Lsup(logit_S1, logit_S2, labels_S1, labels_S2):\n",
    "    ce = nn.CrossEntropyLoss() \n",
    "    loss1 = ce(logit_S1, labels_S1)\n",
    "    loss2 = ce(logit_S2, labels_S2) \n",
    "    return (loss1+loss2)\n",
    "\n",
    "def Lcot(U_p1, U_p2):\n",
    "# the Jensen-Shannon divergence between p1(x) and p2(x)\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    a1 = 0.5 * (S(U_p1) + S(U_p2))\n",
    "    loss1 = a1 * torch.log(a1)\n",
    "    loss1 = -torch.sum(loss1)\n",
    "    loss2 = S(U_p1) * LS(U_p1)\n",
    "    loss2 = -torch.sum(loss2)\n",
    "    loss3 = S(U_p2) * LS(U_p2)\n",
    "    loss3 = -torch.sum(loss3)\n",
    "\n",
    "    return (loss1 - 0.5 * (loss2 + loss3))/U_batch_size\n",
    "\n",
    "\n",
    "def Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2):\n",
    "    S = nn.Softmax(dim = 1)\n",
    "    LS = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    a = S(logit_S2) * LS(perturbed_logit_S1)\n",
    "    a = torch.sum(a)\n",
    "\n",
    "    b = S(logit_S1) * LS(perturbed_logit_S2)\n",
    "    b = torch.sum(b)\n",
    "\n",
    "    c = S(logit_U2) * LS(perturbed_logit_U1)\n",
    "    c = torch.sum(c)\n",
    "\n",
    "    d = S(logit_U1) * LS(perturbed_logit_U2)\n",
    "    d = torch.sum(d)\n",
    "\n",
    "    return -(a+b+c+d)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T10:06:24.031305Z",
     "start_time": "2019-10-31T10:06:24.019836Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"cosine scheduling\"\"\"\n",
    "    epoch = epoch + 1\n",
    "    lr = 0.05*(1.0 + np.cos((epoch-1)*np.pi/600))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def adjust_lamda(epoch):\n",
    "    epoch = epoch + 1\n",
    "    global lambda_cot\n",
    "    global lambda_diff\n",
    "    if epoch <= 80:\n",
    "        lambda_cot = lambda_cot_max*np.exp(-5*(1-epoch/80)**2)\n",
    "        lambda_diff = lambda_diff_max*np.exp(-5*(1-epoch/80)**2)\n",
    "    else: \n",
    "        lambda_cot = lambda_cot_max\n",
    "        lambda_diff = lambda_diff_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T12:58:17.980782Z",
     "start_time": "2019-10-31T12:58:17.845195Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_func = repro\n",
    "\n",
    "net1 = model_func()\n",
    "net2 = model_func()\n",
    "\n",
    "net1.cuda()\n",
    "net2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T12:58:18.055697Z",
     "start_time": "2019-10-31T12:58:18.043947Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "U_batch_size = int(batch_size * 46./50.)\n",
    "S_batch_size = batch_size - U_batch_size\n",
    "nb_batch = len(train_set) // batch_size\n",
    "    \n",
    "S_sampler = torch.utils.data.SubsetRandomSampler(S_idx)\n",
    "U_sampler = torch.utils.data.SubsetRandomSampler(U_idx)\n",
    "\n",
    "S1_loader = torch.utils.data.DataLoader(train_set, batch_size=S_batch_size, sampler=S_sampler)\n",
    "S2_loader = torch.utils.data.DataLoader(train_set, batch_size=S_batch_size, sampler=S_sampler)\n",
    "U_loader = torch.utils.data.DataLoader(train_set, batch_size=U_batch_size, sampler=U_sampler)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "\n",
    "adv_generator_1 = GradientSignAttack( \n",
    "    net1, \n",
    "    loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=0.02, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack( \n",
    "    net2, \n",
    "    loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "    eps=0.02, clip_min=-np.inf, clip_max=np.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T12:58:18.210918Z",
     "start_time": "2019-10-31T12:58:18.206622Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lambda_cot_max = 10\n",
    "lambda_diff_max = 0.5\n",
    "lambda_cot = 0.0\n",
    "lambda_diff = 0.0\n",
    "best_acc = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T12:58:18.376669Z",
     "start_time": "2019-10-31T12:58:18.370535Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    list(net1.parameters()) + list(net2.parameters()),\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    "    lr=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Normal train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:57:37.518550Z",
     "start_time": "2019-10-16T10:57:35.251962Z"
    },
    "hidden": true,
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net1.train()\n",
    "    net2.train()\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    adjust_lamda(epoch)\n",
    "    \n",
    "    total_S1 = 0\n",
    "    total_S2 = 0\n",
    "    total_U1 = 0\n",
    "    total_U2 = 0\n",
    "    train_correct_S1 = 0\n",
    "    train_correct_S2 = 0\n",
    "    train_correct_U1 = 0\n",
    "    train_correct_U2 = 0\n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    # create iterator for b1, b2, bu\n",
    "    S_iter1 = iter(S1_loader)\n",
    "    S_iter2 = iter(S2_loader)\n",
    "    U_iter = iter(U_loader)\n",
    "    print('')\n",
    "    start_time = time.time()\n",
    "    for i in range(nb_batch):\n",
    "        inputs_S1, labels_S1 = S_iter1.next()\n",
    "        inputs_S2, labels_S2 = S_iter2.next()\n",
    "        inputs_U, labels_U = U_iter.next() # note that labels_U will not be used for training. \n",
    "\n",
    "        inputs_S1, labels_S1 = inputs_S1.cuda(), labels_S1.cuda()\n",
    "        inputs_S2, labels_S2 = inputs_S2.cuda(), labels_S2.cuda()\n",
    "        inputs_U = inputs_U.cuda()    \n",
    "\n",
    "\n",
    "        logit_S1 = net1(inputs_S1)\n",
    "        logit_S2 = net2(inputs_S2)\n",
    "        logit_U1 = net1(inputs_U)\n",
    "        logit_U2 = net2(inputs_U)\n",
    "\n",
    "        _, predictions_S1 = torch.max(logit_S1, 1)\n",
    "        _, predictions_S2 = torch.max(logit_S2, 1)\n",
    "\n",
    "        # pseudo labels of U \n",
    "        _, predictions_U1 = torch.max(logit_U1, 1)\n",
    "        _, predictions_U2 = torch.max(logit_U2, 1)\n",
    "\n",
    "        # fix batchnorm\n",
    "        net1.eval()\n",
    "        net2.eval()\n",
    "        #generate adversarial examples\n",
    "        perturbed_data_S1 = adv_generator_1.perturb(inputs_S1, labels_S1)\n",
    "        perturbed_data_U1 = adv_generator_1.perturb(inputs_U, predictions_U1)\n",
    "\n",
    "        perturbed_data_S2 = adv_generator_2.perturb(inputs_S2, labels_S2)\n",
    "        perturbed_data_U2 = adv_generator_2.perturb(inputs_U, predictions_U2)\n",
    "        net1.train()\n",
    "        net2.train()\n",
    "\n",
    "        perturbed_logit_S1 = net1(perturbed_data_S2)\n",
    "        perturbed_logit_S2 = net2(perturbed_data_S1)\n",
    "\n",
    "        perturbed_logit_U1 = net1(perturbed_data_U2)\n",
    "        perturbed_logit_U2 = net2(perturbed_data_U1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        net1.zero_grad()\n",
    "        net2.zero_grad()\n",
    "\n",
    "        \n",
    "        Loss_sup = Lsup(logit_S1, logit_S2, labels_S1, labels_S2)\n",
    "        Loss_cot = Lcot(logit_U1, logit_U2)\n",
    "        Loss_diff = Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2)\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_correct_S1 += np.sum(predictions_S1.cpu().numpy() == labels_S1.cpu().numpy())\n",
    "        total_S1 += labels_S1.size(0)\n",
    "\n",
    "        train_correct_U1 += np.sum(predictions_U1.cpu().numpy() == labels_U.cpu().numpy())\n",
    "        total_U1 += labels_U.size(0)\n",
    "\n",
    "        train_correct_S2 += np.sum(predictions_S2.cpu().numpy() == labels_S2.cpu().numpy())\n",
    "        total_S2 += labels_S2.size(0)\n",
    "\n",
    "        train_correct_U2 += np.sum(predictions_U2.cpu().numpy() == labels_U.cpu().numpy())\n",
    "        total_U2 += labels_U.size(0)\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        acc1 = (train_correct_S1+train_correct_U1) / (total_S1+total_U1)\n",
    "        acc2 = (train_correct_S2+train_correct_U2) / (total_S2+total_U2)\n",
    "        print(\"Epoch {:4}, {:3d}% \\t acc: {:3.4e} {:3.4e} - loss {:3.4e} {:3.4e} {:3.4e} {:3.4e} took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            \n",
    "            acc1, acc2,\n",
    "            running_loss/(i+1), ls/(i+1), lc/(i+1), ld/(i+1), \n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "            \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "        tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_1\", 100. * acc1, epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_2\",  100. * acc2, epoch )\n",
    "\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net1.eval()\n",
    "    net2.eval()\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            outputs1 = net1(inputs)\n",
    "            predicted1 = outputs1.max(1)\n",
    "            total1 += targets.size(0)\n",
    "            correct1 += predicted1[1].eq(targets).sum().item()\n",
    "\n",
    "            outputs2 = net2(inputs)\n",
    "            predicted2 = outputs2.max(1)\n",
    "            total2 += targets.size(0)\n",
    "            correct2 += predicted2[1].eq(targets).sum().item()\n",
    "            \n",
    "    tensorboard.add_scalar('val/acc_1', 100.*correct1/total1, epoch)\n",
    "    tensorboard.add_scalar('val/acc_2', 100.*correct2/total2, epoch)\n",
    "\n",
    "    print('\\nnet1 test acc: %.3f%% (%d/%d) | net2 test acc: %.3f%% (%d/%d)'\n",
    "        % (100.*correct1/total1, correct1, total1, 100.*correct2/total2, correct2, total2))\n",
    "\n",
    "    acc = ((100.*correct1/total1)+(100.*correct2/total2))/2\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "#         checkpoint(epoch, 'best')\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"repro_cotraining/simple_ratio_%s_%s\" % (get_datetime(), model_func.__name__), comment=model_func.__name__)\n",
    "for epoch in range(0, 600):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custum acc train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-31T13:06:57.407Z"
    },
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "acc1_func = CategoricalAccuracy()\n",
    "acc2_func = CategoricalAccuracy()\n",
    "\n",
    "def train(epoch):\n",
    "    net1.train()\n",
    "    net2.train()\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    adjust_lamda(epoch)\n",
    "    \n",
    "    acc1_func.reset()\n",
    "    acc2_func.reset()\n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    # create iterator for b1, b2, bu\n",
    "    S_iter1 = iter(S1_loader)\n",
    "    S_iter2 = iter(S2_loader)\n",
    "    U_iter = iter(U_loader)\n",
    "    \n",
    "    print('')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(nb_batch):\n",
    "        inputs_S1, labels_S1 = S_iter1.next()\n",
    "        inputs_S2, labels_S2 = S_iter2.next()\n",
    "        inputs_U, labels_U = U_iter.next() # note that labels_U will not be used for training. \n",
    "\n",
    "        inputs_S1, labels_S1 = inputs_S1.cuda(), labels_S1.cuda()\n",
    "        inputs_S2, labels_S2 = inputs_S2.cuda(), labels_S2.cuda()\n",
    "        inputs_U, labels_U = inputs_U.cuda(), labels_U.cuda()\n",
    "\n",
    "        logit_S1 = net1(inputs_S1)\n",
    "        logit_S2 = net2(inputs_S2)\n",
    "        logit_U1 = net1(inputs_U)\n",
    "        logit_U2 = net2(inputs_U)\n",
    "\n",
    "        _, predictions_S1 = torch.max(logit_S1, 1)\n",
    "        _, predictions_S2 = torch.max(logit_S2, 1)\n",
    "\n",
    "        # pseudo labels of U \n",
    "        _, predictions_U1 = torch.max(logit_U1, 1)\n",
    "        _, predictions_U2 = torch.max(logit_U2, 1)\n",
    "\n",
    "        # fix batchnorm\n",
    "        net1.eval()\n",
    "        net2.eval()\n",
    "        \n",
    "        #generate adversarial examples\n",
    "        perturbed_data_S1 = adv_generator_1.perturb(inputs_S1, labels_S1)\n",
    "        perturbed_data_S2 = adv_generator_2.perturb(inputs_S2, labels_S2)\n",
    "        \n",
    "        perturbed_data_U1 = adv_generator_1.perturb(inputs_U, predictions_U1)\n",
    "        perturbed_data_U2 = adv_generator_2.perturb(inputs_U, predictions_U2)\n",
    "        \n",
    "        net1.train()\n",
    "        net2.train()\n",
    "\n",
    "        perturbed_logit_S1 = net1(perturbed_data_S2)\n",
    "        perturbed_logit_S2 = net2(perturbed_data_S1)\n",
    "\n",
    "        perturbed_logit_U1 = net1(perturbed_data_U2)\n",
    "        perturbed_logit_U2 = net2(perturbed_data_U1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        net1.zero_grad()\n",
    "        net2.zero_grad()\n",
    "        \n",
    "        Loss_sup = Lsup(logit_S1, logit_S2, labels_S1, labels_S2)\n",
    "        Loss_cot = Lcot(logit_U1, logit_U2)\n",
    "        Loss_diff = Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2)\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc the metrics\n",
    "        \n",
    "        pred_S1U = torch.cat((predictions_S1, predictions_U1), 0)\n",
    "        pred_S2U = torch.cat((predictions_S2, predictions_U2), 0)\n",
    "        y_S1U = torch.cat((labels_S1, labels_U), 0)\n",
    "        y_S2U = torch.cat((labels_S2, labels_U), 0)\n",
    "        \n",
    "        acc1 = acc1_func(pred_S1U, y_S1U)\n",
    "        acc2 = acc2_func(pred_S2U, y_S2U)\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        print(\"Epoch {:4}, {:3d}% \\t acc: {:3.4e} {:3.4e} - loss {:3.4e} {:3.4e} {:3.4e} {:3.4e} took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            \n",
    "            acc1, acc2,\n",
    "            running_loss/(i+1), ls/(i+1), lc/(i+1), ld/(i+1), \n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "            \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "        tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_1\", 100. * acc1, epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_2\",  100. * acc2, epoch )\n",
    "\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net1.eval()\n",
    "    net2.eval()\n",
    "    \n",
    "    acc1_func.reset()\n",
    "    acc2_func.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            outputs1 = net1(inputs)\n",
    "            _, predicted1 = torch.max(outputs1, 1)\n",
    "            acc1 = acc1_func(predicted1, targets)\n",
    "\n",
    "            outputs2 = net2(inputs)\n",
    "            _, predicted2 = torch.max(outputs2, 1)\n",
    "            acc2 = acc2_func(predicted2, targets)\n",
    "            \n",
    "    tensorboard.add_scalar('val/acc_1', acc1, epoch)\n",
    "    tensorboard.add_scalar('val/acc_2', acc2, epoch)\n",
    "\n",
    "    print('\\nnet1 test acc: %.3f%% | net2 test acc: %.3f%%'\n",
    "        % (acc1, acc2))\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"repro_cotraining/custum_acc_%s_%s\" % (get_datetime(), model_func.__name__), comment=model_func.__name__)\n",
    "for epoch in range(0, 600):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add ratio p(m1) <> p(g(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-18T08:17:07.527Z"
    },
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "\n",
    "ratio1_p1pg1 = Ratio()\n",
    "ratio2_p2pg2 = Ratio()\n",
    "ratio_p1pg2 = Ratio()\n",
    "ratio_p2pg1 = Ratio()\n",
    "\n",
    "def train(epoch):\n",
    "    net1.train()\n",
    "    net2.train()\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    adjust_lamda(epoch)\n",
    "    \n",
    "    ratio1_p1pg1.reset()\n",
    "    ratio2_p2pg2.reset()\n",
    "    ratio_p1pg2.reset()\n",
    "    ratio_p2pg1.reset()\n",
    "    \n",
    "    total_S1 = 0\n",
    "    total_S2 = 0\n",
    "    total_U1 = 0\n",
    "    total_U2 = 0\n",
    "    train_correct_S1 = 0\n",
    "    train_correct_S2 = 0\n",
    "    train_correct_U1 = 0\n",
    "    train_correct_U2 = 0\n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    # create iterator for b1, b2, bu\n",
    "    S_iter1 = iter(S1_loader)\n",
    "    S_iter2 = iter(S2_loader)\n",
    "    U_iter = iter(U_loader)\n",
    "    print('')\n",
    "    start_time = time.time()\n",
    "    for i in range(nb_batch):\n",
    "        inputs_S1, labels_S1 = S_iter1.next()\n",
    "        inputs_S2, labels_S2 = S_iter2.next()\n",
    "        inputs_U, labels_U = U_iter.next() # note that labels_U will not be used for training. \n",
    "\n",
    "        inputs_S1, labels_S1 = inputs_S1.cuda(), labels_S1.cuda()\n",
    "        inputs_S2, labels_S2 = inputs_S2.cuda(), labels_S2.cuda()\n",
    "        inputs_U = inputs_U.cuda()    \n",
    "\n",
    "\n",
    "        logit_S1 = net1(inputs_S1)\n",
    "        logit_S2 = net2(inputs_S2)\n",
    "        logit_U1 = net1(inputs_U)\n",
    "        logit_U2 = net2(inputs_U)\n",
    "\n",
    "        _, predictions_S1 = torch.max(logit_S1, 1)\n",
    "        _, predictions_S2 = torch.max(logit_S2, 1)\n",
    "\n",
    "        # pseudo labels of U \n",
    "        _, predictions_U1 = torch.max(logit_U1, 1)\n",
    "        _, predictions_U2 = torch.max(logit_U2, 1)\n",
    "\n",
    "        # fix batchnorm\n",
    "        net1.eval()\n",
    "        net2.eval()\n",
    "        #generate adversarial examples\n",
    "        perturbed_data_S1 = adv_generator_1.perturb(inputs_S1, labels_S1)\n",
    "        perturbed_data_U1 = adv_generator_1.perturb(inputs_U, predictions_U1)\n",
    "\n",
    "        perturbed_data_S2 = adv_generator_2.perturb(inputs_S2, labels_S2)\n",
    "        perturbed_data_U2 = adv_generator_2.perturb(inputs_U, predictions_U2)\n",
    "        net1.train()\n",
    "        net2.train()\n",
    "\n",
    "        perturbed_logit_S1 = net1(perturbed_data_S2)\n",
    "        perturbed_logit_S2 = net2(perturbed_data_S1)\n",
    "\n",
    "        perturbed_logit_U1 = net1(perturbed_data_U2)\n",
    "        perturbed_logit_U2 = net2(perturbed_data_U1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        net1.zero_grad()\n",
    "        net2.zero_grad()\n",
    "\n",
    "        \n",
    "        Loss_sup = Lsup(logit_S1, logit_S2, labels_S1, labels_S2)\n",
    "        Loss_cot = Lcot(logit_U1, logit_U2)\n",
    "        Loss_diff = Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2)\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, prediction_perturbed_U1 = torch.max(perturbed_logit_U1, 1)\n",
    "        _, prediction_perturbed_U2 = torch.max(perturbed_logit_U2, 1)\n",
    "        ratio1 = ratio1_p1pg1(predictions_U1, prediction_perturbed_U1)\n",
    "        ratio2 = ratio2_p2pg2(predictions_U2, prediction_perturbed_U2)\n",
    "        ratio3 = ratio_p1pg2(predictions_U1, prediction_perturbed_U2)\n",
    "        ratio4 = ratio_p2pg1(predictions_U2, prediction_perturbed_U1)\n",
    "        \n",
    "        train_correct_S1 += np.sum(predictions_S1.cpu().numpy() == labels_S1.cpu().numpy())\n",
    "        total_S1 += labels_S1.size(0)\n",
    "\n",
    "        train_correct_U1 += np.sum(predictions_U1.cpu().numpy() == labels_U.cpu().numpy())\n",
    "        total_U1 += labels_U.size(0)\n",
    "\n",
    "        train_correct_S2 += np.sum(predictions_S2.cpu().numpy() == labels_S2.cpu().numpy())\n",
    "        total_S2 += labels_S2.size(0)\n",
    "\n",
    "        train_correct_U2 += np.sum(predictions_U2.cpu().numpy() == labels_U.cpu().numpy())\n",
    "        total_U2 += labels_U.size(0)\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        acc1 = (train_correct_S1+train_correct_U1) / (total_S1+total_U1)\n",
    "        acc2 = (train_correct_S2+train_correct_U2) / (total_S2+total_U2)\n",
    "        print(\"Epoch {:4}, {:3d}% \\t acc: {:3.4e} {:3.4e} - loss {:3.4e} {:3.4e} {:3.4e} {:3.4e} took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            \n",
    "            acc1, acc2,\n",
    "            running_loss/(i+1), ls/(i+1), lc/(i+1), ld/(i+1), \n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "            \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "        tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_1\", 100. * acc1, epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_2\",  100. * acc2, epoch )\n",
    "        tensorboard.add_scalar(\"train/ratio1\", ratio1, epoch )\n",
    "        tensorboard.add_scalar(\"train/ratio2\", ratio2, epoch )\n",
    "        tensorboard.add_scalar(\"train/ratio_p1pg2\", ratio3, epoch )\n",
    "        tensorboard.add_scalar(\"train/ratio_p2pg1\", ratio4, epoch )\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net1.eval()\n",
    "    net2.eval()\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            outputs1 = net1(inputs)\n",
    "            predicted1 = outputs1.max(1)\n",
    "            total1 += targets.size(0)\n",
    "            correct1 += predicted1[1].eq(targets).sum().item()\n",
    "\n",
    "            outputs2 = net2(inputs)\n",
    "            predicted2 = outputs2.max(1)\n",
    "            total2 += targets.size(0)\n",
    "            correct2 += predicted2[1].eq(targets).sum().item()\n",
    "            \n",
    "    tensorboard.add_scalar('val/acc_1', 100.*correct1/total1, epoch)\n",
    "    tensorboard.add_scalar('val/acc_2', 100.*correct2/total2, epoch)\n",
    "\n",
    "    print('\\nnet1 test acc: %.3f%% (%d/%d) | net2 test acc: %.3f%% (%d/%d)'\n",
    "        % (100.*correct1/total1, correct1, total1, 100.*correct2/total2, correct2, total2))\n",
    "\n",
    "    acc = ((100.*correct1/total1)+(100.*correct2/total2))/2\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"repro_cotraining/ratio_p1pg2_%s_%s\" % (get_datetime(), model_func.__name__), comment=model_func.__name__)\n",
    "for epoch in range(0, nb_epoch):\n",
    "    print(\"Epoch     ,    % \\t acc: m1_acc    m2_acc    - loss sup       diff      cot      \")\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-16T11:26:41.565Z"
    },
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ratio1_func = Ratio()\n",
    "ratio2_func = Ratio()\n",
    "\n",
    "def train(epoch):\n",
    "    net1.train()\n",
    "    net2.train()\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    adjust_lamda(epoch)\n",
    "    \n",
    "    ratio1_func.reset()\n",
    "    ratio2_func.reset()\n",
    "    \n",
    "    total_S1 = 0\n",
    "    total_S2 = 0\n",
    "    total_U1 = 0\n",
    "    total_U2 = 0\n",
    "    train_correct_S1 = 0\n",
    "    train_correct_S2 = 0\n",
    "    train_correct_U1 = 0\n",
    "    train_correct_U2 = 0\n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    # create iterator for b1, b2, bu\n",
    "    S_iter1 = iter(S1_loader)\n",
    "    S_iter2 = iter(S2_loader)\n",
    "    U_iter = iter(U_loader)\n",
    "    print('')\n",
    "    start_time = time.time()\n",
    "    for i in range(nb_batch):\n",
    "        inputs_S1, labels_S1 = S_iter1.next()\n",
    "        inputs_S2, labels_S2 = S_iter2.next()\n",
    "        inputs_U, labels_U = U_iter.next() # note that labels_U will not be used for training. \n",
    "\n",
    "        inputs_S1, labels_S1 = inputs_S1.cuda(), labels_S1.cuda()\n",
    "        inputs_S2, labels_S2 = inputs_S2.cuda(), labels_S2.cuda()\n",
    "        inputs_U = inputs_U.cuda()    \n",
    "\n",
    "\n",
    "        logit_S1 = net1(inputs_S1)\n",
    "        logit_S2 = net2(inputs_S2)\n",
    "        logit_U1 = net1(inputs_U)\n",
    "        logit_U2 = net2(inputs_U)\n",
    "\n",
    "        _, predictions_S1 = torch.max(logit_S1, 1)\n",
    "        _, predictions_S2 = torch.max(logit_S2, 1)\n",
    "\n",
    "        # pseudo labels of U \n",
    "        _, predictions_U1 = torch.max(logit_U1, 1)\n",
    "        _, predictions_U2 = torch.max(logit_U2, 1)\n",
    "\n",
    "        # fix batchnorm\n",
    "        net1.eval()\n",
    "        net2.eval()\n",
    "        #generate adversarial examples\n",
    "        perturbed_data_S1 = adv_generator_1.perturb(inputs_S1, labels_S1)\n",
    "        perturbed_data_U1 = adv_generator_1.perturb(inputs_U, predictions_U1)\n",
    "\n",
    "        perturbed_data_S2 = adv_generator_2.perturb(inputs_S2, labels_S2)\n",
    "        perturbed_data_U2 = adv_generator_2.perturb(inputs_U, predictions_U2)\n",
    "        net1.train()\n",
    "        net2.train()\n",
    "\n",
    "        perturbed_logit_S1 = net1(perturbed_data_S2)\n",
    "        perturbed_logit_S2 = net2(perturbed_data_S1)\n",
    "\n",
    "        perturbed_logit_U1 = net1(perturbed_data_U2)\n",
    "        perturbed_logit_U2 = net2(perturbed_data_U1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        net1.zero_grad()\n",
    "        net2.zero_grad()\n",
    "\n",
    "        \n",
    "        Loss_sup = Lsup(logit_S1, logit_S2, labels_S1, labels_S2)\n",
    "        Loss_cot = Lcot(logit_U1, logit_U2)\n",
    "        Loss_diff = Ldiff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2)\n",
    "        \n",
    "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, prediction_perturbed_U1 = torch.max(perturbed_logit_U1, 1)\n",
    "        _, prediction_perturbed_U2 = torch.max(perturbed_logit_U2, 1)\n",
    "        ratio1 = ratio1_func(predictions_U1, prediction_perturbed_U1)\n",
    "        ratio2 = ratio2_func(predictions_U2, prediction_perturbed_U2)\n",
    "        \n",
    "        train_correct_S1 += np.sum(predictions_S1.cpu().numpy() == labels_S1.cpu().numpy())\n",
    "        total_S1 += labels_S1.size(0)\n",
    "\n",
    "        train_correct_U1 += np.sum(predictions_U1.cpu().numpy() == labels_U.cpu().numpy())\n",
    "        total_U1 += labels_U.size(0)\n",
    "\n",
    "        train_correct_S2 += np.sum(predictions_S2.cpu().numpy() == labels_S2.cpu().numpy())\n",
    "        total_S2 += labels_S2.size(0)\n",
    "\n",
    "        train_correct_U2 += np.sum(predictions_U2.cpu().numpy() == labels_U.cpu().numpy())\n",
    "        total_U2 += labels_U.size(0)\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        ls += Loss_sup.item()\n",
    "        lc += Loss_cot.item()\n",
    "        ld += Loss_diff.item()\n",
    "        \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        acc1 = (train_correct_S1+train_correct_U1) / (total_S1+total_U1)\n",
    "        acc2 = (train_correct_S2+train_correct_U2) / (total_S2+total_U2)\n",
    "        print(\"Epoch {:4}, {:3d}% \\t acc: {:3.4e} {:3.4e} - loss {:3.4e} {:3.4e} {:3.4e} {:3.4e} took: {:.2f}s\".format(\n",
    "            epoch+1,\n",
    "            int(100 * (i+1) / nb_batch),\n",
    "            \n",
    "            acc1, acc2,\n",
    "            running_loss/(i+1), ls/(i+1), lc/(i+1), ld/(i+1), \n",
    "            time.time() - start_time,\n",
    "        ), end=\"\\r\")\n",
    "            \n",
    "        # using tensorboard to monitor loss and acc\n",
    "        tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "        tensorboard.add_scalar('train/Lsup', Loss_sup.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Lcot', Loss_cot.item(), epoch )\n",
    "        tensorboard.add_scalar('train/Ldiff', Loss_diff.item(), epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_1\", 100. * acc1, epoch )\n",
    "        tensorboard.add_scalar(\"train/acc_2\",  100. * acc2, epoch )\n",
    "        tensorboard.add_scalar(\"train/ratio1\", ratio1, epoch )\n",
    "        tensorboard.add_scalar(\"train/ratio2\", ratio2, epoch )\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net1.eval()\n",
    "    net2.eval()\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            outputs1 = net1(inputs)\n",
    "            predicted1 = outputs1.max(1)\n",
    "            total1 += targets.size(0)\n",
    "            correct1 += predicted1[1].eq(targets).sum().item()\n",
    "\n",
    "            outputs2 = net2(inputs)\n",
    "            predicted2 = outputs2.max(1)\n",
    "            total2 += targets.size(0)\n",
    "            correct2 += predicted2[1].eq(targets).sum().item()\n",
    "            \n",
    "    tensorboard.add_scalar('val/acc_1', 100.*correct1/total1, epoch)\n",
    "    tensorboard.add_scalar('val/acc_2', 100.*correct2/total2, epoch)\n",
    "\n",
    "    print('\\nnet1 test acc: %.3f%% (%d/%d) | net2 test acc: %.3f%% (%d/%d)'\n",
    "        % (100.*correct1/total1, correct1, total1, 100.*correct2/total2, correct2, total2))\n",
    "\n",
    "    acc = ((100.*correct1/total1)+(100.*correct2/total2))/2\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "#         checkpoint(epoch, 'best')\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\"repro_cotraining/simple_%s_%s\" % (get_datetime(), model_func.__name__), comment=model_func.__name__)\n",
    "for epoch in range(0, 600):\n",
    "    print(\"Epoch     ,    % \\t acc: m1_acc    m2_acc    - loss sup       diff      cot      \")\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-18T08:09:18.742Z"
    }
   },
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "47.2px",
    "left": "831px",
    "top": "133.6px",
    "width": "149.725px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}